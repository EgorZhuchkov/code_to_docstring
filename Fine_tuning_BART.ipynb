{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbw0mEidjZCY",
    "outputId": "d8449f71-cb14-4853-c15b-6877ea93c5df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 12.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
      "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2021.4.4-cp37-cp37m-manylinux2014_x86_64.whl (720 kB)\n",
      "\u001b[K     |████████████████████████████████| 720 kB 13.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 16.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.20.2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.51.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.25.11)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "\u001b[K     |████████████████████████████████| 303 kB 22.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.0.1-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 4.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Installing collected packages: regex, joblib, click, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed click-8.0.1 huggingface-hub-0.0.12 joblib-1.0.1 regex-2021.4.4 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.1\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.96\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.2.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (9.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.9 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.20.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.2.5\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip install sentencepiece\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 27 21:00:12 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 455.23.04    Driver Version: 455.23.04    CUDA Version: 11.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 3090    On   | 00000000:42:00.0 Off |                  N/A |\r\n",
      "|  0%   50C    P8    38W / 350W |      0MiB / 24267MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hhQXskcJFZ2u"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import tokenize\n",
    "import sentencepiece as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "from transformers.models.t5 import T5Model, T5Config, T5ForConditionalGeneration\n",
    "from transformers.models.bart import BartForConditionalGeneration, BartConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ERb8ACZujZCg"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "path = \".\"\n",
    "MODEL_VERSION = 'result'\n",
    "SAMPLE_SIZE = 0.6\n",
    "VOCAB_SIZE = 30_000\n",
    "P_BPE = 0.2\n",
    "D_MODEL = 1024\n",
    "NUM_LAYERS = 8\n",
    "NUM_DECODER_LAYERS = 8\n",
    "D_FF = 3072\n",
    "NUM_HEADS = 16\n",
    "DROPOUT_RATE = 0.1\n",
    "LEARNING_RATE = 5e-5\n",
    "NUM_WARMUP_STEPS = 500\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VAL_BATCH_SIZE = 8\n",
    "MAX_EPOCHS = 8\n",
    "PATIENCE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ASNJxTb2Bl-X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-27 21:01:33--  https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.110.198\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.110.198|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 940909997 (897M) [application/zip]\n",
      "Saving to: ‘python.zip’\n",
      "\n",
      "python.zip          100%[===================>] 897.32M  25.6MB/s    in 36s     \n",
      "\n",
      "2021-06-27 21:02:10 (25.0 MB/s) - ‘python.zip’ saved [940909997/940909997]\n",
      "\n",
      "Archive:  python.zip\n",
      "   creating: python/\n",
      "   creating: python/final/\n",
      "   creating: python/final/jsonl/\n",
      "   creating: python/final/jsonl/train/\n",
      "  inflating: python/final/jsonl/train/python_train_9.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_12.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_10.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_0.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_6.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_2.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_4.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_8.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_11.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_5.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_13.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_3.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_1.jsonl.gz  \n",
      "  inflating: python/final/jsonl/train/python_train_7.jsonl.gz  \n",
      "   creating: python/final/jsonl/test/\n",
      "  inflating: python/final/jsonl/test/python_test_0.jsonl.gz  \n",
      "   creating: python/final/jsonl/valid/\n",
      "  inflating: python/final/jsonl/valid/python_valid_0.jsonl.gz  \n",
      "  inflating: python_dedupe_definitions_v2.pkl  \n",
      "  inflating: python_licenses.pkl     \n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip\n",
    "!unzip python.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "KTPe8ECFJTQ8",
    "outputId": "3f00ed77-01d5-410f-ab03-b4d34ca790a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python/final/jsonl/train/python_train_0.jsonl.gz', 'python/final/jsonl/train/python_train_1.jsonl.gz', 'python/final/jsonl/train/python_train_10.jsonl.gz', 'python/final/jsonl/train/python_train_11.jsonl.gz', 'python/final/jsonl/train/python_train_12.jsonl.gz', 'python/final/jsonl/train/python_train_13.jsonl.gz', 'python/final/jsonl/train/python_train_2.jsonl.gz', 'python/final/jsonl/train/python_train_3.jsonl.gz', 'python/final/jsonl/train/python_train_4.jsonl.gz', 'python/final/jsonl/train/python_train_5.jsonl.gz', 'python/final/jsonl/train/python_train_6.jsonl.gz', 'python/final/jsonl/train/python_train_7.jsonl.gz', 'python/final/jsonl/train/python_train_8.jsonl.gz', 'python/final/jsonl/train/python_train_9.jsonl.gz']\n",
      "['python/final/jsonl/valid/python_valid_0.jsonl.gz']\n",
      "['python/final/jsonl/test/python_test_0.jsonl.gz']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def train(train_dir, model_save_path=None, n_n...</td>\n",
       "      <td>Trains a k-nearest neighbors classifier for fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def predict(X_img_path, knn_clf=None, model_pa...</td>\n",
       "      <td>Recognizes faces in given image using a traine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def show_prediction_labels_on_image(img_path, ...</td>\n",
       "      <td>Shows the face recognition results visually.\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def _rect_to_css(rect):\\n    \"\"\"\\n    Convert ...</td>\n",
       "      <td>Convert a dlib 'rect' object to a plain tuple ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def _trim_css_to_bounds(css, image_shape):\\n  ...</td>\n",
       "      <td>Make sure a tuple in (top, right, bottom, left...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  \\\n",
       "0  def train(train_dir, model_save_path=None, n_n...   \n",
       "1  def predict(X_img_path, knn_clf=None, model_pa...   \n",
       "2  def show_prediction_labels_on_image(img_path, ...   \n",
       "3  def _rect_to_css(rect):\\n    \"\"\"\\n    Convert ...   \n",
       "4  def _trim_css_to_bounds(css, image_shape):\\n  ...   \n",
       "\n",
       "                                           docstring  \n",
       "0  Trains a k-nearest neighbors classifier for fa...  \n",
       "1  Recognizes faces in given image using a traine...  \n",
       "2  Shows the face recognition results visually.\\n...  \n",
       "3  Convert a dlib 'rect' object to a plain tuple ...  \n",
       "4  Make sure a tuple in (top, right, bottom, left...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jsonl_list_to_dataframe(file_list, columns=None):\n",
    "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
    "    return pd.concat([pd.read_json(f,\n",
    "                                   orient='records', \n",
    "                                   compression='gzip',\n",
    "                                   lines=True)[columns] \n",
    "                      for f in file_list], sort=False)\n",
    "\n",
    "def get_dfs(path):\n",
    "    \"\"\"Grabs the different data splits and converts them into dataframes\"\"\"\n",
    "    dfs = []\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        split_dir = os.path.join(path, split)\n",
    "        files = []\n",
    "        for file in os.listdir(split_dir):\n",
    "            if file.endswith('.gz'):\n",
    "                files.append(os.path.join(split_dir, file))\n",
    "        files = sorted(files)\n",
    "        print(files)\n",
    "        df = jsonl_list_to_dataframe(files, [\"code\", \"docstring\"])\n",
    "        dfs.append(df)\n",
    "        \n",
    "    return dfs\n",
    "\n",
    "df_trn, df_val, df_tst = get_dfs(\"python/final/jsonl\")\n",
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-2t_qnopKBQ_"
   },
   "outputs": [],
   "source": [
    "def isLatin(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "df_trn = df_trn.sample(frac = SAMPLE_SIZE)\n",
    "df_val = df_val.sample(frac = SAMPLE_SIZE)\n",
    "df_tst = df_tst.sample(frac = SAMPLE_SIZE)\n",
    "\n",
    "df_trn = df_trn[df_trn['docstring'].apply(lambda x: isLatin(x))]\n",
    "df_val = df_val[df_val['docstring'].apply(lambda x: isLatin(x))]\n",
    "df_tst = df_tst[df_tst['docstring'].apply(lambda x: isLatin(x))]\n",
    "\n",
    "df_trn = df_trn[df_trn['code'].apply(lambda x: isLatin(x))]\n",
    "df_val = df_val[df_val['code'].apply(lambda x: isLatin(x))]\n",
    "df_tst = df_tst[df_tst['code'].apply(lambda x: isLatin(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163,
     "referenced_widgets": [
      "3a0d782f37e645fe9445b1fdc3acfde9",
      "065c90a0fb96402695ecb3d0723bea64",
      "a6bc9d0d8020406f85570c441ebb03a9",
      "7763b7074338498cbdd0ce8fa9534564",
      "3d82364d3bef42fcb15d324891e709e3",
      "dc4a36e3d20f466db30c766b9cad498e",
      "cdc8cb62d0e1416e90c791544be95f66",
      "d4c264c0f19a4265af7148ac0814b48d",
      "58411f1b150249a39158f7ed1e254206",
      "ac1cc59595cd40af90daa26387f853f0",
      "a6eb37f358354c1abd67c876691036c9",
      "4c05e986759b450abe976683f157af42",
      "fb595b0e7fad4cf99bb5f226baaa56c6",
      "db68a029d1cf44ea901142753a0a63a1",
      "5040d47de6a54a4eb48097a58a2dab1d",
      "32ce0ac9d280447587bd4eda65a190d4",
      "9da5f0d18a72477db595f3fb5b404a4c",
      "7d17b5188a924dabbb4d4962d7b826ab",
      "fb257000c67142a9b2e7cac50f879922",
      "cc1e493375c84e55a5e2e7640671d66b",
      "851cf5a5b35142299d2761f0742841a6",
      "464a8630bb5344be9af17d2e3b234544",
      "41836f7bf92c4cc5b7d0ed6753d7566f",
      "1bd81390f4d7430e981c1f13a1c79d7f"
     ]
    },
    "id": "VV0X7smOKFaf",
    "outputId": "4d76114f-19b7-4afe-e6cf-39a9b1c1d621"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67fb9aa11794152902db5e2bbe4903d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=243387.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unindent does not match any outer indentation level (<tokenize>, line 24)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539c9607cc144a1b85af4a4d73853339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=13539.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b47bcd6e2e4471a18407c47fe06243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=13176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_comments_and_docstrings(source):\n",
    "    io_obj = io.StringIO(source)\n",
    "    out = \"\"\n",
    "    prev_toktype = tokenize.INDENT\n",
    "    last_lineno = -1\n",
    "    last_col = 0\n",
    "    for tok in tokenize.generate_tokens(io_obj.readline):\n",
    "        token_type = tok[0]\n",
    "        token_string = tok[1]\n",
    "        start_line, start_col = tok[2]\n",
    "        end_line, end_col = tok[3]\n",
    "        ltext = tok[4]\n",
    "        if start_line > last_lineno:\n",
    "            last_col = 0\n",
    "        if start_col > last_col:\n",
    "            out += (\" \" * (start_col - last_col))\n",
    "        if token_type == tokenize.COMMENT:\n",
    "            pass\n",
    "        elif token_type == tokenize.STRING:\n",
    "            if prev_toktype != tokenize.INDENT:\n",
    "                if prev_toktype != tokenize.NEWLINE:\n",
    "                    if start_col > 0:\n",
    "                        out += token_string\n",
    "        else:\n",
    "            out += token_string\n",
    "        prev_toktype = token_type\n",
    "        last_col = end_col\n",
    "        last_lineno = end_line\n",
    "    out = '\\n'.join(l for l in out.splitlines() if l.strip())\n",
    "    return out\n",
    "\n",
    "def filter_docstrings(df):\n",
    "    methods = []\n",
    "    comments = []\n",
    "    for i, row in tqdm(list(df.iterrows())):\n",
    "        code = row[\"code\"]\n",
    "        try:\n",
    "            methods.append(remove_comments_and_docstrings(code))\n",
    "            comments.append(row[\"docstring\"]) \n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "        \n",
    "    new_df = pd.DataFrame(zip(methods, comments), columns = [\"code\", \"docstring\"])\n",
    "\n",
    "    return new_df\n",
    "\n",
    "df_trn = filter_docstrings(df_trn);\n",
    "df_val = filter_docstrings(df_val);\n",
    "df_tst = filter_docstrings(df_tst);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWhOpLEOKb96",
    "outputId": "41a41929-f8ae-49e5-db4e-8d6b1dda3b5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234017, 13224, 12905)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn = df_trn[~(df_trn['docstring'] == '')]\n",
    "df_val = df_val[~(df_val['docstring'] == '')]\n",
    "df_tst = df_tst[~(df_tst['docstring'] == '')]\n",
    "\n",
    "df_trn = df_trn[~df_trn['docstring'].duplicated()]\n",
    "df_val = df_val[~df_val['docstring'].duplicated()]\n",
    "df_tst = df_tst[~df_tst['docstring'].duplicated()]\n",
    "\n",
    "len(df_trn), len(df_val), len(df_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bH9pjtHcjZCp",
    "outputId": "1f7c1d15-186f-4fba-cfd2-240de8cea68d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x7f3c20b7ad90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.itertuples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GRHsG6LTKix1"
   },
   "outputs": [],
   "source": [
    "def df_to_txt_file(df, output, cols):\n",
    "    \"\"\"Converts a dataframe and converts it into a text file that SentencePiece can use to train a BPE model\"\"\"\n",
    "    file_path = os.path.join(output,'text.txt')\n",
    "    with open(file_path, 'w') as f:\n",
    "        for i, col in enumerate(cols):\n",
    "            f.write('\\n'.join(list(df[col])))\n",
    "            f.write('\\n')\n",
    "            \n",
    "    return file_path\n",
    "\n",
    "def gen_sp_model(df, output, tokenizer_name, cols, vocab_size=8000):\n",
    "    \"\"\"Trains a SentencePiece BPE model from a pandas dataframe\"\"\"\n",
    "    fname = df_to_txt_file(df, output, cols)\n",
    "    sp.SentencePieceTrainer.train(f'--input={fname} --model_prefix={os.path.join(output, tokenizer_name)}'\\\n",
    "                                  f' --hard_vocab_limit=false --vocab_size={vocab_size}'\\\n",
    "                                  ' --unk_id=0 --pad_id=1 --bos_id=2 --eos_id=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6s6mgOhcjZCq"
   },
   "outputs": [],
   "source": [
    "# os.makedirs('shared_bpe', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "K3HnU9rOjZCr"
   },
   "outputs": [],
   "source": [
    "shared_bpe_path = 'shared_bpe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-AlUT0BJKlj0"
   },
   "outputs": [],
   "source": [
    "shared_tokenizer = 'shared_bpe'\n",
    "# gen_sp_model(df_trn.sample(frac = P_BPE), 'shared_bpe', shared_tokenizer, [\"code\", \"docstring\"], vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77EUPTkzjZCs",
    "outputId": "f1a22cb1-f4e2-4494-df63-addb2dde05b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_spm = sp.SentencePieceProcessor()\n",
    "shared_spm.Load(os.path.join(shared_bpe_path, shared_tokenizer + '.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j02GrZgkjZCt",
    "outputId": "317015ae-6b3c-481e-b7f5-5a6c7dd31f19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26, 2621, 7, 79, 6, 151, 17]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_spm.encode_as_ids(\"def bar(x,y):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z25Szv3bjZCt",
    "outputId": "427b164b-10dc-4142-8981-a5856dce197b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁def', '▁bar', '(', 'x', ',', 'y', '):']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_spm.encode_as_pieces(\"def bar(x,y):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5D9gVmoTjZCu",
    "outputId": "9242dbf9-2846-49eb-8e05-42eaa27ea4b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 26, 2621, 7, 79, 6, 151, 17, 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en = shared_spm.tokenize(\"def bar(x,y):\", int,add_bos=True, add_eos=True,)\n",
    "en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "36xWvHlcjZCu",
    "outputId": "6443a877-95e1-4eb9-f7ef-9a144cb5edfd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def mtf_image_transformer_tiny():\\n  hparams =...</td>\n",
       "      <td>Catch bugs locally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def get_child_family_ids(self, family_id):\\n  ...</td>\n",
       "      <td>Gets the child ``Ids`` of the given family.\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def end_headers(self):\\n        if self.reques...</td>\n",
       "      <td>Send the blank line ending the MIME headers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def view_call(method_name, *args, **kwargs):\\n...</td>\n",
       "      <td>Creates an effect that will drop the current e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def get_logger(name=None, level=logging.DEBUG,...</td>\n",
       "      <td>returns a colorized logger. This function can ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  \\\n",
       "0  def mtf_image_transformer_tiny():\\n  hparams =...   \n",
       "1  def get_child_family_ids(self, family_id):\\n  ...   \n",
       "2  def end_headers(self):\\n        if self.reques...   \n",
       "3  def view_call(method_name, *args, **kwargs):\\n...   \n",
       "4  def get_logger(name=None, level=logging.DEBUG,...   \n",
       "\n",
       "                                           docstring  \n",
       "0                              Catch bugs locally...  \n",
       "1  Gets the child ``Ids`` of the given family.\\n\\...  \n",
       "2       Send the blank line ending the MIME headers.  \n",
       "3  Creates an effect that will drop the current e...  \n",
       "4  returns a colorized logger. This function can ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zy5VOj14jZCw",
    "outputId": "5349b6df-df0b-4273-a5e8-695081f2ebb1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shared_spm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-60e613bb665a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshared_spm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'shared_spm' is not defined"
     ]
    }
   ],
   "source": [
    "shared_spm.pad_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JPABab5_jZCw"
   },
   "outputs": [],
   "source": [
    "class CodeToDocstringDataset(Dataset):\n",
    "    def __init__(self, tokenizer,\n",
    "                 codes, docstrings,\n",
    "                 max_code_len=340, max_docstring_len=340\n",
    "                ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.codes = [tokenizer(c)['input_ids'] for c in tqdm(codes)]\n",
    "        self.docstrings = [tokenizer(d)['input_ids'] for d in tqdm(docstrings)]\n",
    "        self.max_code_len = max_code_len\n",
    "        self.max_docstring_len = max_docstring_len\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.codes[idx], self.docstrings[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.codes)\n",
    "\n",
    "    def collate(self, rows):\n",
    "        codes = [torch.tensor(row[0][:self.max_code_len], \n",
    "                              dtype=torch.long) for row in rows]\n",
    "        \n",
    "        comments = [torch.tensor(row[1][:self.max_docstring_len],\n",
    "                                 dtype=torch.long) for row in rows]\n",
    "        \n",
    "        code_tensor = pad_sequence(codes,batch_first=True,padding_value=self.tokenizer.pad_token_id)\n",
    "        doc_tensor = pad_sequence(comments,batch_first=True,padding_value=self.tokenizer.pad_token_id)\n",
    "        \n",
    "        return code_tensor, doc_tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bart_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285439a01b1e4b0f867a2c181c4c0872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=898823.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d74289791c41f1aaa1dbf2b31477eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=456318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad22eae30cb40afb042c022a0fb806e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1355863.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bart_tokenizer = BartTokenizerFast.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['code', 'docstring'], dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn['code_cleaned'] = df_trn['code'].apply(lambda c: re.sub(\"\\s+\",\" \",c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tst['code_cleaned'] = df_tst['code'].apply(lambda c: re.sub(\"\\s+\",\" \",c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['code_cleaned'] = df_val['code'].apply(lambda c: re.sub(\"\\s+\",\" \",c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn['doc_cleaned'] = df_trn['docstring'].apply(lambda c: re.sub(\"\\s+\",\" \",c))\n",
    "df_tst['doc_cleaned'] = df_tst['docstring'].apply(lambda c: re.sub(\"\\s+\",\" \",c))\n",
    "df_val['doc_cleaned'] = df_val['docstring'].apply(lambda c: re.sub(\"\\s+\",\" \",c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def mtf_image_transformer_tiny(): hparams = mtf_image_transformer_base() hparams.hidden_size = 128 hparams.d_ff = 256 hparams.batch_size = 4 hparams.num_encoder_layers = 1 hparams.num_decoder_layers = 4 hparams.num_heads = 4 hparams.attention_key_size = 128 hparams.attention_value_size = 128 hparams.block_length = 32 hparams.mesh_shape = \"batch:2\" hparams.layout = \"batch:batch\" return hparams\n"
     ]
    }
   ],
   "source": [
    "for c in df_trn['code_cleaned']:\n",
    "    print(c)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114,
     "referenced_widgets": [
      "bbce2ecea1d8484b97377ebc9aab5a55",
      "34cd929da6c244b4a4d5cb7dbb1a2e0a",
      "d958317e72594ba8990df000f53ffa3b",
      "18459f31bb9543aa8ad3c2420e134ab2",
      "6e82cf5b42184e078e10c342adbf0fca",
      "1ce0ebb938304d5cafa1b5f23f8ecf69",
      "7a6074e43d65464f8ca5a088ba075ce5",
      "56b0c2d6c01a40eaaec787f2427c9ea4",
      "87676b91d4834f93adb91622e59c63fd",
      "67bbfa6548a34d828cab44ecdb09acba",
      "8a18144909074193bb17ea5c43c7ae11",
      "fcf0e7e96aec45c8b5433524e769c957",
      "d83f2d363c544aadb23733b705a508e3",
      "e8d7607d2b8e40f88e2b433e3d649e21",
      "e4dcef5e456c4b3588f3c23289243291",
      "8449169063654aca906068acff76652d"
     ]
    },
    "id": "ehTu8KswjZCw",
    "outputId": "32f31a0e-821e-46ee-a81d-3c178d6ec995"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f940e04b8f4d6c801a222bc133317a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=234017.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5698 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d966acf400e42debbebe96bc5eb4cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=234017.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CodeToDocstringDataset(bart_tokenizer,\n",
    "                                       df_trn['code_cleaned'], df_trn['doc_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    0,  9232,   475, 41407,  1215, 20094,  1215,  9981, 22098,  1215,\n",
       "          40743, 49536,  1368, 49237,  5457,   475, 41407,  1215, 20094,  1215,\n",
       "           9981, 22098,  1215, 11070, 43048,  1368, 49237,     4, 37392,  1215,\n",
       "          10799,  5457, 13950,  1368, 49237,     4,   417,  1215,  3145,  5457,\n",
       "          22078,  1368, 49237,     4, 35001,  1215, 10799,  5457,   204,  1368,\n",
       "          49237,     4, 42666,  1215, 14210, 15362,  1215,   462, 24950,  5457,\n",
       "            112,  1368, 49237,     4, 42666,  1215, 11127, 15362,  1215,   462,\n",
       "          24950,  5457,   204,  1368, 49237,     4, 42666,  1215, 16560,  5457,\n",
       "            204,  1368, 49237,     4,  2611, 19774,  1215,  5282,  1215, 10799,\n",
       "           5457, 13950,  1368, 49237,     4,  2611, 19774,  1215, 19434,  1215,\n",
       "          10799,  5457, 13950,  1368, 49237,     4, 16776,  1215, 16096,  5457,\n",
       "           2107,  1368, 49237,     4,   119,  4891,  1215, 43882,  5457,    22,\n",
       "          35001,    35,   176,   113,  1368, 49237,     4, 48842,  5457,    22,\n",
       "          35001,    35, 35001,   113,   671,  1368, 49237,     2],\n",
       "         [    0,  9232,   120,  1215, 14069,  1215, 12368,  1215,  7823,  1640,\n",
       "          13367,     6,   284,  1215,   808,  3256,   114,  1403, 48030,  8729,\n",
       "          31263,  1215, 39035,    16,    45,  9291,    35,   671,  1403, 48030,\n",
       "           8729, 31263,  1215, 39035,     4,  6460,  1215, 14069,  1215,  8729,\n",
       "          31263,  1215,  7823,  1640,  8729, 31263,  1215,   808,  5214, 12368,\n",
       "           1215,   808,    43,   671,  1403, 48030,   298,   906, 37208,  1215,\n",
       "          39035,     4,  6460,  1215, 15097,  1640,   808,  1215,  5214, 12368,\n",
       "           1215,   808,    43,     2,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1]]),\n",
       " tensor([[    0,   347, 11175, 19230,  8094,   734,     2,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,   534,  2580,     5,   920, 45518, 28081,    29, 49519,     9,\n",
       "              5,   576,   284,     4, 29480,    35,   284,  1215,   808,    36,\n",
       "            366,   808,     4,   808,     4, 28081,  3256,     5, 45518, 28081,\n",
       "          49519,     7, 25860,   671,    35,    36,   366,   808,     4,   808,\n",
       "              4, 28081, 36583,    43,   111,     5,   408,     9,     5,   284,\n",
       "           1693,    35,  1491, 29991,   111, 45518, 12368,  1215,   808, 49519,\n",
       "             16,    45,   303,  1693,    35, 44840, 45621, 18816,   111, 45518,\n",
       "          12368,  1215,   808, 49519,    16, 45518, 15755, 49519,  1693,    35,\n",
       "          13346,   597, 13355,   111,  3276,     7,  1498,  2069,  1693,    35,\n",
       "           2595, 12478, 27267,  2550,   111, 19234,  2988,  1009, 33182,    35,\n",
       "           8549,   480,   152,  5448,   531,    28,  6264, 26487,     2]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.collate([train_dataset[0], train_dataset[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114,
     "referenced_widgets": [
      "6f6a6fd0019b4e64aae6e8bbf8ce144d",
      "ad564ace58bc43738801d278055cf70b",
      "5b3278fc935e4d07b65aa735590758f9",
      "f6d62ccd9c624855bb953f060020a79b",
      "27e4dd787061427591847cb34c3b03ae",
      "1c5c8da1be4f40d5b2a9e9ac46c643f6",
      "0170fe02467340fb8bfcf58f0f59813d",
      "4cac3adb7ea2492882f11a42f8558189",
      "677a45bd99784f26a4941aabb8784a04",
      "93474465b1d0414ab292bd92aa4e39f4",
      "08310e3e94964cdda729f31fcf46e9fd",
      "6c42ce2210784ffca06210211751bf60",
      "49afa35c19414cff826aac49f948ad4f",
      "26a9432d39384357ae6895db77cd306e",
      "b05d914f44bb4a269383636295f0ddfe",
      "032c5e2018084305857f9ad4c7a09ec3"
     ]
    },
    "id": "58yTbjvVjZCx",
    "outputId": "0e4754eb-2da2-4468-d9ac-4af82462872c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b7446f24634f90b80c91b445eb8fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=13224.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e5bd24461f42a79c1e9d85ca13e297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=13224.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = CodeToDocstringDataset(bart_tokenizer,\n",
    "                                       df_val['code_cleaned'], df_val['doc_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mpo0zQDIjZCx",
    "outputId": "057f3cc5-cc07-4c7b-bbae-24bdbb74eece"
   },
   "outputs": [],
   "source": [
    "# shared_spm.vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "26h35lGTjZCy"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceba9a35ff884417b3df278906d60397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1627.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model_config = BartConfig(\n",
    "#     vocab_size=shared_spm.vocab_size(),\n",
    "#     d_model=D_MODEL,\n",
    "#     num_layers=NUM_LAYERS,\n",
    "#     num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "#     d_ff=D_FF,\n",
    "#     num_heads=NUM_HEADS, \n",
    "#     dropout_rate=DROPOUT_RATE,\n",
    "#     decoder_start_token_id=shared_spm.bos_id(),\n",
    "#     tie_word_embeddings = False\n",
    "# )\n",
    "\n",
    "model_config = BartConfig.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "QqnUHyYbA7Gy"
   },
   "outputs": [],
   "source": [
    "class BestModel:\n",
    "    def __init__(self, path, initial_criterion):\n",
    "        self.path = path\n",
    "        self.criterion = initial_criterion\n",
    "        \n",
    "    def update(self, model, criterion):\n",
    "        self.criterion = criterion\n",
    "        torch.save({'model_state': model.state_dict(), 'criterion': criterion}, self.path)\n",
    "        \n",
    "    def load_model_data(self):\n",
    "        return torch.load(self.path)\n",
    "    \n",
    "    def restore(self, model):\n",
    "        model_data = self.load_model_data()\n",
    "        model.load_state_dict(model_data['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "mnpG9J-pjZCz"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def masked_crossentropy(logits, targets, padding_idx):\n",
    "    return F.cross_entropy(logits.view(-1, logits.size(-1)),\n",
    "                           targets.view(-1), ignore_index=padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "r-56va4mjZCz"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d531b381f347e88a634a451a2cf85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=557941479.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "32ICz4cgjZC0"
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(lr=LEARNING_RATE,params=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "uEF87f4vjZC0"
   },
   "outputs": [],
   "source": [
    "scheduler = get_constant_schedule_with_warmup(optim, num_warmup_steps=NUM_WARMUP_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e50U5nHnjZC0",
    "outputId": "424b6279-96ea-4e05-a8fb-fd1f7f63d342"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(optim.param_groups))['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(df, sample_size = 10):\n",
    "    print(\"[Test model results]\")\n",
    "    \n",
    "    sample = df.sample(sample_size)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, row in sample.iterrows():\n",
    "            code = row['code_cleaned']\n",
    "            inp = bart_tokenizer(code)['input_ids']\n",
    "            inp = torch.tensor(inp).view(1,-1)[:,:330].clone()\n",
    "            generated = model.generate(input_ids=inp.to(model.device), \n",
    "                                       decoder_start_token_id=bart_tokenizer.bos_token_id, \n",
    "                                       num_beams=10,\n",
    "                                       max_length=25, no_repeat_ngram_size=4)\n",
    "            generated = generated[0].cpu().tolist()\n",
    "            print(\"---------------------------\")\n",
    "            print(f\"Code: {code}\")\n",
    "            print(f\"Target: {row.docstring}\")\n",
    "            print(f\"Generated :{bart_tokenizer.decode(generated)}\")\n",
    "            \n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def _kl_uniform_uniform(a, b, name=None): with tf.name_scope(name or \"kl_uniform_uniform\"): final_batch_shape = distribution_util.get_broadcast_shape( a.low, b.low, a.high, b.high) dtype = dtype_util.common_dtype( [a.low, a.high, b.low, b.high], tf.float32) return tf.where((b.low <= a.low) & (a.high <= b.high), tf.math.log(b.high - b.low) - tf.math.log(a.high - a.low), tf.broadcast_to( dtype_util.as_numpy_dtype(dtype)(np.inf), final_batch_shape))\n",
      "Target: Calculate the batched KL divergence KL(a || b) with a and b Uniform.\n",
      "\n",
      "  Note that the KL divergence is infinite if the support of `a` is not a subset\n",
      "  of the support of `b`.\n",
      "\n",
      "  Args:\n",
      "    a: instance of a Uniform distribution object.\n",
      "    b: instance of a Uniform distribution object.\n",
      "    name: (optional) Name to use for created operations.\n",
      "      default is \"kl_uniform_uniform\".\n",
      "\n",
      "  Returns:\n",
      "    Batchwise KL(a || b)\n",
      "Generated :<s><s>def _kl_uniform_uniform(a, b, name=None): with tf.name</s>\n",
      "---------------------------\n",
      "Code: def get_parent(self): if not isinstance(self.parent, Expression): raise FiqlObjectException(\"Parent must be of %s not %s\" % ( Expression, type(self.parent))) return self.parent\n",
      "Target: Get the parent ``Expression`` for this object.\n",
      "\n",
      "        Returns:\n",
      "            Expression: The ``Expression`` which contains this object.\n",
      "\n",
      "        Raises:\n",
      "            FiqlObjectException: Parent is ``None``.\n",
      "Generated :<s><s>def get_parent(self): if not isinstance(self.parent, Expression): raise FiqlObject</s>\n",
      "---------------------------\n",
      "Code: def base64_decode(string): string = want_bytes(string, encoding='ascii', errors='ignore') return base64.urlsafe_b64decode(string + b'=' * (-len(string) % 4))\n",
      "Target: base64 decodes a single bytestring (and is tolerant to getting\n",
      "    called with a unicode string).\n",
      "    The result is also a bytestring.\n",
      "Generated :<s><s>def base64_decode(string): string = want_bytes(string, encoding='ascii</s>\n",
      "---------------------------\n",
      "Code: def convert(self, amount, currency, new_currency='EUR', date=None): for c in currency, new_currency: if c not in self.currencies: raise ValueError('{0} is not a supported currency'.format(c)) if date is None: date = self.bounds[currency].last_date else: try: date = date.date() except AttributeError: pass r0 = self._get_rate(currency, date) r1 = self._get_rate(new_currency, date) return float(amount) / r0 * r1\n",
      "Target: Convert amount from a currency to another one.\n",
      "\n",
      "        :param float amount: The amount of `currency` to convert.\n",
      "        :param str currency: The currency to convert from.\n",
      "        :param str new_currency: The currency to convert to.\n",
      "        :param datetime.date date: Use the conversion rate of this date. If this\n",
      "            is not given, the most recent rate is used.\n",
      "\n",
      "        :return: The value of `amount` in `new_currency`.\n",
      "        :rtype: float\n",
      "\n",
      "        >>> from datetime import date\n",
      "        >>> c = CurrencyConverter()\n",
      "        >>> c.convert(100, 'EUR', 'USD', date=date(2014, 3, 28))\n",
      "        137.5...\n",
      "        >>> c.convert(100, 'USD', date=date(2014, 3, 28))\n",
      "        72.67...\n",
      "        >>> c.convert(100, 'BGN', date=date(2010, 11, 21))\n",
      "        Traceback (most recent call last):\n",
      "        RateNotFoundError: BGN has no rate for 2010-11-21\n",
      "Generated :<s><s>def convert(self, amount, currency, new_currency='EUR', date=None): for c</s>\n",
      "---------------------------\n",
      "Code: def _rename_hstore_unique(self, old_table_name, new_table_name, old_field, new_field, keys): old_name = self._unique_constraint_name( old_table_name, old_field, keys) new_name = self._unique_constraint_name( new_table_name, new_field, keys) sql = self.sql_hstore_unique_rename.format( old_name=self.quote_name(old_name), new_name=self.quote_name(new_name) ) self.execute(sql)\n",
      "Target: Renames an existing UNIQUE constraint for the specified\n",
      "        hstore keys.\n",
      "Generated :<s><s>def _rename_hstore_unique(self, old_table_name, new_table_</s>\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "test_model(df_tst, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_EVERY=25_000 // TRAIN_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "6aJL-XG-jZC1"
   },
   "outputs": [],
   "source": [
    "def compute_loss(model, encoder_inputs, decoder_inputs, decoder_labels, padding_idx):\n",
    "    attention_mask = (encoder_inputs != padding_idx).type(torch.long)\n",
    "    decoder_attention_mask = (decoder_inputs != padding_idx).type(torch.long)\n",
    "            \n",
    "    pass_result = model.forward(encoder_inputs, attention_mask, \n",
    "                          decoder_input_ids=decoder_inputs, \n",
    "                          decoder_attention_mask=decoder_attention_mask, return_dict=True)\n",
    "    logits = pass_result['logits']\n",
    "            \n",
    "    loss = masked_crossentropy(logits, decoder_labels, padding_idx)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_model(model, optimizer, lr_scheduler, scheduler_step_frequency,\n",
    "                loaders, max_epochs, device, \n",
    "                best_model: BestModel, patience, padding_idx):\n",
    "    allowed_epochs_without_improvement = patience\n",
    "    \n",
    "    for epoch in tqdm(range(max_epochs)):\n",
    "        model.train()\n",
    "        train_iter = tqdm(loaders['train'])\n",
    "        running_sum_loss = 0.0\n",
    "        running_total_batches = 0\n",
    "        for i, (xx, yy) in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            encoder_inputs = xx.to(device)\n",
    "            decoder_inputs = yy[:,:-1].clone().to(device)\n",
    "            decoder_labels = yy[:,1:].clone().to(device)\n",
    "\n",
    "            loss = compute_loss(model, encoder_inputs, decoder_inputs, decoder_labels, padding_idx)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if lr_scheduler is not None and scheduler_step_frequency == 'step':\n",
    "                lr_scheduler.step()\n",
    "            \n",
    "            running_sum_loss += loss.item()\n",
    "            running_total_batches += 1\n",
    "            \n",
    "            train_iter.set_postfix({\n",
    "                \"avg_train_loss\": np.round(running_sum_loss / running_total_batches, 4),\n",
    "                \"train_step_loss\": np.round(loss.item(), 4)\n",
    "            })\n",
    "            \n",
    "            if i % GEN_EVERY == 0:\n",
    "                test_model(df_tst, 5)\n",
    "\n",
    "        if lr_scheduler is not None and scheduler_step_frequency == 'epoch':\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            \n",
    "            val_iter = tqdm(loaders['validation'])\n",
    "            \n",
    "            running_sum_loss = 0.0\n",
    "            running_total_batches = 0\n",
    "            \n",
    "            for (xx, yy) in val_iter:\n",
    "                encoder_inputs = xx.to(device)\n",
    "                decoder_inputs = yy[:,:-1].clone().to(device)\n",
    "                decoder_labels = yy[:,1:].clone().to(device)\n",
    "                \n",
    "                loss = compute_loss(model, encoder_inputs, decoder_inputs, decoder_labels, padding_idx)\n",
    "                \n",
    "                running_sum_loss += loss.item()\n",
    "                running_total_batches += 1\n",
    "                \n",
    "                val_iter.set_postfix({\n",
    "                    \"avg_val_loss\": np.round(running_sum_loss / running_total_batches, 4),\n",
    "                    \"val_step_loss\": np.round(loss.item(), 4)\n",
    "                })\n",
    "                \n",
    "            final_val_loss = running_sum_loss / running_total_batches\n",
    "            prev_loss = best_model.criterion\n",
    "            if final_val_loss < prev_loss:\n",
    "                best_model.update(model, final_val_loss)\n",
    "                print(f\"Model saved due to improvement in 'val_loss'\"\\\n",
    "                      f\" from {prev_loss} to {final_val_loss}\")\n",
    "                allowed_epochs_without_improvement = patience\n",
    "            else:\n",
    "                if allowed_epochs_without_improvement <= 0:\n",
    "                    print(f\"Training stopped due to no improvement in {patience} epochs\")\n",
    "                    return\n",
    "                allowed_epochs_without_improvement -= 1\n",
    "           \n",
    "        test_model(df_tst, 5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "-XzxgiWdjZC1"
   },
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "dataloaders['train'] = DataLoader(train_dataset, TRAIN_BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate)\n",
    "dataloaders['validation'] = DataLoader(validation_dataset,\n",
    "                                       VAL_BATCH_SIZE, collate_fn=validation_dataset.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_VERSION = \"rt_bart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "4EzP3FYAjZC2"
   },
   "outputs": [],
   "source": [
    "best_model = BestModel(f\"model_{MODEL_VERSION}\", initial_criterion=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "0QAh7JmdjZC2"
   },
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "BeQE0ODBjZC2"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195,
     "referenced_widgets": [
      "923942a6c1ba4257bdddc929c4ee6892",
      "75cbbe18416b404d9c8b9d88415b05c5",
      "04d20bb756174e6eae44bf594c67bf10",
      "7476d0bd592843f1a06fbd3ce50b15cc",
      "bd97154633b2421bb3f478889c5c58f7",
      "b3974ec9f1be45fcae9f3ac5dd43af5e",
      "cce045e884bd4aac874bc110c14e33b0",
      "4f31cb2b4adf4756ab2d5e08fd980436",
      "27d1422d4bd042938661ac7bab629c0e",
      "4505a00686194672ae8925431e44d346",
      "963f53c3cf404e7595dc554fc63d4df4",
      "74203a2bcb2a4e3e9fb71ca475f9c259",
      "6d40bb935de74b1499d2b33c8b7eca95",
      "3aed8bcf8b564a48a638cadc0b0151e3",
      "ec01cb9d81b947d395a473dba7a0be06",
      "c11e5211a0854a82978fef8e72d2b3ee",
      "88991afeec49461e9aaff7a0d1da365b",
      "0170af284d8f461185a74a9e846ad69e",
      "de2624bf5c7e4e1aaccdf867efced9f9",
      "98f0e71a7a914a7ba68a24b8f79d72fe",
      "c577f0e201ca40b9b6cfedd733b4ef20",
      "8f0d023d16b840379f397ac8e7c3493c",
      "73cf070109c046a6ba833c860918b3df",
      "224af8630019457ba874067c146f1bba",
      "0ff4d3c660114a90a9ecbcc8b74b03fc",
      "dbfe2779d8be4d1882e2e8567b96d1b1",
      "4c951b96425242a9969c08101c187198",
      "a23d31de48ee4de99a6bfbe75d9d5382",
      "c211b46cfe6f4f90b762aad20f7ee2b9",
      "368178aefa7e465aa82222bef29cbed5",
      "8cca36eca10f45a9bd48aa9360c49253",
      "5b510735452c441f9a855ea43b2b3b6d"
     ]
    },
    "id": "8DlSMVZHVqvn",
    "outputId": "e63658ba-55cd-4e1f-b8ef-a2a60af89bce",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93db2e021e174d3db008692eb47ab9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c90cdb120f4c618a9a72895bf9978a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=29253.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def call(self, inputs): collapsed_shape = tf.concat(([-1], tf.shape(input=inputs)[-2:]), axis=0) out = tf.reshape(inputs, collapsed_shape) out = self.bilstm(out) expanded_shape = tf.concat((tf.shape(input=inputs)[:-2], [-1]), axis=0) out = tf.reshape(out, expanded_shape) out = self.output_layer(out) loc = out[..., :self.latent_size] scale_diag = tf.nn.softplus(out[..., self.latent_size:]) + 1e-5 return tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale_diag)\n",
      "Target: Runs the model to generate a distribution `q(f | x_{1:T})`.\n",
      "\n",
      "    This generates a list of batched MultivariateNormalDiag\n",
      "    distributions using the output of the recurrent model at each\n",
      "    timestep to parameterize each distribution.\n",
      "\n",
      "    Args:\n",
      "      inputs: A batch of intermediate representations of image frames\n",
      "        across all timesteps, of shape [..., batch_size, timesteps,\n",
      "        hidden_size].\n",
      "\n",
      "    Returns:\n",
      "      A batched MultivariateNormalDiag distribution with event shape\n",
      "      [latent_size], batch shape [..., batch_size], and sample shape\n",
      "      [sample_shape, ..., batch_size, latent_size].\n",
      "Generated :<s>©def call(self, inputs): collapsed_shape = tf.concat(([-1], tf.</s>\n",
      "---------------------------\n",
      "Code: def derived(self, name, relative_coords, formula): relZ, relN = relative_coords daughter_idx = [(x[0] + relZ, x[1] + relN) for x in self.df.index] values = formula(self.df.values, self.df.loc[daughter_idx].values) return Table(df=pd.Series(values, index=self.df.index, name=name + '(' + self.name + ')'))\n",
      "Target: Helper function for derived quantities\n",
      "Generated :<s> =):[self[].].()]self.self=self.self[self].].self.</s>\n",
      "---------------------------\n",
      "Code: def expand_aliases(self, fn, rest): line = fn + \" \" + rest done = set() while 1: pre,_,fn,rest = split_user_input(line, shell_line_split) if fn in self.alias_table: if fn in done: warn(\"Cyclic alias definition, repeated '%s'\" % fn) return \"\" done.add(fn) l2 = self.transform_alias(fn, rest) if l2 == line: break if l2.split(None,1)[0] == line.split(None,1)[0]: line = l2 break line=l2 else: break return line\n",
      "Target: Expand multiple levels of aliases:\n",
      "\n",
      "        if:\n",
      "\n",
      "        alias foo bar /tmp\n",
      "        alias baz foo\n",
      "\n",
      "        then:\n",
      "\n",
      "        baz huhhahhei -> bar /tmp huhhahhei\n",
      "Generated :<s><s>def expand_aliases(self, fn, rest): line = fn + \" \" + rest done =</s>\n",
      "---------------------------\n",
      "Code: def _maybe_validate_perm(perm, validate_args, name=None): with tf.name_scope(name or 'maybe_validate_perm'): assertions = [] if not dtype_util.is_integer(perm.dtype): raise TypeError('`perm` must be integer type') msg = '`perm` must be a vector.' if tensorshape_util.rank(perm.shape) is not None: if tensorshape_util.rank(perm.shape) != 1: raise ValueError( msg[:-1] + ', saw rank: {}.'.format(tensorshape_util.rank(perm.shape))) elif validate_args: assertions += [assert_util.assert_rank(perm, 1, message=msg)] perm_ = tf.get_static_value(perm) msg = '`perm` must be a valid permutation vector.' if perm_ is not None: if not np.all(np.arange(np.size(perm_)) == np.sort(perm_)): raise ValueError(msg[:-1] + ', saw: {}.'.format(perm_)) elif validate_args: assertions += [ assert_util.assert_equal( tf.sort(perm), tf.range(tf.size(input=perm)), message=msg) ] return assertions\n",
      "Target: Checks that `perm` is valid.\n",
      "Generated :<s><s>def _maybe_validate_perm(perm, validate_args, name=None): with tf.</s>\n",
      "---------------------------\n",
      "Code: def accept(self, node, **kwargs): if node is None: return for v in self.visitors: v.enter(node) name = 'accept_' + node.__class__.__name__ fn = getattr(self, name, self.default_accept) r = fn(node, **kwargs) for v in self.visitors: v.leave(node) return r\n",
      "Target: Invoke the visitors before and after decending down the tree. \n",
      "        The walker will also try to invoke a method matching the pattern \n",
      "        *accept_<type name>*, where <type name> is the name of the accepted\n",
      "        *node*.\n",
      "Generated :<s>:L.: v.leave(self, node, **kwargs): if node is None: return for</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def shutdown(self): if self._child: try: self._child.terminate() except OSError, exc: if exc.errno == 3: pass\n",
      "Target: send SIGTERM to the tagger child process\n",
      "Generated :<s>: the shutdown of the child.</s>\n",
      "---------------------------\n",
      "Code: def _get_conn_params(self): conn = self.get_connection(self.snowflake_conn_id) account = conn.extra_dejson.get('account', None) warehouse = conn.extra_dejson.get('warehouse', None) database = conn.extra_dejson.get('database', None) region = conn.extra_dejson.get(\"region\", None) role = conn.extra_dejson.get('role', None) conn_config = { \"user\": conn.login, \"password\": conn.password or '', \"schema\": conn.schema or '', \"database\": self.database or database or '', \"account\": self.account or account or '', \"warehouse\": self.warehouse or warehouse or '', \"region\": self.region or region or '', \"role\": self.role or role or '', } \"\"\" If private_key_file is specified in the extra json, load the contents of the file as a private key and specify that in the connection configuration. The connection password then becomes the passphrase for the private key. If your private key file is not encrypted (not recommended), then leave the password empty. \"\"\" private_key_file = conn.extra_dejson.get('private_key_file', None) if private_key_file: with open(private_key_file, \"rb\") as key: passphrase = None if conn.password: passphrase = conn.password.strip().encode() p_key = serialization.load_pem_private_key( key.read(), password=passphrase, backend=default_backend() ) pkb = p_key.private_bytes(encoding=serialization.Encoding.DER, format=serialization.PrivateFormat.PKCS8, encryption_algorithm=serialization.NoEncryption()) conn_config['private_key'] = pkb conn_config.pop('password', None) return conn_config\n",
      "Target: one method to fetch connection params as a dict\n",
      "        used in get_uri() and get_connection()\n",
      "Generated :<s>Get the connection parameters for the private key file.</s>\n",
      "---------------------------\n",
      "Code: def __fn_recur_to_py_ast(ctx: GeneratorContext, node: Recur) -> GeneratedPyAST: assert node.op == NodeOp.RECUR assert ctx.recur_point.is_variadic is not None recur_nodes: List[ast.AST] = [] recur_deps: List[ast.AST] = [] for expr in node.exprs: expr_ast = gen_py_ast(ctx, expr) recur_nodes.append(expr_ast.node) recur_deps.extend(expr_ast.dependencies) return GeneratedPyAST( node=ast.Call( func=_TRAMPOLINE_ARGS_FN_NAME, args=list( chain([ast.NameConstant(ctx.recur_point.is_variadic)], recur_nodes) ), keywords=[], ), dependencies=recur_deps, )\n",
      "Target: Return a Python AST node for `recur` occurring inside a `fn*`.\n",
      "Generated :<s>Generate py_ast to py_ast :param ctx: GeneratorContext :param node: Node instance :</s>\n",
      "---------------------------\n",
      "Code: def calculate_statistics(stat, frequencies): stats = ensure_list(stat) frequencies = ensure_list(frequencies) for stat in stats: for f in frequencies: print \"Calculating %s (%s)...\" % (stat.__name__, settings.STATISTIC_FREQUENCY_DICT[f]) stat.calculate(f)\n",
      "Target: Calculates all of the metrics associated with the registered gadgets.\n",
      "Generated :<s>Compculate a stat's frequencies.</s>\n",
      "---------------------------\n",
      "Code: def save_to_file(self, path, filename, **params): url = ensure_trailing_slash(self.url + path.lstrip('/')) content = self._request('get', url, params=params).content with open(filename, 'wb') as f: f.write(content)\n",
      "Target: Saves binary content to a file with name filename. filename should\n",
      "        include the appropriate file extension, such as .xlsx or .txt, e.g.,\n",
      "        filename = 'sample.xlsx'.\n",
      "\n",
      "        Useful for downloading .xlsx files.\n",
      "Generated :<s>Save a file. :param path: path to file :type path: str</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def quadrature_scheme_softmaxnormal_gauss_hermite( normal_loc, normal_scale, quadrature_size, validate_args=False, name=None): with tf.name_scope( name or \"quadrature_scheme_softmaxnormal_gauss_hermite\"): normal_loc = tf.convert_to_tensor(value=normal_loc, name=\"normal_loc\") npdt = dtype_util.as_numpy_dtype(normal_loc.dtype) normal_scale = tf.convert_to_tensor( value=normal_scale, dtype=npdt, name=\"normal_scale\") normal_scale = maybe_check_quadrature_param( normal_scale, \"normal_scale\", validate_args) grid, probs = np.polynomial.hermite.hermgauss(deg=quadrature_size) grid = grid.astype(npdt) probs = probs.astype(npdt) probs /= np.linalg.norm(probs, ord=1, keepdims=True) probs = tf.convert_to_tensor(value=probs, name=\"probs\", dtype=npdt) grid = softmax( -distribution_util.pad( (normal_loc[..., tf.newaxis] + np.sqrt(2.) * normal_scale[..., tf.newaxis] * grid), axis=-2, front=True), axis=-2) return grid, probs\n",
      "Target: Use Gauss-Hermite quadrature to form quadrature on `K - 1` simplex.\n",
      "\n",
      "  A `SoftmaxNormal` random variable `Y` may be generated via\n",
      "\n",
      "  ```\n",
      "  Y = SoftmaxCentered(X),\n",
      "  X = Normal(normal_loc, normal_scale)\n",
      "  ```\n",
      "\n",
      "  Note: for a given `quadrature_size`, this method is generally less accurate\n",
      "  than `quadrature_scheme_softmaxnormal_quantiles`.\n",
      "\n",
      "  Args:\n",
      "    normal_loc: `float`-like `Tensor` with shape `[b1, ..., bB, K-1]`, B>=0.\n",
      "      The location parameter of the Normal used to construct the SoftmaxNormal.\n",
      "    normal_scale: `float`-like `Tensor`. Broadcastable with `normal_loc`.\n",
      "      The scale parameter of the Normal used to construct the SoftmaxNormal.\n",
      "    quadrature_size: Python `int` scalar representing the number of quadrature\n",
      "      points.\n",
      "    validate_args: Python `bool`, default `False`. When `True` distribution\n",
      "      parameters are checked for validity despite possibly degrading runtime\n",
      "      performance. When `False` invalid inputs may silently render incorrect\n",
      "      outputs.\n",
      "    name: Python `str` name prefixed to Ops created by this class.\n",
      "\n",
      "  Returns:\n",
      "    grid: Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the\n",
      "      convex combination of affine parameters for `K` components.\n",
      "      `grid[..., :, n]` is the `n`-th grid point, living in the `K - 1` simplex.\n",
      "    probs:  Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the\n",
      "      associated with each grid point.\n",
      "Generated :<s>Calculate quadrature_scheme_softmaxnormal_gauss_hermite Parameters ---------- normal</s>\n",
      "---------------------------\n",
      "Code: def _to_install(self): order = [] ordered_reqs = set() def schedule(req): if req.satisfied_by or req in ordered_reqs: return ordered_reqs.add(req) for dep in self._dependencies[req]: schedule(dep) order.append(req) for install_req in self.requirements.values(): schedule(install_req) return order\n",
      "Target: Create the installation order.\n",
      "\n",
      "        The installation order is topological - requirements are installed\n",
      "        before the requiring thing. We break cycles at an arbitrary point,\n",
      "        and make no other guarantees.\n",
      "Generated :<s>Return a list of all available install requests.</s>\n",
      "---------------------------\n",
      "Code: def output_json(self, file_name='/tmp/ND.json'): complete_example = ( self.dataset, self.project, self.channels, self.metadata) data = json.loads(self.nd_json(*complete_example)) self.verify_path(data, VERIFY_BY_SLICE) f = open(file_name, 'w') f.write(str(data)) f.close()\n",
      "Target: Arguments:\n",
      "            file_name(str : '/tmp/ND.json'): The file name to store the json to\n",
      "\n",
      "        Returns:\n",
      "            None\n",
      "Generated :<s>Output a JSON file to disk. Args: file_name (str): Name of the file. Returns:</s>\n",
      "---------------------------\n",
      "Code: def create_hosted_zone_writer(connection, name, caller_reference, comment): if not caller_reference: caller_reference = str(uuid.uuid4()) e_root = etree.Element( \"CreateHostedZoneRequest\", xmlns=connection._xml_namespace ) e_name = etree.SubElement(e_root, \"Name\") e_name.text = name e_caller_reference = etree.SubElement(e_root, \"CallerReference\") e_caller_reference.text = caller_reference if comment: e_config = etree.SubElement(e_root, \"HostedZoneConfig\") e_comment = etree.SubElement(e_config, \"Comment\") e_comment.text = comment e_tree = etree.ElementTree(element=e_root) fobj = BytesIO() e_tree.write(fobj, xml_declaration=True, encoding='utf-8', method=\"xml\") return fobj.getvalue().decode('utf-8')\n",
      "Target: Forms an XML string that we'll send to Route53 in order to create\n",
      "    a new hosted zone.\n",
      "\n",
      "    :param Route53Connection connection: The connection instance used to\n",
      "        query the API.\n",
      "    :param str name: The name of the hosted zone to create.\n",
      "Generated :<s>Create a hosted zone. :param connection: The connection to create. :param name: The name of the</s>\n",
      "---------------------------\n",
      "Code: def src(filename): if filename is None: return filename if sys.platform.startswith('java') and filename.endswith('$py.class'): return '.'.join((filename[:-9], 'py')) base, ext = os.path.splitext(filename) if ext in ('.pyc', '.pyo', '.py'): return '.'.join((base, 'py')) return filename\n",
      "Target: Find the python source file for a .pyc, .pyo or $py.class file on\n",
      "    jython. Returns the filename provided if it is not a python source\n",
      "    file.\n",
      "Generated :<s>Return the filename of the given file.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def send(evt): msg = ctrl_input.value gui.alert(msg, \"Message\") log(msg) ctrl_input.value = \"\" ctrl_input.set_focus()\n",
      "Target: Process an outgoing communication\n",
      "Generated :<s>Sends a message to the user.</s>\n",
      "---------------------------\n",
      "Code: def list_to_tf_input(data, response_index, num_outcomes): matrix = np.matrix([row[:response_index] + row[response_index+1:] for row in data]) outcomes = np.asarray([row[response_index] for row in data], dtype=np.uint8) outcomes_onehot = (np.arange(num_outcomes) == outcomes[:, None]).astype(np.float32) return matrix, outcomes_onehot\n",
      "Target: Separates the outcome feature from the data and creates the onehot vector for each row.\n",
      "Generated :<s>:param data: :param response_index: :param num_outcomes:</s>\n",
      "---------------------------\n",
      "Code: def radial_flare2(script, flare_radius=None, start_radius=None, end_radius=None, end_height=None): if (end_radius is not None) and (end_height is not None): if (end_radius - start_radius) < end_height: flare_radius = -((start_radius-end_radius)**2 + end_height**2)/(2*(start_radius-end_radius)) else: print('Error, end_radius is too large for end_height; angle is > 90d') r_func = 'if(z>0, (r) + (flare_radius) - (flare_radius)*sqrt(1-z^2/(flare_radius)^2), (r))'.replace('flare_radius', str(flare_radius)) function_cyl_co(script, r_func) return None\n",
      "Target: flare_radius must be >= end_height (height)\n",
      "    end_radius max = flare_radius + r\n",
      "    \n",
      "    end_radius (num): radius of mesh at end of flare\n",
      "    \n",
      "    +15 r= 8.8205\n",
      "    -15 r= 1.1795\n",
      "    \n",
      "    z=10, 5 +/-15 - +/-15*0.74535599249992989880305788957709\n",
      "Generated :<s>Calculate the radius of a circle. :param script: :param flare_radius: :param start_</s>\n",
      "---------------------------\n",
      "Code: def parallel(collection, method, processes=None, args=None, **kwargs): if processes is None: processes = min(mp.cpu_count(), 20) print \"Running parallel process on \" + str(processes) + \" cores. :-)\" pool = mp.Pool(processes=processes) PROC = [] tic = time.time() for main_arg in collection: if args is None: ARGS = (main_arg,) else: if isinstance(args, tuple) == False: args = (args,) ARGS = (main_arg,) + args PROC.append(pool.apply_async(method, args=ARGS, kwds=kwargs)) RES = [] for p in PROC: try: RES.append(p.get()) except Exception as e: print \"shit happens...\" print e RES.append(None) pool.close() pool.join() toc = time.time() elapsed = toc - tic print \"Elapsed time: %s on %s processes :-)\\n\" % (str(elapsed), str(processes)) return RES\n",
      "Target: Processes a collection in parallel.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    collection : list\n",
      "        i.e. list of Record objects\n",
      "    method : method to call on each Record\n",
      "    processes : int\n",
      "        number of processes to run on [defaults to number of cores on machine]\n",
      "    batch_size : int\n",
      "        lenght of each batch [defaults to number of elements / number of processes]\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    collection : list\n",
      "        list of Record objects after going through method called\n",
      "\n",
      "    Example\n",
      "    -------\n",
      "    adding 2 to every number in a range\n",
      "\n",
      "    >>> import turntable\n",
      "    >>> collection = range(100)\n",
      "    >>> def jam(record):\n",
      "    >>>     return record + 2\n",
      "    >>> collection = turntable.spin.parallel(collection, jam)\n",
      "\n",
      "    Note\n",
      "    ----\n",
      "\n",
      "    lambda functions do not work in parallel\n",
      "Generated :<s>Generate a parallel process on the specified collection.</s>\n",
      "---------------------------\n",
      "Code: def is_virtual(value): return value is None or isinstance(value, tuple) and len(value) == 2 and isinstance( value[0], Register) and isinstance(value[1], int)\n",
      "Target: Checks if value has the format of a virtual qubit\n",
      "Generated :<s>Check if the value is a virtual value.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def round_accuracy(y_true, y_predicted): predictions = [round(x) for x in y_predicted] examples_len = len(y_true) correct = sum([y1 == y2 for y1, y2 in zip(y_true, predictions)]) return correct / examples_len if examples_len else 0\n",
      "Target: Rounds predictions and calculates accuracy in terms of absolute coincidence.\n",
      "\n",
      "    Args:\n",
      "        y_true: list of true values\n",
      "        y_predicted: list of predicted values\n",
      "\n",
      "    Returns:\n",
      "        portion of absolutely coincidental samples\n",
      "Generated :<s>r\"\"\"Calculate the round accuracy of the given predictions. Parameters ---------- y_true : array-like</s>\n",
      "---------------------------\n",
      "Code: def add_parameters(traj): traj.f_add_parameter('steps', 10000, comment='Number of time steps to simulate') traj.f_add_parameter('dt', 0.01, comment='Step size') traj.f_add_parameter(ArrayParameter,'initial_conditions', np.array([0.0,0.0,0.0]), comment = 'Our initial conditions, as default we will start from' ' origin!') traj.f_add_parameter('diff_name','diff_lorenz', comment= 'Name of our differential equation') if traj.diff_name == 'diff_lorenz': traj.f_add_parameter('func_params.sigma', 10.0) traj.f_add_parameter('func_params.beta', 8.0/3.0) traj.f_add_parameter('func_params.rho', 28.0) elif traj.diff_name == 'diff_roessler': traj.f_add_parameter('func_params.a', 0.1) traj.f_add_parameter('func_params.c', 14.0) else: raise ValueError('I don\\'t know what %s is.' % traj.diff_name)\n",
      "Target: Adds all necessary parameters to the `traj` container.\n",
      "\n",
      "    You can choose between two parameter sets. One for the Lorenz attractor and\n",
      "    one for the Roessler attractor.\n",
      "    The former is chosen for `traj.diff_name=='diff_lorenz'`, the latter for\n",
      "    `traj.diff_name=='diff_roessler'`.\n",
      "    You can use parameter presetting to switch between the two cases.\n",
      "\n",
      "    :raises: A ValueError if `traj.diff_name` is none of the above\n",
      "Generated :<s>Adds parameters to traj. Parameters ---------- traj : numpy.ndarray, shape=(n_s</s>\n",
      "---------------------------\n",
      "Code: def quick_completer(cmd, completions): if isinstance(completions, basestring): completions = completions.split() def do_complete(self, event): return completions get_ipython().set_hook('complete_command',do_complete, str_key = cmd)\n",
      "Target: Easily create a trivial completer for a command.\n",
      "\n",
      "    Takes either a list of completions, or all completions in string (that will\n",
      "    be split on whitespace).\n",
      "\n",
      "    Example::\n",
      "\n",
      "        [d:\\ipython]|1> import ipy_completers\n",
      "        [d:\\ipython]|2> ipy_completers.quick_completer('foo', ['bar','baz'])\n",
      "        [d:\\ipython]|3> foo b<TAB>\n",
      "        bar baz\n",
      "        [d:\\ipython]|3> foo ba\n",
      "Generated :<s>Execute a quick completions command :param cmd: the command to execute :param completions: the completions</s>\n",
      "---------------------------\n",
      "Code: def random_von_mises(shape, concentration, dtype=tf.float32, seed=None): seed = SeedStream(seed, salt=\"von_mises\") concentration = tf.convert_to_tensor( value=concentration, dtype=dtype, name=\"concentration\") @tf.custom_gradient def rejection_sample_with_gradient(concentration): r = 1. + tf.sqrt(1. + 4. * concentration ** 2) rho = (r - tf.sqrt(2. * r)) / (2. * concentration) s_exact = (1. + rho ** 2) / (2. * rho) s_approximate = 1. / concentration s_concentration_cutoff_dict = { tf.float16: 1.8e-1, tf.float32: 2e-2, tf.float64: 1.2e-4, } s_concentration_cutoff = s_concentration_cutoff_dict[dtype] s = tf.where(concentration > s_concentration_cutoff, s_exact, s_approximate) def loop_body(done, u, w): u = tf.random.uniform( shape, minval=-1., maxval=1., dtype=dtype, seed=seed()) z = tf.cos(np.pi * u) w = tf.where(done, w, (1. + s * z) / (s + z)) y = concentration * (s - w) v = tf.random.uniform( shape, minval=0., maxval=1., dtype=dtype, seed=seed()) accept = (y * (2. - y) >= v) | (tf.math.log(y / v) + 1. >= y) return done | accept, u, w _, u, w = tf.while_loop( cond=lambda done, *_: ~tf.reduce_all(input_tensor=done), body=loop_body, loop_vars=( tf.zeros(shape, dtype=tf.bool, name=\"done\"), tf.zeros(shape, dtype=dtype, name=\"u\"), tf.zeros(shape, dtype=dtype, name=\"w\"), ), maximum_iterations=100, parallel_iterations=1 if seed.original_seed is None else 10, ) x = tf.sign(u) * tf.math.acos(w) def grad(dy): broadcast_concentration = concentration + tf.zeros_like(x) _, dcdf_dconcentration = value_and_gradient( lambda conc: von_mises_cdf(x, conc), broadcast_concentration) inv_prob = tf.exp(-broadcast_concentration * (tf.cos(x) - 1.)) * ( (2. * np.pi) * tf.math.bessel_i0e(broadcast_concentration)) ret = dy * (-inv_prob * dcdf_dconcentration) num_sample_dimensions = (tf.rank(broadcast_concentration) - tf.rank(concentration)) return tf.reduce_sum( input_tensor=ret, axis=tf.range(num_sample_dimensions)) return x, grad return rejection_sample_with_gradient(concentration)\n",
      "Target: Samples from the standardized von Mises distribution.\n",
      "\n",
      "  The distribution is vonMises(loc=0, concentration=concentration), so the mean\n",
      "  is zero.\n",
      "  The location can then be changed by adding it to the samples.\n",
      "\n",
      "  The sampling algorithm is rejection sampling with wrapped Cauchy proposal [1].\n",
      "  The samples are pathwise differentiable using the approach of [2].\n",
      "\n",
      "  Arguments:\n",
      "    shape: The output sample shape.\n",
      "    concentration: The concentration parameter of the von Mises distribution.\n",
      "    dtype: The data type of concentration and the outputs.\n",
      "    seed: (optional) The random seed.\n",
      "\n",
      "  Returns:\n",
      "    Differentiable samples of standardized von Mises.\n",
      "\n",
      "  References:\n",
      "    [1] Luc Devroye \"Non-Uniform Random Variate Generation\", Springer-Verlag,\n",
      "    1986; Chapter 9, p. 473-476.\n",
      "    http://www.nrbook.com/devroye/Devroye_files/chapter_nine.pdf\n",
      "    + corrections http://www.nrbook.com/devroye/Devroye_files/errors.pdf\n",
      "    [2] Michael Figurnov, Shakir Mohamed, Andriy Mnih. \"Implicit\n",
      "    Reparameterization Gradients\", 2018.\n",
      "Generated :<s>Generates randomvon mises. Parameters ---------- shape : :py:class:`tf.tf.tf</s>\n",
      "---------------------------\n",
      "Code: def set_up(self): self.menu.pause() curses.def_prog_mode() self.menu.clear_screen()\n",
      "Target: This class overrides this method\n",
      "Generated :<s>Set up the menu.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def send(self): url = ( \"https://api.twilio.com/2010-04-01/Accounts/\" + self._auth[0] + \"/Messages.json\" ) data = { \"From\": self.from_, \"To\": self.to, \"Body\": self.body, \"MediaUrl\": self.attachments, } if self.verbose: print( \"Debugging info\" \"\\n--------------\" \"\\n{} Message created.\".format(timestamp()) ) try: resp = requests.post(url, data=data, auth=(self._auth[0], self._auth[1])) resp.raise_for_status() except requests.exceptions.RequestException as e: exc = \"{}\\n{}\".format(e, resp.json()[\"message\"]) raise MessageSendError(exc) self.sid = resp.json()[\"sid\"] if self.verbose: print( timestamp(), type(self).__name__ + \" info:\", self.__str__(indentation=\"\\n * \"), \"\\n * HTTP status code:\", resp.status_code, ) print(\"Message sent.\") return resp\n",
      "Target: Send the SMS/MMS message.\n",
      "        Set self.sid to return code of message.\n",
      "Generated :<s>Sends a message to the Twilio API</s>\n",
      "---------------------------\n",
      "Code: def spherical_vert(script, radius=1.0, center_pt=(0.0, 0.0, 0.0)): function = 'sqrt((x-{})^2+(y-{})^2+(z-{})^2)<={}'.format( center_pt[0], center_pt[1], center_pt[2], radius) vert_function(script, function=function) return None\n",
      "Target: Select all vertices within a spherical radius\n",
      "\n",
      "    Args:\n",
      "        radius (float): radius of the sphere\n",
      "        center_pt (3 coordinate tuple or list): center point of the sphere\n",
      "\n",
      "    Layer stack:\n",
      "        No impacts\n",
      "\n",
      "    MeshLab versions:\n",
      "        2016.12\n",
      "        1.3.4BETA\n",
      "Generated :<s>:param script: :param radius: :param center_pt: :return:</s>\n",
      "---------------------------\n",
      "Code: def transformProperMotions(self, phi, theta, muphistar, mutheta): c, s = self._getJacobian(phi,theta) return c*muphistar+s*mutheta, c*mutheta-s*muphistar\n",
      "Target: Converts proper motions from one reference system to another, using the prescriptions in section\n",
      "        1.5.3 of the Hipparcos Explanatory Volume 1 (equations 1.5.18, 1.5.19).\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "\n",
      "        phi       - The longitude-like angle of the position of the source (radians).\n",
      "        theta     - The latitude-like angle of the position of the source (radians).\n",
      "        muphistar - Value of the proper motion in the longitude-like angle, multiplied by cos(latitude).\n",
      "        mutheta   - Value of the proper motion in the latitude-like angle.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "\n",
      "        muphistarrot - Value of the transformed proper motion in the longitude-like angle (including the\n",
      "        cos(latitude) factor).\n",
      "        muthetarot   - Value of the transformed proper motion in the latitude-like angle.\n",
      "Generated :<s>TransformProperMotions :param phi: :param theta: :param muphistar: :param</s>\n",
      "---------------------------\n",
      "Code: def _get_range_session(self, start=1, stop=None, raw=True, output=False): input_hist = self.input_hist_raw if raw else self.input_hist_parsed n = len(input_hist) if start < 0: start += n if not stop or (stop > n): stop = n elif stop < 0: stop += n for i in range(start, stop): if output: line = (input_hist[i], self.output_hist_reprs.get(i)) else: line = input_hist[i] yield (0, i, line)\n",
      "Target: Get input and output history from the current session. Called by\n",
      "        get_range, and takes similar parameters.\n",
      "Generated :<s>Yields the range from the input hist. :param start: The start of the range. :param stop</s>\n",
      "---------------------------\n",
      "Code: def search_dict(data, key): if isinstance(data, dict): for dkey, value in data.items(): if dkey == key: yield value for result in search_dict(value, key): yield result elif isinstance(data, list): for value in data: for result in search_dict(value, key): yield result\n",
      "Target: Search for a key in a nested dict, or list of nested dicts, and return the values.\n",
      "\n",
      "    :param data: dict/list to search\n",
      "    :param key: key to find\n",
      "    :return: matches for key\n",
      "Generated :<s>Yields a list of dictionaries for the given key.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def mode(self, expression, binby=[], limits=None, shape=256, mode_shape=64, mode_limits=None, progressbar=False, selection=None): if len(binby) == 0: raise ValueError(\"only supported with binby argument given\") else: try: len(shape) shape = tuple(shape) except: shape = len(binby) * (shape,) shape = (mode_shape,) + shape subspace = self(*(list(binby) + [expression])) if selection: subspace = subspace.selected() limits = self.limits(list(binby), limits) mode_limits = self.limits([expression], mode_limits) limits = list(limits) + list(mode_limits) counts = subspace.histogram(limits=limits, size=shape, progressbar=progressbar) indices = np.argmax(counts, axis=0) pmin, pmax = limits[-1] centers = np.linspace(pmin, pmax, mode_shape + 1)[:-1] centers += (centers[1] - centers[0]) / 2 modes = centers[indices] ok = counts.sum(axis=0) > 0 modes[~ok] = np.nan return modes\n",
      "Target: Calculate/estimate the mode.\n",
      "Generated :<s>Compute the mode of an expression. Parameters ---------- expression : :class:`numpy.ndarray`</s>\n",
      "---------------------------\n",
      "Code: def transform(self, Z): if isinstance(Z, DictRDD): X = Z[:, 'X'] else: X = Z Zs = [_transform_one(trans, name, X, self.transformer_weights) for name, trans in self.transformer_list] X_rdd = reduce(lambda x, y: x.zip(y._rdd), Zs) X_rdd = X_rdd.map(flatten) mapper = np.hstack for item in X_rdd.first(): if sp.issparse(item): mapper = sp.hstack X_rdd = X_rdd.map(lambda x: mapper(x)) if isinstance(Z, DictRDD): return DictRDD([X_rdd, Z[:, 'y']], columns=Z.columns, dtype=Z.dtype, bsize=Z.bsize) else: return X_rdd\n",
      "Target: TODO: rewrite docstring\n",
      "        Transform X separately by each transformer, concatenate results.\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "            Input data to be transformed.\n",
      "        Returns\n",
      "        -------\n",
      "        X_t : array-like or sparse matrix, shape (n_samples, sum_n_components)\n",
      "            hstack of results of transformers. sum_n_components is the\n",
      "            sum of n_components (output dimension) over transformers.\n",
      "Generated :<s>Given a DictRDD, transform the RDD. Parameters ---------- Z : ndarray DictR</s>\n",
      "---------------------------\n",
      "Code: def find_packages(top=HERE): packages = [] for d, dirs, _ in os.walk(top, followlinks=True): if os.path.exists(pjoin(d, '__init__.py')): packages.append(os.path.relpath(d, top).replace(os.path.sep, '.')) elif d != top: dirs[:] = [] return packages\n",
      "Target: Find all of the packages.\n",
      "Generated :<s>Returns a list of all packages in the top directory.</s>\n",
      "---------------------------\n",
      "Code: def output_dims(self, qargs=None): if qargs is None: return self._output_dims return tuple(self._output_dims[i] for i in qargs)\n",
      "Target: Return tuple of output dimension for specified subsystems.\n",
      "Generated :<s>:param qargs: :type qargs: list :return: :rtype: tuple</s>\n",
      "---------------------------\n",
      "Code: def write_data(msg_type, profile_name, data, cfg): if profile_name not in cfg.data: cfg.data[profile_name] = {} cfg.data[profile_name][msg_type] = data\n",
      "Target: Write the settings into the data portion of the cfg.\n",
      "\n",
      "    Args:\n",
      "        :msg_type: (str) message type to create config entry.\n",
      "        :profile_name: (str) name of the profile entry\n",
      "        :data: (dict) dict values for the 'settings'\n",
      "        :cfg: (jsonconfig.Config) config instance.\n",
      "Generated :<s>write_data(msg_type, profile_name, data, cfg.data)</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def add_children(G, parent, level, n=2): if level == 0: return for i in range(n): child = parent+str(i) G.add_node(child) G.add_edge(parent,child) add_children(G, child, level-1, n)\n",
      "Target: Add children recursively to a binary tree.\n",
      "Generated :<s>Add children of a node to a graph. Parameters ---------- G : graph The graph to be added. parent :</s>\n",
      "---------------------------\n",
      "Code: def fmt(y, t_min=0.5, n_fmt=None, kind='cubic', beta=0.5, over_sample=1, axis=-1): n = y.shape[axis] if n < 3: raise ParameterError('y.shape[{:}]=={:} < 3'.format(axis, n)) if t_min <= 0: raise ParameterError('t_min must be a positive number') if n_fmt is None: if over_sample < 1: raise ParameterError('over_sample must be >= 1') log_base = np.log(n - 1) - np.log(n - 2) n_fmt = int(np.ceil(over_sample * (np.log(n - 1) - np.log(t_min)) / log_base)) elif n_fmt < 3: raise ParameterError('n_fmt=={:} < 3'.format(n_fmt)) else: log_base = (np.log(n_fmt - 1) - np.log(n_fmt - 2)) / over_sample if not np.all(np.isfinite(y)): raise ParameterError('y must be finite everywhere') base = np.exp(log_base) x = np.linspace(0, 1, num=n, endpoint=False) f_interp = scipy.interpolate.interp1d(x, y, kind=kind, axis=axis) n_over = int(np.ceil(over_sample)) x_exp = np.logspace((np.log(t_min) - np.log(n)) / log_base, 0, num=n_fmt + n_over, endpoint=False, base=base)[:-n_over] if x_exp[0] < t_min or x_exp[-1] > float(n - 1.0) / n: x_exp = np.clip(x_exp, float(t_min) / n, x[-1]) if len(np.unique(x_exp)) != len(x_exp): raise RuntimeError('Redundant sample positions in Mellin transform') y_res = f_interp(x_exp) shape = [1] * y_res.ndim shape[axis] = -1 fft = get_fftlib() return fft.rfft(y_res * ((x_exp**beta).reshape(shape) * np.sqrt(n) / n_fmt), axis=axis)\n",
      "Target: The fast Mellin transform (FMT) [1]_ of a uniformly sampled signal y.\n",
      "\n",
      "    When the Mellin parameter (beta) is 1/2, it is also known as the scale transform [2]_.\n",
      "    The scale transform can be useful for audio analysis because its magnitude is invariant\n",
      "    to scaling of the domain (e.g., time stretching or compression).  This is analogous\n",
      "    to the magnitude of the Fourier transform being invariant to shifts in the input domain.\n",
      "\n",
      "\n",
      "    .. [1] De Sena, Antonio, and Davide Rocchesso.\n",
      "        \"A fast Mellin and scale transform.\"\n",
      "        EURASIP Journal on Applied Signal Processing 2007.1 (2007): 75-75.\n",
      "\n",
      "    .. [2] Cohen, L.\n",
      "        \"The scale representation.\"\n",
      "        IEEE Transactions on Signal Processing 41, no. 12 (1993): 3275-3292.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    y : np.ndarray, real-valued\n",
      "        The input signal(s).  Can be multidimensional.\n",
      "        The target axis must contain at least 3 samples.\n",
      "\n",
      "    t_min : float > 0\n",
      "        The minimum time spacing (in samples).\n",
      "        This value should generally be less than 1 to preserve as much information as\n",
      "        possible.\n",
      "\n",
      "    n_fmt : int > 2 or None\n",
      "        The number of scale transform bins to use.\n",
      "        If None, then `n_bins = over_sample * ceil(n * log((n-1)/t_min))` is taken,\n",
      "        where `n = y.shape[axis]`\n",
      "\n",
      "    kind : str\n",
      "        The type of interpolation to use when re-sampling the input.\n",
      "        See `scipy.interpolate.interp1d` for possible values.\n",
      "\n",
      "        Note that the default is to use high-precision (cubic) interpolation.\n",
      "        This can be slow in practice; if speed is preferred over accuracy,\n",
      "        then consider using `kind='linear'`.\n",
      "\n",
      "    beta : float\n",
      "        The Mellin parameter.  `beta=0.5` provides the scale transform.\n",
      "\n",
      "    over_sample : float >= 1\n",
      "        Over-sampling factor for exponential resampling.\n",
      "\n",
      "    axis : int\n",
      "        The axis along which to transform `y`\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    x_scale : np.ndarray [dtype=complex]\n",
      "        The scale transform of `y` along the `axis` dimension.\n",
      "\n",
      "    Raises\n",
      "    ------\n",
      "    ParameterError\n",
      "        if `n_fmt < 2` or `t_min <= 0`\n",
      "        or if `y` is not finite\n",
      "        or if `y.shape[axis] < 3`.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    This function caches at level 30.\n",
      "\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> # Generate a signal and time-stretch it (with energy normalization)\n",
      "    >>> scale = 1.25\n",
      "    >>> freq = 3.0\n",
      "    >>> x1 = np.linspace(0, 1, num=1024, endpoint=False)\n",
      "    >>> x2 = np.linspace(0, 1, num=scale * len(x1), endpoint=False)\n",
      "    >>> y1 = np.sin(2 * np.pi * freq * x1)\n",
      "    >>> y2 = np.sin(2 * np.pi * freq * x2) / np.sqrt(scale)\n",
      "    >>> # Verify that the two signals have the same energy\n",
      "    >>> np.sum(np.abs(y1)**2), np.sum(np.abs(y2)**2)\n",
      "        (255.99999999999997, 255.99999999999969)\n",
      "    >>> scale1 = librosa.fmt(y1, n_fmt=512)\n",
      "    >>> scale2 = librosa.fmt(y2, n_fmt=512)\n",
      "    >>> # And plot the results\n",
      "    >>> import matplotlib.pyplot as plt\n",
      "    >>> plt.figure(figsize=(8, 4))\n",
      "    >>> plt.subplot(1, 2, 1)\n",
      "    >>> plt.plot(y1, label='Original')\n",
      "    >>> plt.plot(y2, linestyle='--', label='Stretched')\n",
      "    >>> plt.xlabel('time (samples)')\n",
      "    >>> plt.title('Input signals')\n",
      "    >>> plt.legend(frameon=True)\n",
      "    >>> plt.axis('tight')\n",
      "    >>> plt.subplot(1, 2, 2)\n",
      "    >>> plt.semilogy(np.abs(scale1), label='Original')\n",
      "    >>> plt.semilogy(np.abs(scale2), linestyle='--', label='Stretched')\n",
      "    >>> plt.xlabel('scale coefficients')\n",
      "    >>> plt.title('Scale transform magnitude')\n",
      "    >>> plt.legend(frameon=True)\n",
      "    >>> plt.axis('tight')\n",
      "    >>> plt.tight_layout()\n",
      "\n",
      "    >>> # Plot the scale transform of an onset strength autocorrelation\n",
      "    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n",
      "    ...                      offset=10.0, duration=30.0)\n",
      "    >>> odf = librosa.onset.onset_strength(y=y, sr=sr)\n",
      "    >>> # Auto-correlate with up to 10 seconds lag\n",
      "    >>> odf_ac = librosa.autocorrelate(odf, max_size=10 * sr // 512)\n",
      "    >>> # Normalize\n",
      "    >>> odf_ac = librosa.util.normalize(odf_ac, norm=np.inf)\n",
      "    >>> # Compute the scale transform\n",
      "    >>> odf_ac_scale = librosa.fmt(librosa.util.normalize(odf_ac), n_fmt=512)\n",
      "    >>> # Plot the results\n",
      "    >>> plt.figure()\n",
      "    >>> plt.subplot(3, 1, 1)\n",
      "    >>> plt.plot(odf, label='Onset strength')\n",
      "    >>> plt.axis('tight')\n",
      "    >>> plt.xlabel('Time (frames)')\n",
      "    >>> plt.xticks([])\n",
      "    >>> plt.legend(frameon=True)\n",
      "    >>> plt.subplot(3, 1, 2)\n",
      "    >>> plt.plot(odf_ac, label='Onset autocorrelation')\n",
      "    >>> plt.axis('tight')\n",
      "    >>> plt.xlabel('Lag (frames)')\n",
      "    >>> plt.xticks([])\n",
      "    >>> plt.legend(frameon=True)\n",
      "    >>> plt.subplot(3, 1, 3)\n",
      "    >>> plt.semilogy(np.abs(odf_ac_scale), label='Scale transform magnitude')\n",
      "    >>> plt.axis('tight')\n",
      "    >>> plt.xlabel('scale coefficients')\n",
      "    >>> plt.legend(frameon=True)\n",
      "    >>> plt.tight_layout()\n",
      "Generated :<s>fmt(y, t_min=0.5, n_fmt=1, kind='c</s>\n",
      "---------------------------\n",
      "Code: def get_slider_items(context, amount=None): req = context.get('request') qs = SliderItem.objects.published(req).order_by('position') if amount: qs = qs[:amount] return qs\n",
      "Target: Returns the published slider items.\n",
      "Generated :<s>Return a list of SliderItem objects.</s>\n",
      "---------------------------\n",
      "Code: def create(self, path, data=None): return self.handleresult(self.r.post(urljoin(self.url + CRUD_PATH, path), data=json.dumps(data)))\n",
      "Target: Send a POST CRUD API request to the given path using the given data which will be converted\n",
      "        to json\n",
      "Generated :<s>Create a CRUD file. :param path: CRUD path. :type path: str :param data:</s>\n",
      "---------------------------\n",
      "Code: def flat_matches(self, key): for val in self.dispatch(key): for el in val: yield el[1] return\n",
      "Target: Yield all 'value' targets, without priority\n",
      "Generated :<s>Returns True if key is a flat match, False otherwise.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def remove_virtual_meta(self): dir = self.get_private_dir(create=True) path = os.path.join(dir, \"virtual_meta.yaml\") try: if os.path.exists(path): os.remove(path) if not os.listdir(dir): os.rmdir(dir) except: logger.exception(\"error while trying to remove %s or %s\", path, dir)\n",
      "Target: Removes the file with the virtual column etc, it does not change the current virtual columns etc.\n",
      "Generated :<s>Remove virtual_meta.yaml.yaml</s>\n",
      "---------------------------\n",
      "Code: def parse_genelists(self): if isinstance(self.gene_list, list): genes = self.gene_list elif isinstance(self.gene_list, pd.DataFrame): if self.gene_list.shape[1] >=3: genes= self.gene_list.iloc[:,:3].apply(lambda x: \"\\t\".join([str(i) for i in x]), axis=1).tolist() elif self.gene_list.shape[1] == 2: genes= self.gene_list.apply(lambda x: \",\".join([str(i) for i in x]), axis=1).tolist() else: genes = self.gene_list.squeeze().tolist() elif isinstance(self.gene_list, pd.Series): genes = self.gene_list.squeeze().tolist() else: genes=[] with open(self.gene_list) as f: for gene in f: genes.append(gene.strip()) self._isezid = all(map(self._is_entrez_id, genes)) if self._isezid: self._gls = set(map(int, self._gls)) else: self._gls = genes return '\\n'.join(genes)\n",
      "Target: parse gene list\n",
      "Generated :<s>Parse the gene list and return it as a string.</s>\n",
      "---------------------------\n",
      "Code: def get_table_location(self, database_name, table_name): table = self.get_table(database_name, table_name) return table['StorageDescriptor']['Location']\n",
      "Target: Get the physical location of the table\n",
      "\n",
      "        :param database_name: Name of hive database (schema) @table belongs to\n",
      "        :type database_name: str\n",
      "        :param table_name: Name of hive table\n",
      "        :type table_name: str\n",
      "        :return: str\n",
      "Generated :<s>:param database_name: database name :param table_name: table name :return: table location</s>\n",
      "---------------------------\n",
      "Code: def get_spark_context(conf=None): if hasattr(SparkContext, \"getOrCreate\"): with SparkContext._lock: if SparkContext._active_spark_context is None: spark_conf = create_spark_conf() if conf is None else conf return SparkContext.getOrCreate(spark_conf) else: return SparkContext.getOrCreate() else: if SparkContext._active_spark_context is None: spark_conf = create_spark_conf() if conf is None else conf return SparkContext(conf=spark_conf) else: return SparkContext._active_spark_context\n",
      "Target: Get the current active spark context and create one if no active instance\n",
      "    :param conf: combining bigdl configs into spark conf\n",
      "    :return: SparkContext\n",
      "Generated :<s>Gets a SparkContext instance. :param conf: The SparkContext instance :returns: A SparkContext instance</s>\n",
      "---------------------------\n",
      "Code: def ignore_comments(iterator): for line in iterator: line = COMMENT_RE.sub('', line) line = line.strip() if line: yield line\n",
      "Target: Strips and filters empty or commented lines.\n",
      "Generated :<s>Remove comments from the given iterator.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def get_item_metadata(self, handle): bucket = self.s3resource.Bucket(self.bucket) metadata = {} identifier = generate_identifier(handle) prefix = self.fragments_key_prefix + '{}'.format(identifier) for obj in bucket.objects.filter(Prefix=prefix).all(): metadata_key = obj.key.split('.')[-2] response = obj.get() value_as_string = response['Body'].read().decode('utf-8') value = json.loads(value_as_string) metadata[metadata_key] = value return metadata\n",
      "Target: Return dictionary containing all metadata associated with handle.\n",
      "\n",
      "        In other words all the metadata added using the ``add_item_metadata``\n",
      "        method.\n",
      "\n",
      "        :param handle: handle for accessing an item before the dataset is\n",
      "                       frozen\n",
      "        :returns: dictionary containing item metadata\n",
      "Generated :<s>Retrieve the metadata for a given item. Args: handle: The object to retrieve metadata for. Returns:</s>\n",
      "---------------------------\n",
      "Code: def update(self, t_obj): if isinstance(t_obj, Iterable): self._session.add_all(t_obj) else: self._session.add(t_obj)\n",
      "Target: [update table]\n",
      "\n",
      "        Arguments:\n",
      "            t_obj {[objs of DeclarativeMeta]} -- [update the table]\n",
      "Generated :<s>:param t_obj: :return:</s>\n",
      "---------------------------\n",
      "Code: def stop(self): self._stop_event.set() if self.select_greenlet is not None: self.select_greenlet.kill() self.select_greenlet.get() gevent.sleep()\n",
      "Target: Stop subtasks and let run() finish.\n",
      "Generated :<s>Stop the greenlet.</s>\n",
      "---------------------------\n",
      "Code: def gcm_send_message(registration_id, data, encoding='utf-8', **kwargs): messenger = GCMMessenger(registration_id, data, encoding=encoding, **kwargs) return messenger.send_plain()\n",
      "Target: Standalone method to send a single gcm notification\n",
      "Generated :<s>Send a message :param registration_id: :param data: :param encoding: :return:</s>\n",
      "---------------------------\n",
      "Code: def tree(ontology, output, ols_base): for parent, child in get_hierarchy(ontology=ontology, ols_base=ols_base): click.echo('{}\\t{}'.format(parent, child), file=output)\n",
      "Target: Output the parent-child relations to the given file\n",
      "Generated :<s>%prog tree.ontology.ontology</s>\n",
      "---------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87b0e09bdbe413087d84567c4507842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1653.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved due to improvement in 'val_loss' from 10000 to 2.834005860653489\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def parse_log(log_file): template = OrderedDict([ (\"clean_len\", 0), (\"total_trim\", 0), (\"total_trim_perc\", 0), (\"5trim\", 0), (\"3trim\", 0), (\"bad_reads\", 0) ]) with open(log_file) as fh: for line in fh: fields = [int(x) for x in line.strip().split()[-4:]] if not fields[0]: template[\"bad_reads\"] += 1 template[\"5trim\"] += fields[1] template[\"3trim\"] += fields[3] template[\"total_trim\"] += fields[1] + fields[3] template[\"clean_len\"] += fields[0] total_len = template[\"clean_len\"] + template[\"total_trim\"] if total_len: template[\"total_trim_perc\"] = round( (template[\"total_trim\"] / total_len) * 100, 2) else: template[\"total_trim_perc\"] = 0 return template\n",
      "Target: Retrieves some statistics from a single Trimmomatic log file.\n",
      "\n",
      "    This function parses Trimmomatic's log file and stores some trimming\n",
      "    statistics in an :py:class:`OrderedDict` object. This object contains\n",
      "    the following keys:\n",
      "\n",
      "        - ``clean_len``: Total length after trimming.\n",
      "        - ``total_trim``: Total trimmed base pairs.\n",
      "        - ``total_trim_perc``: Total trimmed base pairs in percentage.\n",
      "        - ``5trim``: Total base pairs trimmed at 5' end.\n",
      "        - ``3trim``: Total base pairs trimmed at 3' end.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    log_file : str\n",
      "        Path to trimmomatic log file.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    x : :py:class:`OrderedDict`\n",
      "        Object storing the trimming statistics.\n",
      "Generated :<s>Parse a log file. Args: log_file (str): The log file to parse. Returns:</s>\n",
      "---------------------------\n",
      "Code: def unregisterObserver(self, observer): if observer in self.m_observers: self.m_observers.remove(observer) pass\n",
      "Target: Remove an observer from the meter update() chain.\n",
      "\n",
      "        Args:\n",
      "            observer (MeterObserver): Subclassed MeterObserver.\n",
      "Generated :<s>Removes an observer from the m_observers.</s>\n",
      "---------------------------\n",
      "Code: def hide_error_message(self): self.ui.error_label.setScaledContents(False) self.ui.error_text_label.hide()\n",
      "Target: This function hides the error message when all values are correct.\n",
      "Generated :<s>Hide error message.</s>\n",
      "---------------------------\n",
      "Code: def activate(self): if self.active: return None self.mock_engine.activate() self.active = True\n",
      "Target: Activates the registered interceptors in the mocking engine.\n",
      "\n",
      "        This means any HTTP traffic captures by those interceptors will\n",
      "        trigger the HTTP mock matching engine in order to determine if a given\n",
      "        HTTP transaction should be mocked out or not.\n",
      "Generated :<s>Activate the engine. :return: None</s>\n",
      "---------------------------\n",
      "Code: def eaf_from_chat(file_path, codec='ascii', extension='wav'): eafob = Eaf() eafob.add_linguistic_type('parent') eafob.add_linguistic_type( 'child', constraints='Symbolic_Association', timealignable=False) participantsdb = {} last_annotation = None with open(file_path, 'r') as chatin: while True: line = chatin.readline().strip().decode(codec) if line == '@UTF8': codec = 'utf8' continue elif line == '@End': break elif line.startswith('@') and line != '@Begin': key, value = line.split(':\\t') eafob.add_property('{}:\\t'.format(key), value) if key == '@Languages': for language in value.split(','): eafob.add_language(language) elif key == '@Participants': for participant in value.split(','): splits = participant.strip().split(' ') splits = map(lambda x: x.replace('_', ' '), splits) if len(splits) == 2: participantsdb[splits[0]] = (None, splits[1]) elif len(splits) == 3: participantsdb[splits[0]] = (splits[1], splits[2]) elif key == '@ID': ids = map(lambda x: x.replace('_', ''), value.split('|')) eafob.add_tier(ids[2], part=participantsdb[ids[2]][0], language=ids[0]) elif key == '@Media': media = value.split(',') eafob.add_linked_file( 'file://{}.{}'.format(media[0], extension)) elif key == '@Transcriber:': for tier in eafob.get_tier_names(): eafob.tiers[tier][2]['ANNOTATOR'] = value elif line.startswith('*'): while len(line.split('\\x15')) != 3: line += chatin.readline().decode(codec).strip() for participant in participantsdb.keys(): if line.startswith('*{}:'.format(participant)): splits = ''.join(line.split(':')[1:]).strip() utt, time, _ = splits.split('\\x15') time = map(int, time.split('_')) last_annotation = (participant, time[0], time[1], utt) eafob.add_annotation(*last_annotation) elif line.startswith('%'): splits = line.split(':') name = '{}_{}'.format(last_annotation[0], splits[0][1:]) if name not in eafob.get_tier_names(): eafob.add_tier(name, 'child', last_annotation[0]) eafob.add_ref_annotation( name, last_annotation[0], sum(last_annotation[1:3])/2, ''.join(splits[1:]).strip()) return eafob\n",
      "Target: Reads a .cha file and converts it to an elan object. The functions tries\n",
      "    to mimic the CHAT2ELAN program that comes with the CLAN package as close as\n",
      "    possible. This function however converts to the latest ELAN file format\n",
      "    since the library is designed for it. All CHAT headers will be added as\n",
      "    Properties in the object and the headers that have a similar field in an\n",
      "    Eaf file will be added there too. The file description of chat files can be\n",
      "    found `here <http://childes.psy.cmu.edu/manuals/CHAT.pdf>`_.\n",
      "\n",
      "    :param str file_path: The file path of the .cha file.\n",
      "    :param str codec: The codec, if the @UTF8 header is present it will choose\n",
      "        utf-8, default is ascii. Older CHAT files don't have their encoding\n",
      "        embedded in a header so you will probably need to choose some obscure\n",
      "        ISO charset then.\n",
      "    :param str extension: The extension of the media file.\n",
      "    :throws StopIteration: If the file doesn't contain a @End header, thus\n",
      "        inferring the file is broken.\n",
      "Generated :<s>Create an EAF object from a chat file. Args: file_path (str): Path to the chat</s>\n",
      "---------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9433c784c48c414c91822cbaeaaa20e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=29253.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def normalize_profile(in_profile, log=False, return_offset = True): if log: tmp_prefactor = in_profile.max(axis=1) tmp_prof = np.exp(in_profile.T - tmp_prefactor).T else: tmp_prefactor = 0.0 tmp_prof = in_profile norm_vector = tmp_prof.sum(axis=1) return (np.copy(np.einsum('ai,a->ai',tmp_prof,1.0/norm_vector)), (np.log(norm_vector) + tmp_prefactor) if return_offset else None)\n",
      "Target: return a normalized version of a profile matrix\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    in_profile : np.array\n",
      "        shape Lxq, will be normalized to one across each row\n",
      "    log : bool, optional\n",
      "        treat the input as log probabilities\n",
      "    return_offset : bool, optional\n",
      "        return the log of the scale factor for each row\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    tuple\n",
      "        normalized profile (fresh np object) and offset (if return_offset==True)\n",
      "Generated :<s>Normalize a profile. Parameters ---------- in_profile : numpy.ndarray, shape = [n_</s>\n",
      "---------------------------\n",
      "Code: def upgrade_dir(srcdir, tgtdir): def pr(s): print s junk = ['.svn','ipythonrc*','*.pyc', '*.pyo', '*~', '.hg'] def ignorable(p): for pat in junk: if p.startswith(pat) or p.fnmatch(pat): return True return False modded = [] files = [path(srcdir).relpathto(p) for p in path(srcdir).walkfiles()] rep = tgtdir / '.upgrade_report' try: rpt = pickle.load(rep.open()) except: rpt = {} for f in files: if ignorable(f): continue src = srcdir / f tgt = tgtdir / f if not tgt.isfile(): pr(\"Creating %s\" % str(tgt)) tgt.write_text(src.text()) rpt[str(tgt)] = hashlib.md5(tgt.text()).hexdigest() else: cont = tgt.text() sum = rpt.get(str(tgt), None) if sum and hashlib.md5(cont).hexdigest() == sum: pr(\"%s: Unedited, installing new version\" % tgt) tgt.write_text(src.text()) rpt[str(tgt)] = hashlib.md5(tgt.text()).hexdigest() else: pr(' == Modified, skipping %s, diffs below == ' % tgt) real = showdiff(tgt,src) pr('') if not real: pr(\"(Ok, it was identical, only upgrading checksum)\") rpt[str(tgt)] = hashlib.md5(tgt.text()).hexdigest() else: modded.append(tgt) pickle.dump(rpt, rep.open('w')) if modded: print \"\\n\\nDelete the following files manually (and rerun %upgrade)\\nif you need a full upgrade:\" for m in modded: print m\n",
      "Target: Copy over all files in srcdir to tgtdir w/ native line endings\n",
      "\n",
      "    Creates .upgrade_report in tgtdir that stores md5sums of all files\n",
      "    to notice changed files b/w upgrades.\n",
      "Generated :<s>Upgrade a directory to a new version of the report.</s>\n",
      "---------------------------\n",
      "Code: def load_trajectory(name=None, index=None, as_new=False, load_parameters=pypetconstants.LOAD_DATA, load_derived_parameters=pypetconstants.LOAD_SKELETON, load_results=pypetconstants.LOAD_SKELETON, load_other_data=pypetconstants.LOAD_SKELETON, recursive=True, load_data=None, max_depth=None, force=False, dynamic_imports=None, new_name='my_trajectory', add_time=True, wildcard_functions=None, with_run_information=True, storage_service=storage.HDF5StorageService, **kwargs): if name is None and index is None: raise ValueError('Please specify either a name or an index') elif name is not None and index is not None: raise ValueError('Please specify either a name or an index') traj = Trajectory(name=new_name, add_time=add_time, dynamic_imports=dynamic_imports, wildcard_functions=wildcard_functions) traj.f_load(name=name, index=index, as_new=as_new, load_parameters=load_parameters, load_derived_parameters=load_derived_parameters, load_results=load_results, load_other_data=load_other_data, recursive=recursive, load_data=load_data, max_depth=max_depth, force=force, with_run_information=with_run_information, storage_service=storage_service, **kwargs) return traj\n",
      "Target: Helper function that creates a novel trajectory and loads it from disk.\n",
      "\n",
      "    For the parameters see :func:`~pypet.trajectory.Trajectory.f_load`.\n",
      "\n",
      "    ``new_name`` and ``add_time`` are only used in case ``as_new`` is ``True``.\n",
      "    Accordingly, they determine the new name of trajectory.\n",
      "Generated :<s>Load a Trajectory Parameters ---------- name : str Name of the Trajectory index : str Index of the</s>\n",
      "---------------------------\n",
      "Code: def decode_date(self, val): if isinstance(val, basestring) and val.count('-') == 2 and len(val) > 9: try: dt = dateutil.parser.parse(val) if val.endswith(('+00:00', '-00:00', 'Z')): dt = dt.replace(tzinfo=None) return dt except (TypeError, ValueError): pass return val\n",
      "Target: Tries to decode strings that look like dates into datetime objects.\n",
      "Generated :<s>Decode a datetime object into a datetime.date object.</s>\n",
      "---------------------------\n",
      "Code: def load_entrypoint_plugins(entry_points, airflow_plugins): for entry_point in entry_points: log.debug('Importing entry_point plugin %s', entry_point.name) plugin_obj = entry_point.load() if is_valid_plugin(plugin_obj, airflow_plugins): if callable(getattr(plugin_obj, 'on_load', None)): plugin_obj.on_load() airflow_plugins.append(plugin_obj) return airflow_plugins\n",
      "Target: Load AirflowPlugin subclasses from the entrypoints\n",
      "    provided. The entry_point group should be 'airflow.plugins'.\n",
      "\n",
      "    :param entry_points: A collection of entrypoints to search for plugins\n",
      "    :type entry_points: Generator[setuptools.EntryPoint, None, None]\n",
      "    :param airflow_plugins: A collection of existing airflow plugins to\n",
      "        ensure we don't load duplicates\n",
      "    :type airflow_plugins: list[type[airflow.plugins_manager.AirflowPlugin]]\n",
      "    :rtype: list[airflow.plugins_manager.AirflowPlugin]\n",
      "Generated :<s>Load an entry point plugin. Args: entries_point: A list of entry points. airflow_plugins:</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def _get_pretty_exception_message(e): if (hasattr(e, 'message') and 'errorName' in e.message and 'message' in e.message): return ('{name}: {message}'.format( name=e.message['errorName'], message=e.message['message'])) else: return str(e)\n",
      "Target: Parses some DatabaseError to provide a better error message\n",
      "Generated :<s>Return a pretty error message from e. :param e: :return:</s>\n",
      "---------------------------\n",
      "Code: def get_entries(latest, filters, exclude, limit=None): entry_list = _list_all_latest() if latest is True or not _is_valid_cache() else _list_all_cached() filtered = filter_entries(entry_list, filters, exclude) if limit is not None: return filtered[:limit] else: return filtered\n",
      "Target: Lists all available instances.\n",
      "\n",
      "    :param latest: If true, ignores the cache and grabs the latest list.\n",
      "    :type latest: ``bool``\n",
      "    :param filters: Filters to apply to results. A result will only be shown\n",
      "                    if it includes all text in all filters.\n",
      "    :type filters: [``str``]\n",
      "    :param exclude: The opposite of filters. Results will be rejected if they\n",
      "                    include any of these strings.\n",
      "    :type exclude: [``str``]\n",
      "    :param limit: Maximum number of entries to show (default no maximum).\n",
      "    :type limit: ``int`` or ``NoneType``\n",
      "\n",
      "    :return: A list of host entries.\n",
      "    :rtype: ``list`` of :py:class:`HostEntry`\n",
      "Generated :<s>Return a list of entries filtered by filters. :param latest: The latest entry to filter. :param filters:</s>\n",
      "---------------------------\n",
      "Code: def _write_local_data_files(self, cursor): schema = list(map(lambda schema_tuple: schema_tuple[0].replace(' ', '_'), cursor.description)) file_no = 0 tmp_file_handle = NamedTemporaryFile(delete=True) tmp_file_handles = {self.filename.format(file_no): tmp_file_handle} for row in cursor: row = map(self.convert_types, row) row_dict = dict(zip(schema, row)) s = json.dumps(row_dict, sort_keys=True) s = s.encode('utf-8') tmp_file_handle.write(s) tmp_file_handle.write(b'\\n') if tmp_file_handle.tell() >= self.approx_max_file_size_bytes: file_no += 1 tmp_file_handle = NamedTemporaryFile(delete=True) tmp_file_handles[self.filename.format(file_no)] = tmp_file_handle return tmp_file_handles\n",
      "Target: Takes a cursor, and writes results to a local file.\n",
      "\n",
      "        :return: A dictionary where keys are filenames to be used as object\n",
      "            names in GCS, and values are file handles to local files that\n",
      "            contain the data for the GCS objects.\n",
      "Generated :<s>Write the local data files to the file handle. :param cursor: :return:</s>\n",
      "---------------------------\n",
      "Code: def extend_reservation(self, timedelta): self.validate_cart() cart = self.cart cart.refresh_from_db() elapsed = (timezone.now() - cart.time_last_updated) if cart.reservation_duration - elapsed > timedelta: return cart.time_last_updated = timezone.now() cart.reservation_duration = timedelta cart.save()\n",
      "Target: Extends the reservation on this cart by the given timedelta.\n",
      "        This can only be done if the current state of the cart is valid (i.e\n",
      "        all items and discounts in the cart are still available.)\n",
      "\n",
      "        Arguments:\n",
      "            timedelta (timedelta): The amount of time to extend the cart by.\n",
      "                The resulting reservation_duration will be now() + timedelta,\n",
      "                unless the requested extension is *LESS* than the current\n",
      "                reservation deadline.\n",
      "Generated :<s>Extend the reservation for a given timeelta. :param timedelta: :return:</s>\n",
      "---------------------------\n",
      "Code: def batch_list(sequence, batch_size, mod = 0, randomize = False): if randomize: sequence = random.sample(sequence, len(sequence)) return [sequence[x:x + batch_size] for x in xrange(0, len(sequence)-mod, batch_size)]\n",
      "Target: Converts a list into a list of lists with equal batch_size.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    sequence : list\n",
      "        list of items to be placed in batches\n",
      "    batch_size : int\n",
      "        length of each sub list\n",
      "    mod : int\n",
      "        remainder of list length devided by batch_size\n",
      "        mod = len(sequence) % batch_size\n",
      "    randomize = bool\n",
      "        should the initial sequence be randomized before being batched\n",
      "Generated :<s>Generate a list of batches for a given sequence. Parameters ---------- sequence : iterable batch_size : int</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def rmi(self, image_name): container = self.rm(image_name, delete=True) if container is not None: bot.info(\"[rmi] %s\" % container)\n",
      "Target: Remove an image from the database and filesystem.\n",
      "Generated :<s>rmi :param image_name: :return:</s>\n",
      "---------------------------\n",
      "Code: def format_screen(strng): par_re = re.compile(r'\\\\$',re.MULTILINE) strng = par_re.sub('',strng) return strng\n",
      "Target: Format a string for screen printing.\n",
      "\n",
      "    This removes some latex-type format codes.\n",
      "Generated :<s>Returns a string representation of the screen.</s>\n",
      "---------------------------\n",
      "Code: def fuzz(self, obj): buf = list(obj) FuzzFactor = random.randrange(1, len(buf)) numwrites=random.randrange(math.ceil((float(len(buf)) / FuzzFactor)))+1 for j in range(numwrites): self.random_action(buf) return self.safe_unicode(buf)\n",
      "Target: Perform the fuzzing\n",
      "Generated :<s>Randomly generate a FuzzFactor from the given object.</s>\n",
      "---------------------------\n",
      "Code: def render(self, name, color=True, just=None, **kwargs): res = self._render(name, color=color, **kwargs) invis_chars = self.invisible_chars[name] if color else 0 self.txtwidth = _lenlastline(res) - invis_chars just = self.justify if (just is None) else just if just and name != 'in' and ('\\n' not in res) and ('\\r' not in res): res = res.rjust(self.width + invis_chars) self.width = _lenlastline(res) - invis_chars return res\n",
      "Target: Render the selected prompt.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        name : str\n",
      "          Which prompt to render. One of 'in', 'in2', 'out', 'rewrite'\n",
      "        color : bool\n",
      "          If True (default), include ANSI escape sequences for a coloured prompt.\n",
      "        just : bool\n",
      "          If True, justify the prompt to the width of the last prompt. The\n",
      "          default is stored in self.justify.\n",
      "        **kwargs :\n",
      "          Additional arguments will be passed to the string formatting operation,\n",
      "          so they can override the values that would otherwise fill in the\n",
      "          template.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        A string containing the rendered prompt.\n",
      "Generated :<s>Render the text with the given color. :param name: The name of the text to render. :param color</s>\n",
      "---------------------------\n",
      "Code: def write_to(self, out_stream, do_compress=False): self.update_header() if ( self.vlrs.get(\"ExtraBytesVlr\") and not self.points_data.extra_dimensions_names ): logger.error( \"Las contains an ExtraBytesVlr, but no extra bytes were found in the point_record, \" \"removing the vlr\" ) self.vlrs.extract(\"ExtraBytesVlr\") if do_compress: laz_vrl = create_laz_vlr(self.points_data) self.vlrs.append(known.LasZipVlr(laz_vrl.data())) raw_vlrs = vlrlist.RawVLRList.from_list(self.vlrs) self.header.offset_to_point_data = ( self.header.size + raw_vlrs.total_size_in_bytes() ) self.header.point_format_id = uncompressed_id_to_compressed( self.header.point_format_id ) self.header.number_of_vlr = len(raw_vlrs) points_bytes = compress_buffer( np.frombuffer(self.points_data.array, np.uint8), laz_vrl.schema, self.header.offset_to_point_data, ).tobytes() else: raw_vlrs = vlrlist.RawVLRList.from_list(self.vlrs) self.header.number_of_vlr = len(raw_vlrs) self.header.offset_to_point_data = ( self.header.size + raw_vlrs.total_size_in_bytes() ) points_bytes = self.points_data.raw_bytes() self.header.write_to(out_stream) self._raise_if_not_expected_pos(out_stream, self.header.size) raw_vlrs.write_to(out_stream) self._raise_if_not_expected_pos(out_stream, self.header.offset_to_point_data) out_stream.write(points_bytes)\n",
      "Target: writes the data to a stream\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        out_stream: file object\n",
      "            the destination stream, implementing the write method\n",
      "        do_compress: bool, optional, default False\n",
      "            Flag to indicate if you want the date to be compressed\n",
      "Generated :<s>Write the vlr to the stream. :param out_stream: The stream to write to. :type out</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def template_tag(library, name): def _inner(cls): if hasattr(cls, 'parse'): compile_function = cls.parse else: compile_function = cls library.tag(name, compile_function) return cls return _inner\n",
      "Target: Decorator to register class tags\n",
      "\n",
      "    :param library: The template tag library, typically instantiated as ``register = Library()``.\n",
      "    :type library: :class:`~django.template.Library`\n",
      "    :param name: The name of the template tag\n",
      "    :type name: str\n",
      "\n",
      "    Example:\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        @template_tag(register, 'my_tag')\n",
      "        class MyTag(BaseNode):\n",
      "            pass\n",
      "Generated :<s>A decorator for registering a template tag with the given name.</s>\n",
      "---------------------------\n",
      "Code: def remove(self, it: Signature) -> bool: txt = it.internal_name() if txt not in self._hsig: raise KeyError(it.show_name() + ' not in Set') sig = self._hsig[txt] if isinstance(sig, Scope): sig.state = StateScope.LINKED del self._hsig[txt] return True\n",
      "Target: Remove it but raise KeyError if not found\n",
      "Generated :<s>Removes the value from the set. :param it: The value to remove. :returns: True if</s>\n",
      "---------------------------\n",
      "Code: def open_slots(self, session): from airflow.models.taskinstance import TaskInstance as TI used_slots = session.query(func.count()).filter(TI.pool == self.pool).filter( TI.state.in_([State.RUNNING, State.QUEUED])).scalar() return self.slots - used_slots\n",
      "Target: Returns the number of slots open at the moment\n",
      "Generated :<s>Opens the slots. :param session: :return:</s>\n",
      "---------------------------\n",
      "Code: def to_epw(self, buffer_or_path=None): df = self._weather_series.copy() df[\"hour\"] += 1 epw_content = self._headers_to_epw() + df.to_csv(header=False, index=False, line_terminator=\"\\n\") return multi_mode_write( lambda buffer: buffer.write(epw_content), lambda: epw_content, buffer_or_path=buffer_or_path )\n",
      "Target: Parameters\n",
      "        ----------\n",
      "        buffer_or_path: buffer or path, default None\n",
      "            Buffer or path to write into. If None, will return a string containing epw info.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        None or a string if buffer_or_path is None.\n",
      "Generated :<s>Writes the weather data to an epw file. Args: buffer_or_path (Optional[str</s>\n",
      "---------------------------\n",
      "Code: def raw_print(*args, **kw): print(*args, sep=kw.get('sep', ' '), end=kw.get('end', '\\n'), file=sys.__stdout__) sys.__stdout__.flush()\n",
      "Target: Raw print to sys.__stdout__, otherwise identical interface to print().\n",
      "Generated :<s>print(*args, sep=str, end=str) Print a message to stdout.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def _get_block_plain_text(self, block): cursor = QtGui.QTextCursor(block) cursor.movePosition(QtGui.QTextCursor.StartOfBlock) cursor.movePosition(QtGui.QTextCursor.EndOfBlock, QtGui.QTextCursor.KeepAnchor) return cursor.selection().toPlainText()\n",
      "Target: Given a QTextBlock, return its unformatted text.\n",
      "Generated :<s>Get the plain text for a block.</s>\n",
      "---------------------------\n",
      "Code: def fake_data_generator(conf: List[dict]) -> pd.DataFrame: label_confs = [x for x in conf if x['type'] == 'label'] label_names = [x['name'] for x in label_confs] label_values = [x['values'] for x in label_confs] df = pd.DataFrame(list(product(*label_values)), columns=label_names) number_confs = [x for x in conf if x['type'] == 'number'] for num_conf in number_confs: num_column = np.random.uniform(low=num_conf['min'], high=num_conf['max'], size=df.shape[0]) df[num_conf['name']] = num_column.round(num_conf.get('digits', 4)) return df\n",
      "Target: `conf` is a list of dictionaries like\n",
      "      {'type': 'label', 'values': ['Paris', 'Marseille', 'Lyons'], 'name': 'Cities'}\n",
      "    and each dictionary will add a column.\n",
      "\n",
      "    There are two different behaviours:\n",
      "    - type: 'label' -> the new column will be taken into account for a cartesian product\n",
      "                       with all other labels\n",
      "    - type: 'number' -> the new column will contain simple numbers\n",
      "Generated :<s>Generate a pandas data frame from a list of dicts. Args: conf: A list of dict</s>\n",
      "---------------------------\n",
      "Code: def correlation_matrix_volume_rejection_samples( det_bounds, dim, sample_shape, dtype, seed): with tf.compat.v1.name_scope(\"rejection_sampler\"): rej_proposals = _uniform_correlation_like_matrix( dim, sample_shape, dtype, seed=seed) rej_proposal_volume = 2. ** (dim * (dim - 1) / 2.) rej_weights = rej_proposal_volume * _psd_mask( rej_proposals) * _det_large_enough_mask(rej_proposals, det_bounds) return rej_weights, rej_proposal_volume\n",
      "Target: Returns rejection samples from trying to get good correlation matrices.\n",
      "\n",
      "  The proposal being rejected from is the uniform distribution on\n",
      "  \"correlation-like\" matrices.  We say a matrix is \"correlation-like\"\n",
      "  if it is a symmetric square matrix with all entries between -1 and 1\n",
      "  (inclusive) and 1s on the main diagonal.  Of these, the ones that\n",
      "  are positive semi-definite are exactly the correlation matrices.\n",
      "\n",
      "  The rejection algorithm, then, is to sample a `Tensor` of\n",
      "  `sample_shape` correlation-like matrices of dimensions `dim` by\n",
      "  `dim`, and check each one for (i) being a correlation matrix (i.e.,\n",
      "  PSD), and (ii) having determinant at least the corresponding entry\n",
      "  of `det_bounds`.\n",
      "\n",
      "  Args:\n",
      "    det_bounds: A `Tensor` of lower bounds on the determinants of\n",
      "      acceptable matrices.  The shape must broadcast with `sample_shape`.\n",
      "    dim: A Python `int` dimension of correlation matrices to sample.\n",
      "    sample_shape: Python `tuple` of `int` shape of the samples to\n",
      "      compute, excluding the two matrix dimensions.\n",
      "    dtype: The `dtype` in which to do the computation.\n",
      "    seed: Random seed.\n",
      "\n",
      "  Returns:\n",
      "    weights: A `Tensor` of shape `sample_shape`.  Each entry is 0 if the\n",
      "      corresponding matrix was not a correlation matrix, or had too\n",
      "      small of a determinant.  Otherwise, the entry is the\n",
      "      multiplicative inverse of the density of proposing that matrix\n",
      "      uniformly, i.e., the volume of the set of `dim` by `dim`\n",
      "      correlation-like matrices.\n",
      "    volume: The volume of the set of `dim` by `dim` correlation-like\n",
      "      matrices.\n",
      "Generated :<s>Calculate the correlation matrix for the volume rejection. Args: det_bounds (:obj:`</s>\n",
      "---------------------------\n",
      "Code: def resolve(self, requirements, env=None, installer=None, replace_conflicting=False): requirements = list(requirements)[::-1] processed = {} best = {} to_activate = [] required_by = collections.defaultdict(set) while requirements: req = requirements.pop(0) if req in processed: continue dist = best.get(req.key) if dist is None: dist = self.by_key.get(req.key) if dist is None or (dist not in req and replace_conflicting): ws = self if env is None: if dist is None: env = Environment(self.entries) else: env = Environment([]) ws = WorkingSet([]) dist = best[req.key] = env.best_match(req, ws, installer) if dist is None: raise DistributionNotFound(req) to_activate.append(dist) if dist not in req: tmpl = \"%s is installed but %s is required by %s\" args = dist, req, list(required_by.get(req, [])) raise VersionConflict(tmpl % args) new_requirements = dist.requires(req.extras)[::-1] requirements.extend(new_requirements) for new_requirement in new_requirements: required_by[new_requirement].add(req.project_name) processed[req] = True return to_activate\n",
      "Target: List all distributions needed to (recursively) meet `requirements`\n",
      "\n",
      "        `requirements` must be a sequence of ``Requirement`` objects.  `env`,\n",
      "        if supplied, should be an ``Environment`` instance.  If\n",
      "        not supplied, it defaults to all distributions available within any\n",
      "        entry or distribution in the working set.  `installer`, if supplied,\n",
      "        will be invoked with each requirement that cannot be met by an\n",
      "        already-installed distribution; it should return a ``Distribution`` or\n",
      "        ``None``.\n",
      "\n",
      "        Unless `replace_conflicting=True`, raises a VersionConflict exception if\n",
      "        any requirements are found on the path that have the correct name but\n",
      "        the wrong version.  Otherwise, if an `installer` is supplied it will be\n",
      "        invoked to obtain the correct version of the requirement and activate\n",
      "        it.\n",
      "Generated :<s>Resolve the given requirements. Args: requirements: A list of requirements to resolve. env: The environment to</s>\n",
      "---------------------------\n",
      "Code: def on_exit(self, info): if self.prompt_on_exit: retval = confirm(parent = info.ui.control, message = \"Exit Godot?\", title = \"Confirm exit\", default = YES) if retval == YES: self._on_close( info ) else: self._on_close( info )\n",
      "Target: Handles the user attempting to exit Godot.\n",
      "Generated :<s>Called when the user exits.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def get_pull_requests(app, repo_config): response = get_api_response(app, repo_config, \"/repos/{repo_name}/pulls\") if not response.ok: raise Exception(\"Unable to get pull requests: status code {}\".format(response.status_code)) return (item for item in response.json)\n",
      "Target: Last 30 pull requests from a repository.\n",
      "\n",
      "    :param app: Flask app\n",
      "    :param repo_config: dict with ``github_repo`` key\n",
      "\n",
      "    :returns: id for a pull request\n",
      "Generated :<s>Returns a list of pull requests. :param app: :param repo_config: :return:</s>\n",
      "---------------------------\n",
      "Code: def instruction_list(self): instruction_list = [] for instruction in self.data: if isinstance(instruction, CompositeGate): instruction_list.extend(instruction.instruction_list()) else: instruction_list.append(instruction) return instruction_list\n",
      "Target: Return a list of instructions for this CompositeGate.\n",
      "\n",
      "        If the CompositeGate itself contains composites, call\n",
      "        this method recursively.\n",
      "Generated :<s>Returns a list of all instruction objects. Returns: list: A list of all the instructions.</s>\n",
      "---------------------------\n",
      "Code: def nesting_level(self) -> int: ss, se = self._span level = 0 type_to_spans = self._type_to_spans for type_ in ('Template', 'ParserFunction'): spans = type_to_spans[type_] for s, e in spans[:bisect(spans, [ss + 1])]: if se <= e: level += 1 return level\n",
      "Target: Return the nesting level of self.\n",
      "\n",
      "        The minimum nesting_level is 0. Being part of any Template or\n",
      "        ParserFunction increases the level by one.\n",
      "Generated :<s>Returns the nesting level of this parser.</s>\n",
      "---------------------------\n",
      "Code: def validate_changeset(changeset): errors = [] changes = changeset.findall('.//{%s}Change' % R53_XMLNS) num_changes = len(changes) if num_changes == 0: errors.append('changeset must have at least one <Change> element') if num_changes > 100: errors.append('changeset has %d <Change> elements: max is 100' % num_changes) rrs = changeset.findall('.//{%s}ResourceRecord' % R53_XMLNS) num_rrs = len(rrs) if num_rrs > 1000: errors.append('changeset has %d ResourceRecord elements: max is 1000' % num_rrs) values = changeset.findall('.//{%s}Value' % R53_XMLNS) num_chars = 0 for value in values: num_chars += len(value.text) if num_chars > 10000: errors.append('changeset has %d chars in <Value> text: max is 10000' % num_chars) return errors\n",
      "Target: Validate a changeset is compatible with Amazon's API spec.\n",
      "\n",
      "  Args: changeset: lxml.etree.Element (<ChangeResourceRecordSetsRequest>)\n",
      "  Returns: [ errors ] list of error strings or [].\n",
      "Generated :<s>Validate that changeset has at least one <Change> element. Args: changeset (Changeset):</s>\n",
      "---------------------------\n",
      "Code: def get_partitions(self, database_name, table_name, expression='', page_size=None, max_items=None): config = { 'PageSize': page_size, 'MaxItems': max_items, } paginator = self.get_conn().get_paginator('get_partitions') response = paginator.paginate( DatabaseName=database_name, TableName=table_name, Expression=expression, PaginationConfig=config ) partitions = set() for page in response: for p in page['Partitions']: partitions.add(tuple(p['Values'])) return partitions\n",
      "Target: Retrieves the partition values for a table.\n",
      "\n",
      "        :param database_name: The name of the catalog database where the partitions reside.\n",
      "        :type database_name: str\n",
      "        :param table_name: The name of the partitions' table.\n",
      "        :type table_name: str\n",
      "        :param expression: An expression filtering the partitions to be returned.\n",
      "            Please see official AWS documentation for further information.\n",
      "            https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-catalog-partitions.html#aws-glue-api-catalog-partitions-GetPartitions\n",
      "        :type expression: str\n",
      "        :param page_size: pagination size\n",
      "        :type page_size: int\n",
      "        :param max_items: maximum items to return\n",
      "        :type max_items: int\n",
      "        :return: set of partition values where each value is a tuple since\n",
      "            a partition may be composed of multiple columns. For example:\n",
      "            ``{('2018-01-01','1'), ('2018-01-01','2')}``\n",
      "Generated :<s>Returns a set of partitions. :param database_name: The name of the database. :param table_name</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def _handle_weekly_repeat_out(self): start_d = _first_weekday( self.event.l_start_date.weekday(), date(self.year, self.month, 1) ) self.day = start_d.day self.count_first = True if self.event.repeats('BIWEEKLY'): self._biweekly_helper() elif self.event.repeats('WEEKLY'): self.repeat() if self.event.is_chunk(): diff = self.event.start_end_diff self.count = _chunk_fill_out_first_week( self.year, self.month, self.count, self.event, diff ) for i in xrange(diff): self.day = start_d.day + i + 1 self.repeat()\n",
      "Target: Handles repeating an event weekly (or biweekly) if the current\n",
      "        year and month are outside of its start year and month.\n",
      "        It takes care of cases 3 and 4 in _handle_weekly_repeat_in() comments.\n",
      "Generated :<s>Handlesweekly repeat out events.</s>\n",
      "---------------------------\n",
      "Code: def cache( requires=None, disabled=False, applied_on_method=False, check_param=True, limit=None ): if not requires: requires = [] elif isinstance(requires, collections.Callable): requires = [requires] if not isinstance(check_param, (bool, str)): raise TypeError(\"'check_param' must be a str (name of the param to check) or a bool\") if limit is not None and not isinstance(limit, int): raise TypeError(\"'limit' must be an int (number of cache entries to keep) or None\") if not hasattr(cache, 'funcs_references'): cache.funcs_references = {} if not hasattr(cache, 'dependencies'): cache.dependencies = {} if not hasattr(cache, 'memories'): cache.memories = {} def decorator(func): cache.funcs_references[func.__name__] = get_orig_function(func) dependencies_names = [] for requirement in requires: if isinstance(requirement, collections.Callable): req_name = requirement.__name__ cache.funcs_references[req_name] = get_orig_function(requirement) elif requirement not in cache.funcs_references: req_name = requirement cache.funcs_references[req_name] = None dependencies_names.append(req_name) cache.dependencies[func.__name__] = dependencies_names @wraps(func) def wrapper(*args, **kwargs): current_memory = cache.memories.get(current_thread().name) if disabled is True or current_memory is None: return func(*args, **kwargs) concatenated_source_code = '' dependencies = resolve_dependencies(func.__name__, cache.dependencies) for func_name in dependencies: function = cache.funcs_references[func_name] if function is None: raise Exception(f\"Can't get source code of function '{func_name}'\") source_code = get_func_sourcecode(function) concatenated_source_code += source_code md5_hash = md5(str.encode(concatenated_source_code)).hexdigest() tmp_extra_kwargs = { '__func_dependencies_hash__': md5_hash, '__original_func_name__': func.__name__, } if check_param is True: kwargs.update(tmp_extra_kwargs) if applied_on_method: self_arg, args = args[0], args[1:] @wraps(func) def f(*args, **kwargs): for k in tmp_extra_kwargs.keys(): del kwargs[k] if applied_on_method: args = (self_arg,) + args return func(*args, **kwargs) f = current_memory.cache(f) result = f(*args, **kwargs) else: if isinstance(check_param, str): check_only_param_value = get_param_value_from_func_call( param_name=check_param, func=func, call_args=args, call_kwargs=kwargs, ) tmp_extra_kwargs['__check_only__'] = check_only_param_value @wraps(func) def f(**tmp_extra_kwargs): return func(*args, **kwargs) f = current_memory.cache(f) result = f(**tmp_extra_kwargs) if limit is not None: clean_cachedir_old_entries(f.store_backend, func.__name__, limit) return result return wrapper return decorator\n",
      "Target: Avoid to recompute a function if its parameters and its source code doesnt have changed.\n",
      "\n",
      "        Args:\n",
      "            requires: list of dependencies (functions or function names)\n",
      "            disabled (bool): disable the cache mecanism for this function (useful if you\n",
      "                                 only want to use the dependency mecanism)\n",
      "            applied_on_method (bool): ignore the first argument (useful to ignore \"self\")\n",
      "            check_param (True, False or a str): the name of the parameter to check.\n",
      "                                                    False to not check any of them.\n",
      "                                                    True (default) to check all of them.\n",
      "            limit (int or None): number of cache entries to keep (no limit by default)\n",
      "Generated :<s>Decorator to add a dependency to the cache. Parameters ---------- requires : list, optional, default = None</s>\n",
      "---------------------------\n",
      "Code: def accept_alert(self, text=None, wait=None): wait = wait or capybara.default_max_wait_time with self.driver.accept_modal(\"alert\", text=text, wait=wait): yield\n",
      "Target: Execute the wrapped code, accepting an alert.\n",
      "\n",
      "        Args:\n",
      "            text (str | RegexObject, optional): Text to match against the text in the modal.\n",
      "            wait (int | float, optional): Maximum time to wait for the modal to appear after\n",
      "                executing the wrapped code.\n",
      "\n",
      "        Raises:\n",
      "            ModalNotFound: If a modal dialog hasn't been found.\n",
      "Generated :<s>Accepts an alert. :param text: Text to be accepted. :type text: str :param wait:</s>\n",
      "---------------------------\n",
      "Code: def create_py(self, nb, force=False): if list(map(int, re.findall('\\d+', nbconvert.__version__))) >= [4, 2]: py_file = os.path.basename(self.py_file) else: py_file = self.py_file try: level = logger.logger.level except AttributeError: level = logger.level spr.call(['jupyter', 'nbconvert', '--to=python', '--output=' + py_file, '--log-level=%s' % level, self.outfile]) with open(self.py_file) as f: py_content = f.read() py_content = re.sub('^\\s*get_ipython\\(\\).magic.*', '# \\g<0>', py_content, flags=re.MULTILINE) with open(self.py_file, 'w') as f: f.write(py_content)\n",
      "Target: Create the python script from the notebook node\n",
      "Generated :<s>:param nb: :param force: :return:</s>\n",
      "---------------------------\n",
      "Code: def create_topic(self, topic_name, topic=None, fail_on_exist=False): _validate_not_none('topic_name', topic_name) request = HTTPRequest() request.method = 'PUT' request.host = self._get_host() request.path = '/' + _str(topic_name) + '' request.body = _get_request_body(_convert_topic_to_xml(topic)) request.path, request.query = self._httpclient._update_request_uri_query(request) request.headers = self._update_service_bus_header(request) if not fail_on_exist: try: self._perform_request(request) return True except AzureHttpError as ex: _dont_fail_on_exist(ex) return False else: self._perform_request(request) return True\n",
      "Target: Creates a new topic. Once created, this topic resource manifest is\n",
      "        immutable.\n",
      "\n",
      "        topic_name:\n",
      "            Name of the topic to create.\n",
      "        topic:\n",
      "            Topic object to create.\n",
      "        fail_on_exist:\n",
      "            Specify whether to throw an exception when the topic exists.\n",
      "Generated :<s>:param topic_name: :param topic: :param fail_on_exist: :return:</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def is_positive_semidefinite_matrix(mat, rtol=RTOL_DEFAULT, atol=ATOL_DEFAULT): if atol is None: atol = ATOL_DEFAULT if rtol is None: rtol = RTOL_DEFAULT if not is_hermitian_matrix(mat, rtol=rtol, atol=atol): return False vals = np.linalg.eigvalsh(mat) for v in vals: if v < -atol: return False return True\n",
      "Target: Test if a matrix is positive semidefinite\n",
      "Generated :<s>Return True if the matrix is positive, False otherwise. Args: matrix (numpy.ndarray): Matrix</s>\n",
      "---------------------------\n",
      "Code: def cost(self, t_node, branch_length, multiplicity=2.0): merger_time = t_node+branch_length return self.integral_merger_rate(merger_time) - self.integral_merger_rate(t_node) - np.log(self.total_merger_rate(merger_time))*(multiplicity-1.0)/multiplicity\n",
      "Target: returns the cost associated with a branch starting at t_node\n",
      "        t_node is time before present, the branch goes back in time\n",
      "\n",
      "        Args:\n",
      "            - t_node:           time of the node\n",
      "            - branch_length:    branch length, determines when this branch merges with sister\n",
      "            - multiplicity:     2 if merger is binary, higher if this is a polytomy\n",
      "Generated :<s>Calculate the cost of a node. :param t_node: :param branch_length: :param</s>\n",
      "---------------------------\n",
      "Code: def get_colors(stylename): style = get_style_by_name(stylename) fgcolor = style.style_for_token(Token.Text)['color'] or '' if len(fgcolor) in (3,6): try: int(fgcolor, 16) except TypeError: pass else: fgcolor = \"#\"+fgcolor return dict( bgcolor = style.background_color, select = style.highlight_color, fgcolor = fgcolor )\n",
      "Target: Construct the keys to be used building the base stylesheet\n",
      "    from a templatee.\n",
      "Generated :<s>Returns a dictionary of colors for a given token.</s>\n",
      "---------------------------\n",
      "Code: def cq_to_chroma(n_input, bins_per_octave=12, n_chroma=12, fmin=None, window=None, base_c=True, dtype=np.float32): n_merge = float(bins_per_octave) / n_chroma if fmin is None: fmin = note_to_hz('C1') if np.mod(n_merge, 1) != 0: raise ParameterError('Incompatible CQ merge: ' 'input bins must be an ' 'integer multiple of output bins.') cq_to_ch = np.repeat(np.eye(n_chroma), n_merge, axis=1) cq_to_ch = np.roll(cq_to_ch, - int(n_merge // 2), axis=1) n_octaves = np.ceil(np.float(n_input) / bins_per_octave) cq_to_ch = np.tile(cq_to_ch, int(n_octaves))[:, :n_input] midi_0 = np.mod(hz_to_midi(fmin), 12) if base_c: roll = midi_0 else: roll = midi_0 - 9 roll = int(np.round(roll * (n_chroma / 12.))) cq_to_ch = np.roll(cq_to_ch, roll, axis=0).astype(dtype) if window is not None: cq_to_ch = scipy.signal.convolve(cq_to_ch, np.atleast_2d(window), mode='same') return cq_to_ch\n",
      "Target: Convert a Constant-Q basis to Chroma.\n",
      "\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    n_input : int > 0 [scalar]\n",
      "        Number of input components (CQT bins)\n",
      "\n",
      "    bins_per_octave : int > 0 [scalar]\n",
      "        How many bins per octave in the CQT\n",
      "\n",
      "    n_chroma : int > 0 [scalar]\n",
      "        Number of output bins (per octave) in the chroma\n",
      "\n",
      "    fmin : None or float > 0\n",
      "        Center frequency of the first constant-Q channel.\n",
      "        Default: 'C1' ~= 32.7 Hz\n",
      "\n",
      "    window : None or np.ndarray\n",
      "        If provided, the cq_to_chroma filter bank will be\n",
      "        convolved with `window`.\n",
      "\n",
      "    base_c : bool\n",
      "        If True, the first chroma bin will start at 'C'\n",
      "        If False, the first chroma bin will start at 'A'\n",
      "\n",
      "    dtype : np.dtype\n",
      "        The data type of the output basis.\n",
      "        By default, uses 32-bit (single-precision) floating point.\n",
      "\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    cq_to_chroma : np.ndarray [shape=(n_chroma, n_input)]\n",
      "        Transformation matrix: `Chroma = np.dot(cq_to_chroma, CQT)`\n",
      "\n",
      "    Raises\n",
      "    ------\n",
      "    ParameterError\n",
      "        If `n_input` is not an integer multiple of `n_chroma`\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    This function caches at level 10.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    Get a CQT, and wrap bins to chroma\n",
      "\n",
      "    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n",
      "    >>> CQT = np.abs(librosa.cqt(y, sr=sr))\n",
      "    >>> chroma_map = librosa.filters.cq_to_chroma(CQT.shape[0])\n",
      "    >>> chromagram = chroma_map.dot(CQT)\n",
      "    >>> # Max-normalize each time step\n",
      "    >>> chromagram = librosa.util.normalize(chromagram, axis=0)\n",
      "\n",
      "    >>> import matplotlib.pyplot as plt\n",
      "    >>> plt.subplot(3, 1, 1)\n",
      "    >>> librosa.display.specshow(librosa.amplitude_to_db(CQT,\n",
      "    ...                                                  ref=np.max),\n",
      "    ...                          y_axis='cqt_note')\n",
      "    >>> plt.title('CQT Power')\n",
      "    >>> plt.colorbar()\n",
      "    >>> plt.subplot(3, 1, 2)\n",
      "    >>> librosa.display.specshow(chromagram, y_axis='chroma')\n",
      "    >>> plt.title('Chroma (wrapped CQT)')\n",
      "    >>> plt.colorbar()\n",
      "    >>> plt.subplot(3, 1, 3)\n",
      "    >>> chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
      "    >>> librosa.display.specshow(chroma, y_axis='chroma', x_axis='time')\n",
      "    >>> plt.title('librosa.feature.chroma_stft')\n",
      "    >>> plt.colorbar()\n",
      "    >>> plt.tight_layout()\n",
      "Generated :<s>Convert cq to chroma. Parameters ---------- cq_to_chroma : np.ndarray</s>\n",
      "---------------------------\n",
      "Code: def kill(self, ti): if self.cmd is None: if not ti and not self.task_instance: raise Exception(\"Unable to cancel Qubole Command, context is unavailable!\") elif not ti: ti = self.task_instance cmd_id = ti.xcom_pull(key=\"qbol_cmd_id\", task_ids=ti.task_id) self.cmd = self.cls.find(cmd_id) if self.cls and self.cmd: self.log.info('Sending KILL signal to Qubole Command Id: %s', self.cmd.id) self.cmd.cancel()\n",
      "Target: Kill (cancel) a Qubole command\n",
      "        :param ti: Task Instance of the dag, used to determine the Quboles command id\n",
      "        :return: response from Qubole\n",
      "Generated :<s>Kill a Qubole command. :param ti | <QuboleCommand></s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def render_template(content, context): rendered = Template(content).render(Context(context)) return rendered\n",
      "Target: renders context aware template\n",
      "Generated :<s>Render a template. :param content: The content of the template. :type content: unicode :param context</s>\n",
      "---------------------------\n",
      "Code: def main(argv): global g_script_name global g_num_clouds global g_nodes_per_cloud global g_output_dir global g_test_to_run global g_test_list_file global g_exclude_list_file global g_test_group global g_runner global g_nopass global g_nointernal global g_path_to_tar global g_path_to_whl global g_perf global g_git_hash global g_git_branch global g_machine_ip global g_date global g_build_id global g_ncpu global g_os global g_job_name global g_test_ssl g_script_name = os.path.basename(argv[0]) test_root_dir = os.path.realpath(os.getcwd()) g_output_dir = os.path.join(test_root_dir, str(\"results\")) g_failed_output_dir = os.path.join(g_output_dir, str(\"failed\")) testreport_dir = os.path.join(test_root_dir, str(\"../build/test-results\")) parse_args(argv) h2o_jar = g_path_to_h2o_jar if h2o_jar is None: possible_h2o_jar_parent_dir = test_root_dir while True: possible_h2o_jar_dir = os.path.join(possible_h2o_jar_parent_dir, \"build\") possible_h2o_jar = os.path.join(possible_h2o_jar_dir, \"h2o.jar\") if os.path.exists(possible_h2o_jar): h2o_jar = possible_h2o_jar break next_possible_h2o_jar_parent_dir = os.path.dirname(possible_h2o_jar_parent_dir) if next_possible_h2o_jar_parent_dir == possible_h2o_jar_parent_dir: break possible_h2o_jar_parent_dir = next_possible_h2o_jar_parent_dir if g_wipe_output_dir: wipe_output_dir() if g_wipe_test_state: wipe_test_state(test_root_dir) if g_test_to_run is not None: g_num_clouds = 1 g_runner = TestRunner(test_root_dir, g_use_cloud, g_use_cloud2, g_use_client, g_config, g_use_ip, g_use_port, g_num_clouds, g_nodes_per_cloud, h2o_jar, g_base_port, g_jvm_xmx, g_jvm_cp, g_output_dir, g_failed_output_dir, g_path_to_tar, g_path_to_whl, g_produce_unit_reports, testreport_dir, g_r_pkg_ver_chk, g_hadoop_namenode, g_on_hadoop, g_perf, g_test_ssl, g_ldap_config, g_jvm_opts) if g_exclude_list_file is not None: g_runner.read_exclude_list_file(g_exclude_list_file) if g_test_to_run is not None: g_runner.add_test(g_test_to_run) elif g_test_list_file is not None: g_runner.read_test_list_file(g_test_list_file) else: g_runner.build_test_list(g_test_group, g_run_small, g_run_medium, g_run_large, g_run_xlarge, g_nopass, g_nointernal) if g_no_run: sys.exit(0) signal.signal(signal.SIGINT, signal_handler) signal.signal(signal.SIGTERM, signal_handler) if not (h2o_jar and os.path.exists(h2o_jar)): print(\"\") print(\"ERROR: H2O jar not found\") print(\"\") sys.exit(1) try: g_runner.start_clouds() g_runner.run_tests(g_nopass) finally: g_runner.check_clouds() g_runner.stop_clouds() g_runner.report_summary(g_nopass) if not g_runner.get_regression_passed(): sys.exit(1)\n",
      "Target: Main program.\n",
      "    :param argv Command-line arguments\n",
      "    :return none\n",
      "Generated :<s>Parse command line arguments. :param argv: :return:</s>\n",
      "---------------------------\n",
      "Code: def from_image(cls, filename, start, stop, legend, source=\"Image\", col_offset=0.1, row_offset=2, tolerance=0): rgb = utils.loglike_from_image(filename, col_offset) loglike = np.array([utils.rgb_to_hex(t) for t in rgb]) tops, hexes = utils.tops_from_loglike(loglike, offset=row_offset) nonconsecutive = np.append(np.diff(tops), 2) tops = tops[nonconsecutive > 1] hexes = hexes[nonconsecutive > 1] hexes_reduced = list(set(hexes)) components = [legend.get_component(h, tolerance=tolerance) for h in hexes_reduced] values = [hexes_reduced.index(i) for i in hexes] basis = np.linspace(start, stop, loglike.size) list_of_Intervals = cls.__intervals_from_tops(tops, values, basis, components) return cls(list_of_Intervals, source=\"Image\")\n",
      "Target: Read an image and generate Striplog.\n",
      "\n",
      "        Args:\n",
      "            filename (str): An image file, preferably high-res PNG.\n",
      "            start (float or int): The depth at the top of the image.\n",
      "            stop (float or int): The depth at the bottom of the image.\n",
      "            legend (Legend): A legend to look up the components in.\n",
      "            source (str): A source for the data. Default: 'Image'.\n",
      "            col_offset (Number): The proportion of the way across the image\n",
      "                from which to extract the pixel column. Default: 0.1 (ie 10%).\n",
      "            row_offset (int): The number of pixels to skip at the top of\n",
      "                each change in colour. Default: 2.\n",
      "            tolerance (float): The Euclidean distance between hex colours,\n",
      "                which has a maximum (black to white) of 441.67 in base 10.\n",
      "                Default: 0.\n",
      "\n",
      "        Returns:\n",
      "            Striplog: The ``striplog`` object.\n",
      "Generated :<s>Creates a new :class:`Image` from an image. :param filename: The name of the image</s>\n",
      "---------------------------\n",
      "Code: def set_hit_fields(self, hit_fields): if not hit_fields: hit_fields_mapping_inverse = {} hit_fields_mapping = {} else: hit_fields_mapping_inverse = dict((k, v) for k, v in hit_fields.items()) hit_fields_mapping = dict((v, k) for k, v in hit_fields.items()) for old_name, new_name in self._default_hit_fields_mapping.items(): if old_name not in hit_fields_mapping: hit_fields_mapping[old_name] = new_name hit_fields_mapping_inverse[new_name] = old_name self._hit_fields_mapping = hit_fields_mapping self._hit_fields_mapping_inverse = hit_fields_mapping_inverse\n",
      "Target: Tell the clusterizer the meaning of the field names.\n",
      "\n",
      "        The hit_fields parameter is a dict, e.g., {\"new field name\": \"standard field name\"}.\n",
      "\n",
      "        If None default mapping is set.\n",
      "\n",
      "        Example:\n",
      "        --------\n",
      "        Internally, the clusterizer uses the hit fields names \"column\"/\"row\". If the name of the hits fields are \"x\"/\"y\", call:\n",
      "        set_hit_fields(hit_fields={'x': 'column',\n",
      "                                   'y': 'row'})\n",
      "Generated :<s>set_hit_fields(dict) -> hit_fields_mapping_inverse(dict, dict)</s>\n",
      "---------------------------\n",
      "Code: def databases_to_delete(cls): directory = PyFunceble.CURRENT_DIRECTORY result = [] result.append( directory + PyFunceble.CONFIGURATION[\"outputs\"][\"default_files\"][\"dir_structure\"] ) result.append( directory + PyFunceble.CONFIGURATION[\"outputs\"][\"default_files\"][\"iana\"] ) result.append( directory + PyFunceble.CONFIGURATION[\"outputs\"][\"default_files\"][\"public_suffix\"] ) result.append( directory + PyFunceble.CONFIGURATION[\"outputs\"][\"default_files\"][\"inactive_db\"] ) result.append( directory + PyFunceble.CONFIGURATION[\"outputs\"][\"default_files\"][\"mining\"] ) result.append( directory + PyFunceble.CONFIGURATION[\"outputs\"][\"default_files\"][\"whois_db\"] ) return result\n",
      "Target: Set the databases files to delete.\n",
      "Generated :<s>Returns a list of databases to delete. :return: A list of databases.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def arguments(self) -> List[Argument]: shadow = self._shadow split_spans = self._args_matcher(shadow).spans('arg') if not split_spans: return [] arguments = [] arguments_append = arguments.append type_to_spans = self._type_to_spans ss, se = span = self._span type_ = id(span) lststr = self._lststr string = lststr[0] arg_spans = type_to_spans.setdefault(type_, []) span_tuple_to_span_get = {(s[0], s[1]): s for s in arg_spans}.get for arg_self_start, arg_self_end in split_spans: s, e = arg_span = [ss + arg_self_start, ss + arg_self_end] old_span = span_tuple_to_span_get((s, e)) if old_span is None: insort(arg_spans, arg_span) else: arg_span = old_span arg = Argument(lststr, type_to_spans, arg_span, type_) arg._shadow_cache = ( string[s:e], shadow[arg_self_start:arg_self_end]) arguments_append(arg) return arguments\n",
      "Target: Parse template content. Create self.name and self.arguments.\n",
      "Generated :<s>Return a list of :class:`argparse.Argument`.</s>\n",
      "---------------------------\n",
      "Code: def init(name, *args): matcher = get(name) if not matcher: raise ValueError('Cannot find matcher: {}'.format(name)) return matcher(*args)\n",
      "Target: Initializes a matcher instance passing variadic arguments to\n",
      "    its constructor. Acts as a delegator proxy.\n",
      "\n",
      "    Arguments:\n",
      "        name (str): matcher class name or alias to execute.\n",
      "        *args (mixed): variadic argument\n",
      "\n",
      "    Returns:\n",
      "        matcher: matcher instance.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: if matcher was not found.\n",
      "Generated :<s>Initialize a matcher. Args: name (str): The name of the matcher. *args (</s>\n",
      "---------------------------\n",
      "Code: def init_widget(self): d = self.declaration if d.source: self.set_source(d.source) else: super(RawComponent, self).init_widget()\n",
      "Target: Initialize the widget with the source.\n",
      "Generated :<s>Initialize the widget.</s>\n",
      "---------------------------\n",
      "Code: def make_path_relative(path, rel_to): path_filename = os.path.basename(path) path = os.path.dirname(path) path = os.path.normpath(os.path.abspath(path)) rel_to = os.path.normpath(os.path.abspath(rel_to)) path_parts = path.strip(os.path.sep).split(os.path.sep) rel_to_parts = rel_to.strip(os.path.sep).split(os.path.sep) while path_parts and rel_to_parts and path_parts[0] == rel_to_parts[0]: path_parts.pop(0) rel_to_parts.pop(0) full_parts = ['..'] * len(rel_to_parts) + path_parts + [path_filename] if full_parts == ['']: return '.' + os.path.sep return os.path.sep.join(full_parts)\n",
      "Target: Make a filename relative, where the filename path, and it is\n",
      "    relative to rel_to\n",
      "\n",
      "        >>> make_path_relative('/usr/share/something/a-file.pth',\n",
      "        ...                    '/usr/share/another-place/src/Directory')\n",
      "        '../../../something/a-file.pth'\n",
      "        >>> make_path_relative('/usr/share/something/a-file.pth',\n",
      "        ...                    '/home/user/src/Directory')\n",
      "        '../../../usr/share/something/a-file.pth'\n",
      "        >>> make_path_relative('/usr/share/a-file.pth', '/usr/share/')\n",
      "        'a-file.pth'\n",
      "Generated :<s>Make a relative path relative to the given relative path. Args: path (str): relative path relative_to</s>\n",
      "---------------------------\n",
      "Code: def get_template(name): name = name.lower() templates = dict() templates['tarinfo'] = {\"gid\": 0, \"uid\": 0, \"uname\": \"root\", \"gname\": \"root\", \"mode\": 493} if name in templates: bot.debug(\"Found template for %s\" % (name)) return templates[name] else: bot.warning(\"Cannot find template %s\" % (name))\n",
      "Target: return a default template for some function in sregistry\n",
      "       If there is no template, None is returned.\n",
      "\n",
      "       Parameters\n",
      "       ==========\n",
      "       name: the name of the template to retrieve\n",
      "Generated :<s>Find a template for the given template name.</s>\n",
      "---------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb7c62457bd479d9881e5b4a88cc9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1653.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved due to improvement in 'val_loss' from 2.834005860653489 to 2.752179807413728\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def action_logging(f): @functools.wraps(f) def wrapper(*args, **kwargs): assert args assert isinstance(args[0], Namespace), \"1st positional argument should be argparse.Namespace instance, \" \"but {}\".format(args[0]) metrics = _build_metrics(f.__name__, args[0]) cli_action_loggers.on_pre_execution(**metrics) try: return f(*args, **kwargs) except Exception as e: metrics['error'] = e raise finally: metrics['end_datetime'] = datetime.utcnow() cli_action_loggers.on_post_execution(**metrics) return wrapper\n",
      "Target: Decorates function to execute function at the same time submitting action_logging\n",
      "    but in CLI context. It will call action logger callbacks twice,\n",
      "    one for pre-execution and the other one for post-execution.\n",
      "\n",
      "    Action logger will be called with below keyword parameters:\n",
      "        sub_command : name of sub-command\n",
      "        start_datetime : start datetime instance by utc\n",
      "        end_datetime : end datetime instance by utc\n",
      "        full_command : full command line arguments\n",
      "        user : current user\n",
      "        log : airflow.models.log.Log ORM instance\n",
      "        dag_id : dag id (optional)\n",
      "        task_id : task_id (optional)\n",
      "        execution_date : execution date (optional)\n",
      "        error : exception instance if there's an exception\n",
      "\n",
      "    :param f: function instance\n",
      "    :return: wrapped function\n",
      "Generated :<s>Decorator for action logging.</s>\n",
      "---------------------------\n",
      "Code: def connect_qtconsole(connection_file=None, argv=None, profile=None): argv = [] if argv is None else argv if connection_file is None: cf = get_connection_file() else: cf = find_connection_file(connection_file, profile=profile) cmd = ';'.join([ \"from IPython.frontend.qt.console import qtconsoleapp\", \"qtconsoleapp.main()\" ]) return Popen([sys.executable, '-c', cmd, '--existing', cf] + argv, stdout=PIPE, stderr=PIPE)\n",
      "Target: Connect a qtconsole to the current kernel.\n",
      "    \n",
      "    This is useful for connecting a second qtconsole to a kernel, or to a\n",
      "    local notebook.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    connection_file : str [optional]\n",
      "        The connection file to be used. Can be given by absolute path, or\n",
      "        IPython will search in the security directory of a given profile.\n",
      "        If run from IPython, \n",
      "        \n",
      "        If unspecified, the connection file for the currently running\n",
      "        IPython Kernel will be used, which is only allowed from inside a kernel.\n",
      "    argv : list [optional]\n",
      "        Any extra args to be passed to the console.\n",
      "    profile : str [optional]\n",
      "        The name of the profile to use when searching for the connection file,\n",
      "        if different from the current IPython session or 'default'.\n",
      "    \n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    subprocess.Popen instance running the qtconsole frontend\n",
      "Generated :<s>Connect Qtconsole to IPython.frontend.qt.console.</s>\n",
      "---------------------------\n",
      "Code: def random_walk_uniform_fn(scale=1., name=None): def _fn(state_parts, seed): with tf.compat.v1.name_scope( name, 'random_walk_uniform_fn', values=[state_parts, scale, seed]): scales = scale if mcmc_util.is_list_like(scale) else [scale] if len(scales) == 1: scales *= len(state_parts) if len(state_parts) != len(scales): raise ValueError('`scale` must broadcast with `state_parts`.') seed_stream = distributions.SeedStream(seed, salt='RandomWalkUniformFn') next_state_parts = [ tf.random.uniform( minval=state_part - scale_part, maxval=state_part + scale_part, shape=tf.shape(input=state_part), dtype=state_part.dtype.base_dtype, seed=seed_stream()) for scale_part, state_part in zip(scales, state_parts) ] return next_state_parts return _fn\n",
      "Target: Returns a callable that adds a random uniform perturbation to the input.\n",
      "\n",
      "  For more details on `random_walk_uniform_fn`, see\n",
      "  `random_walk_normal_fn`. `scale` might\n",
      "  be a `Tensor` or a list of `Tensor`s that should broadcast with state parts\n",
      "  of the `current_state`. The generated uniform perturbation is sampled as a\n",
      "  uniform point on the rectangle `[-scale, scale]`.\n",
      "\n",
      "  Args:\n",
      "    scale: a `Tensor` or Python `list` of `Tensor`s of any shapes and `dtypes`\n",
      "      controlling the upper and lower bound of the uniform proposal\n",
      "      distribution.\n",
      "    name: Python `str` name prefixed to Ops created by this function.\n",
      "        Default value: 'random_walk_uniform_fn'.\n",
      "\n",
      "  Returns:\n",
      "    random_walk_uniform_fn: A callable accepting a Python `list` of `Tensor`s\n",
      "      representing the state parts of the `current_state` and an `int`\n",
      "      representing the random seed used to generate the proposal. The callable\n",
      "      returns the same-type `list` of `Tensor`s as the input and represents the\n",
      "      proposal for the RWM algorithm.\n",
      "Generated :<s>Random walk uniform function. Args: scale: scale to use. name: optional name scope. Returns:</s>\n",
      "---------------------------\n",
      "Code: def _sentences(self, clean_visible): previous_end = 0 clean_visible = clean_visible.decode('utf8') for start, end in self.sentence_tokenizer.span_tokenize(clean_visible): if start < previous_end: start = previous_end if start > end: continue try: label = self.label_index.find_le(end) except ValueError: label = None if label: off = label.offsets[OffsetType.CHARS] end = max(off.first + off.length, end) previous_end = end sent_str = clean_visible[start:end] yield start, end, sent_str\n",
      "Target: generate strings identified as sentences\n",
      "Generated :<s>Iterate over all sentences in the sentence.</s>\n",
      "---------------------------\n",
      "Code: def _get_config_or_default(self, key, default, as_type=lambda x: x): if self.main_config.has_option(self.main_section, key): return as_type(self.main_config.get(self.main_section, key)) return default\n",
      "Target: Return a main config value, or default if it does not exist.\n",
      "Generated :<s>Get the configuration for the given key. Args: key (str): The name of the configuration. default (</s>\n",
      "---------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132bd0814eeb4a34b8e7a627567453bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=29253.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def reg_on_exit(self, callable_object, *args, **kwargs): persistent = kwargs.pop('persistent', False) event = self._create_event(callable_object, 'exit', persistent, *args, **kwargs) self.exit_callbacks.append(event) return event\n",
      "Target: Register a function/method to be called on program exit,\n",
      "        will get executed regardless of successs/failure of the program running\n",
      "Generated :<s>Registers an event to be executed when an exception is raised. Args: callable_object (callable</s>\n",
      "---------------------------\n",
      "Code: def signin_card(card: SigninCard) -> Attachment: if not isinstance(card, SigninCard): raise TypeError('CardFactory.signin_card(): `card` argument is not an instance of an SigninCard, ' 'unable to prepare attachment.') return Attachment(content_type=CardFactory.content_types.signin_card, content=card)\n",
      "Target: Returns an attachment for a signin card. For channels that don't natively support signin cards an alternative\n",
      "        message will be rendered. Will raise a TypeError if 'card' argument is not a SigninCard.\n",
      "        :param card:\n",
      "        :return:\n",
      "Generated :<s>Creates a :class:`SigninCard <CardFactory.content_types.signin_card></s>\n",
      "---------------------------\n",
      "Code: def add_template_filter(self, f, name=None): self.jinja_env.filters[name or f.__name__] = f\n",
      "Target: Register a custom template filter.  Works exactly like the\n",
      "        :meth:`template_filter` decorator.\n",
      "\n",
      "        :param name: the optional name of the filter, otherwise the\n",
      "                     function name will be used.\n",
      "Generated :<s>Add a filter to the template. :param f: Filter to add. :param name: Filter name.</s>\n",
      "---------------------------\n",
      "Code: def get_endpoint_path(self, endpoint_id): config = os.path.expanduser(\"~/.globusonline/lta/config-paths\") if not os.path.exists(config): bot.error('%s not found for a local Globus endpoint.') sys.exit(1) path = None config = [x.split(',')[0] for x in read_file(config)] for path in config: if os.path.exists(path): break if path is None: bot.error('No path was found for a local Globus endpoint.') sys.exit(1) return path\n",
      "Target: return the first fullpath to a folder in the endpoint based on\n",
      "       expanding the user's home from the globus config file. This\n",
      "       function is fragile but I don't see any other way to do it.\n",
      "    \n",
      "       Parameters\n",
      "       ==========\n",
      "       endpoint_id: the endpoint id to look up the path for\n",
      "Generated :<s>Get the Globus endpoint path. Args: endpoint_id (str): Globus endpoint identifier. Returns:</s>\n",
      "---------------------------\n",
      "Code: def minimize_one_step(gradient_unregularized_loss, hessian_unregularized_loss_outer, hessian_unregularized_loss_middle, x_start, tolerance, l1_regularizer, l2_regularizer=None, maximum_full_sweeps=1, learning_rate=None, name=None): graph_deps = [ gradient_unregularized_loss, hessian_unregularized_loss_outer, hessian_unregularized_loss_middle, x_start, l1_regularizer, l2_regularizer, maximum_full_sweeps, tolerance, learning_rate, ] with tf.compat.v1.name_scope(name, 'minimize_one_step', graph_deps): x_shape = _get_shape(x_start) batch_shape = x_shape[:-1] dims = x_shape[-1] def _hessian_diag_elt_with_l2(coord): inner_square = tf.reduce_sum( input_tensor=_sparse_or_dense_matmul_onehot( hessian_unregularized_loss_outer, coord)**2, axis=-1) unregularized_component = ( hessian_unregularized_loss_middle[..., coord] * inner_square) l2_component = _mul_or_none(2., l2_regularizer) return _add_ignoring_nones(unregularized_component, l2_component) grad_loss_with_l2 = _add_ignoring_nones( gradient_unregularized_loss, _mul_or_none(2., l2_regularizer, x_start)) x_update_diff_norm_sq_convergence_threshold = ( tolerance * (1. + tf.norm(tensor=x_start, ord=2, axis=-1))**2) update_shape = tf.concat([[dims], batch_shape], axis=-1) def _loop_cond(iter_, x_update_diff_norm_sq, x_update, hess_matmul_x_update): del x_update del hess_matmul_x_update sweep_complete = (iter_ > 0) & tf.equal(iter_ % dims, 0) small_delta = ( x_update_diff_norm_sq < x_update_diff_norm_sq_convergence_threshold) converged = sweep_complete & small_delta allowed_more_iterations = iter_ < maximum_full_sweeps * dims return allowed_more_iterations & tf.reduce_any(input_tensor=~converged) def _loop_body( iter_, x_update_diff_norm_sq, x_update, hess_matmul_x_update): coord = iter_ % dims x_update_diff_norm_sq = tf.where( tf.equal(coord, 0), tf.zeros_like(x_update_diff_norm_sq), x_update_diff_norm_sq) w_old = x_start[..., coord] + x_update[coord, ...] second_deriv = _hessian_diag_elt_with_l2(coord) newton_step = -_mul_ignoring_nones( learning_rate, grad_loss_with_l2[..., coord] + hess_matmul_x_update[coord, ...]) / second_deriv delta = ( soft_threshold( w_old + newton_step, _mul_ignoring_nones(learning_rate, l1_regularizer) / second_deriv) - w_old) def _do_update(x_update_diff_norm_sq, x_update, hess_matmul_x_update): hessian_column_with_l2 = sparse_or_dense_matvecmul( hessian_unregularized_loss_outer, hessian_unregularized_loss_middle * _sparse_or_dense_matmul_onehot( hessian_unregularized_loss_outer, coord), adjoint_a=True) if l2_regularizer is not None: hessian_column_with_l2 += _one_hot_like( hessian_column_with_l2, coord, on_value=2. * l2_regularizer) n = tf.rank(hessian_column_with_l2) perm = tf.roll(tf.range(n), shift=1, axis=0) hessian_column_with_l2 = tf.transpose( a=hessian_column_with_l2, perm=perm) x_update = tf.tensor_scatter_nd_add(x_update, [[coord]], [delta]) with tf.control_dependencies([x_update]): x_update_diff_norm_sq_ = x_update_diff_norm_sq + delta**2 hess_matmul_x_update_ = ( hess_matmul_x_update + delta * hessian_column_with_l2) x_update_diff_norm_sq_.set_shape( x_update_diff_norm_sq_.shape.merge_with( x_update_diff_norm_sq.shape)) hess_matmul_x_update_.set_shape( hess_matmul_x_update_.shape.merge_with( hess_matmul_x_update.shape)) return [x_update_diff_norm_sq_, x_update, hess_matmul_x_update_] inputs_to_update = [x_update_diff_norm_sq, x_update, hess_matmul_x_update] return [iter_ + 1] + prefer_static.cond( tf.reduce_all(input_tensor=tf.equal(delta, 0.)), lambda: inputs_to_update, lambda: _do_update(*inputs_to_update)) base_dtype = x_start.dtype.base_dtype iter_, x_update_diff_norm_sq, x_update, _ = tf.while_loop( cond=_loop_cond, body=_loop_body, loop_vars=[ tf.zeros([], dtype=np.int32, name='iter'), tf.zeros( batch_shape, dtype=base_dtype, name='x_update_diff_norm_sq'), tf.zeros(update_shape, dtype=base_dtype, name='x_update'), tf.zeros( update_shape, dtype=base_dtype, name='hess_matmul_x_update'), ]) n = tf.rank(x_update) perm = tf.roll(tf.range(n), shift=-1, axis=0) x_update = tf.transpose(a=x_update, perm=perm) converged = tf.reduce_all(input_tensor=x_update_diff_norm_sq < x_update_diff_norm_sq_convergence_threshold) return x_start + x_update, converged, iter_ / dims\n",
      "Target: One step of (the outer loop of) the minimization algorithm.\n",
      "\n",
      "  This function returns a new value of `x`, equal to `x_start + x_update`.  The\n",
      "  increment `x_update in R^n` is computed by a coordinate descent method, that\n",
      "  is, by a loop in which each iteration updates exactly one coordinate of\n",
      "  `x_update`.  (Some updates may leave the value of the coordinate unchanged.)\n",
      "\n",
      "  The particular update method used is to apply an L1-based proximity operator,\n",
      "  \"soft threshold\", whose fixed point `x_update_fix` is the desired minimum\n",
      "\n",
      "  ```none\n",
      "  x_update_fix = argmin{\n",
      "      Loss(x_start + x_update')\n",
      "        + l1_regularizer * ||x_start + x_update'||_1\n",
      "        + l2_regularizer * ||x_start + x_update'||_2**2\n",
      "      : x_update' }\n",
      "  ```\n",
      "\n",
      "  where in each iteration `x_update'` is constrained to have at most one nonzero\n",
      "  coordinate.\n",
      "\n",
      "  This update method preserves sparsity, i.e., tends to find sparse solutions if\n",
      "  `x_start` is sparse.  Additionally, the choice of step size is based on\n",
      "  curvature (Hessian), which significantly speeds up convergence.\n",
      "\n",
      "  This algorithm assumes that `Loss` is convex, at least in a region surrounding\n",
      "  the optimum.  (If `l2_regularizer > 0`, then only weak convexity is needed.)\n",
      "\n",
      "  Args:\n",
      "    gradient_unregularized_loss: (Batch of) `Tensor` with the same shape and\n",
      "      dtype as `x_start` representing the gradient, evaluated at `x_start`, of\n",
      "      the unregularized loss function (denoted `Loss` above).  (In all current\n",
      "      use cases, `Loss` is the negative log likelihood.)\n",
      "    hessian_unregularized_loss_outer: (Batch of) `Tensor` or `SparseTensor`\n",
      "      having the same dtype as `x_start`, and shape `[N, n]` where `x_start` has\n",
      "      shape `[n]`, satisfying the property\n",
      "      `Transpose(hessian_unregularized_loss_outer)\n",
      "      @ diag(hessian_unregularized_loss_middle)\n",
      "      @ hessian_unregularized_loss_inner\n",
      "      = (approximation of) Hessian matrix of Loss, evaluated at x_start`.\n",
      "    hessian_unregularized_loss_middle: (Batch of) vector-shaped `Tensor` having\n",
      "      the same dtype as `x_start`, and shape `[N]` where\n",
      "      `hessian_unregularized_loss_outer` has shape `[N, n]`, satisfying the\n",
      "      property\n",
      "      `Transpose(hessian_unregularized_loss_outer)\n",
      "      @ diag(hessian_unregularized_loss_middle)\n",
      "      @ hessian_unregularized_loss_inner\n",
      "      = (approximation of) Hessian matrix of Loss, evaluated at x_start`.\n",
      "    x_start: (Batch of) vector-shaped, `float` `Tensor` representing the current\n",
      "      value of the argument to the Loss function.\n",
      "    tolerance: scalar, `float` `Tensor` representing the convergence threshold.\n",
      "      The optimization step will terminate early, returning its current value of\n",
      "      `x_start + x_update`, once the following condition is met:\n",
      "      `||x_update_end - x_update_start||_2 / (1 + ||x_start||_2)\n",
      "      < sqrt(tolerance)`,\n",
      "      where `x_update_end` is the value of `x_update` at the end of a sweep and\n",
      "      `x_update_start` is the value of `x_update` at the beginning of that\n",
      "      sweep.\n",
      "    l1_regularizer: scalar, `float` `Tensor` representing the weight of the L1\n",
      "      regularization term (see equation above).  If L1 regularization is not\n",
      "      required, then `tfp.glm.fit_one_step` is preferable.\n",
      "    l2_regularizer: scalar, `float` `Tensor` representing the weight of the L2\n",
      "      regularization term (see equation above).\n",
      "      Default value: `None` (i.e., no L2 regularization).\n",
      "    maximum_full_sweeps: Python integer specifying maximum number of sweeps to\n",
      "      run.  A \"sweep\" consists of an iteration of coordinate descent on each\n",
      "      coordinate. After this many sweeps, the algorithm will terminate even if\n",
      "      convergence has not been reached.\n",
      "      Default value: `1`.\n",
      "    learning_rate: scalar, `float` `Tensor` representing a multiplicative factor\n",
      "      used to dampen the proximal gradient descent steps.\n",
      "      Default value: `None` (i.e., factor is conceptually `1`).\n",
      "    name: Python string representing the name of the TensorFlow operation.\n",
      "      The default name is `\"minimize_one_step\"`.\n",
      "\n",
      "  Returns:\n",
      "    x: (Batch of) `Tensor` having the same shape and dtype as `x_start`,\n",
      "      representing the updated value of `x`, that is, `x_start + x_update`.\n",
      "    is_converged: scalar, `bool` `Tensor` indicating whether convergence\n",
      "      occurred across all batches within the specified number of sweeps.\n",
      "    iter: scalar, `int` `Tensor` representing the actual number of coordinate\n",
      "      updates made (before achieving convergence).  Since each sweep consists of\n",
      "      `tf.size(x_start)` iterations, the maximum number of updates is\n",
      "      `maximum_full_sweeps * tf.size(x_start)`.\n",
      "\n",
      "  #### References\n",
      "\n",
      "  [1]: Jerome Friedman, Trevor Hastie and Rob Tibshirani. Regularization Paths\n",
      "       for Generalized Linear Models via Coordinate Descent. _Journal of\n",
      "       Statistical Software_, 33(1), 2010.\n",
      "       https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf\n",
      "\n",
      "  [2]: Guo-Xun Yuan, Chia-Hua Ho and Chih-Jen Lin. An Improved GLMNET for\n",
      "       L1-regularized Logistic Regression. _Journal of Machine Learning\n",
      "       Research_, 13, 2012.\n",
      "       http://www.jmlr.org/papers/volume13/yuan12a/yuan12a.pdf\n",
      "Generated :<s>Minimize one-step loss. Args: gradient_unregularized_loss: The unregularized loss</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def push(self, path, name, tag=None): path = os.path.abspath(path) bot.debug(\"PUSH %s\" % path) if not os.path.exists(path): bot.error('%s does not exist.' %path) sys.exit(1) names = parse_image_name(remove_uri(name), tag=tag) metadata = self.get_metadata(path, names=names) file_size = os.path.getsize(path) chunk_size = 4 * 1024 * 1024 storage_path = \"/%s\" %names['storage'] progress = 0 bot.show_progress(progress, file_size, length=35) with open(path, 'rb') as F: if file_size <= chunk_size: self.dbx.files_upload(F.read(), storage_path) else: start = self.dbx.files_upload_session_start(F.read(chunk_size)) cursor = dropbox.files.UploadSessionCursor(session_id=start.session_id, offset=F.tell()) commit = dropbox.files.CommitInfo(path=storage_path) while F.tell() < file_size: progress+=chunk_size if ((file_size - F.tell()) <= chunk_size): self.dbx.files_upload_session_finish(F.read(chunk_size), cursor, commit) else: self.dbx.files_upload_session_append(F.read(chunk_size), cursor.session_id, cursor.offset) cursor.offset = F.tell() bot.show_progress(iteration=progress, total=file_size, length=35, carriage_return=False) bot.show_progress(iteration=file_size, total=file_size, length=35, carriage_return=True) sys.stdout.write('\\n')\n",
      "Target: push an image to your Dropbox\n",
      "   \n",
      "       Parameters\n",
      "       ==========\n",
      "       path: should correspond to an absolute image path (or derive it)\n",
      "       name: should be the complete uri that the user has requested to push.\n",
      "       tag: should correspond with an image tag. This is provided to mirror Docker\n",
      "\n",
      "       if the image is less than 150MB, the standard file_upload is used. If\n",
      "       larger, the image is uploaded in chunks with a progress bar.\n",
      "Generated :<s>Uploads an image to the database. Args: path (str): Path to upload. name (str):</s>\n",
      "---------------------------\n",
      "Code: def del_handler(self, args): self.validate('cmd|s3', args) source = args[1] self.s3handler().del_files(source)\n",
      "Target: Handler for del command\n",
      "Generated :<s>Perform a DELETE command.</s>\n",
      "---------------------------\n",
      "Code: def as_future(self, query): if not self._pool: self._pool = ThreadPoolExecutor(max_workers=self._max_workers) old_future = self._pool.submit(query) new_future = Future() IOLoop.current().add_future( old_future, lambda f: chain_future(f, new_future) ) return new_future\n",
      "Target: Wrap a `sqlalchemy.orm.query.Query` object into a\n",
      "        `concurrent.futures.Future` so that it can be yielded.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        query : sqlalchemy.orm.query.Query\n",
      "            SQLAlchemy query object to execute\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "            tornado.concurrent.Future\n",
      "                A `Future` object wrapping the given query so that tornado can\n",
      "                await/yield on it\n",
      "Generated :<s>Creates a new :class:`Future` for the given query. :param str query: The query to</s>\n",
      "---------------------------\n",
      "Code: def _get_build_prefix(): path = os.path.join( tempfile.gettempdir(), 'pip_build_%s' % __get_username().replace(' ', '_') ) if WINDOWS: return path try: os.mkdir(path) write_delete_marker_file(path) except OSError: file_uid = None try: file_uid = get_path_uid(path) except OSError: file_uid = None if file_uid != os.geteuid(): msg = ( \"The temporary folder for building (%s) is either not owned by\" \" you, or is a symlink.\" % path ) print(msg) print( \"pip will not work until the temporary folder is either \" \"deleted or is a real directory owned by your user account.\" ) raise exceptions.InstallationError(msg) return path\n",
      "Target: Returns a safe build_prefix\n",
      "Generated :<s>Get the name of the build prefix.</s>\n",
      "---------------------------\n",
      "Code: def _pre_init(self): if self.parsed_args.git_log: git_path = self.parsed_args.git_log elif not self.parsed_args.git_path: base_path = os.path.expanduser('~/.perceval/repositories/') processed_uri = self.parsed_args.uri.lstrip('/') git_path = os.path.join(base_path, processed_uri) + '-git' else: git_path = self.parsed_args.git_path setattr(self.parsed_args, 'gitpath', git_path)\n",
      "Target: Initialize repositories directory path\n",
      "Generated :<s>Initialize the git_path and git_path.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def show(self, use_pandas=False, rows=10, cols=200): if self._ex is None: print(\"This H2OFrame has been removed.\") return if not self._has_content(): print(\"This H2OFrame is empty and not initialized.\") return if self.nrows == 0: print(\"This H2OFrame is empty.\") return if not self._ex._cache.is_valid(): self._frame()._ex._cache.fill() if H2ODisplay._in_ipy(): import IPython.display if use_pandas and can_use_pandas(): IPython.display.display(self.head(rows=rows, cols=cols).as_data_frame(fill_cache=True)) else: IPython.display.display_html(self._ex._cache._tabulate(\"html\", False), raw=True) else: if use_pandas and can_use_pandas(): print(self.head(rows=rows, cols=cols).as_data_frame(True)) else: s = self.__unicode__() stk = traceback.extract_stack() if \"IPython\" in stk[-3][0]: s = \"\\n%s\" % s try: print(s) except UnicodeEncodeError: print(s.encode(\"ascii\", \"replace\"))\n",
      "Target: Used by the H2OFrame.__repr__ method to print or display a snippet of the data frame.\n",
      "\n",
      "        If called from IPython, displays an html'ized result. Else prints a tabulate'd result.\n",
      "Generated :<s>Display the contents of this H2OFrame. :param use_pandas: Whether to use pandas</s>\n",
      "---------------------------\n",
      "Code: def to_scientific_tuple(number): convert = not isinstance(number, str) if (convert and (number == 0)) or ( (not convert) and (not number.strip(\"0\").strip(\".\")) ): return (\"0\", 0) sign, digits, exp = Decimal(str(number) if convert else number).as_tuple() mant = ( \"{sign}{itg}{frac}\".format( sign=\"-\" if sign else \"\", itg=digits[0], frac=( \".{frac}\".format(frac=\"\".join([str(num) for num in digits[1:]])) if len(digits) > 1 else \"\" ), ) .rstrip(\"0\") .rstrip(\".\") ) exp += len(digits) - 1 return NumComp(mant, exp)\n",
      "Target: Return mantissa and exponent of a number in scientific notation.\n",
      "\n",
      "    Full precision is maintained if the number is represented as a string\n",
      "\n",
      "    :param number: Number\n",
      "    :type  number: integer, float or string\n",
      "\n",
      "    :rtype: named tuple in which the first item is the mantissa (*string*)\n",
      "            and the second item is the exponent (*integer*) of the number\n",
      "            when expressed in scientific notation\n",
      "\n",
      "    For example:\n",
      "\n",
      "        >>> import peng\n",
      "        >>> peng.to_scientific_tuple('135.56E-8')\n",
      "        NumComp(mant='1.3556', exp=-6)\n",
      "        >>> peng.to_scientific_tuple(0.0000013556)\n",
      "        NumComp(mant='1.3556', exp=-6)\n",
      "Generated :<s>Convert a number to a scientific notation.</s>\n",
      "---------------------------\n",
      "Code: def generate(secret, age, **payload): jti = str(uuid.uuid1()) if not payload: payload = {} payload['exp'] = int(time.time() + age) payload['jti'] = jti return jwt.encode(payload, decode_secret(secret))\n",
      "Target: Generate a one-time jwt with an age in seconds\n",
      "Generated :<s>Generate a JWT. :param secret: The JWT secret. :param age: The age of the</s>\n",
      "---------------------------\n",
      "Code: def import_users(self, directory): exportInfoFile = os.path.join(directory, \"connectordb.json\") with open(exportInfoFile) as f: exportInfo = json.load(f) if exportInfo[\"Version\"] != 1: raise ValueError(\"Not able to read this import version\") for name in os.listdir(directory): udir = os.path.join(directory, name) if os.path.isdir(udir): with open(os.path.join(udir, \"user.json\")) as f: usrdata = json.load(f) u = self(usrdata[\"name\"]) if u.exists(): raise ValueError(\"The user \" + name + \" already exists\") del usrdata[\"name\"] u.create(password=name, **usrdata) for dname in os.listdir(udir): ddir = os.path.join(udir, dname) if os.path.isdir(ddir): u.import_device(ddir)\n",
      "Target: Imports version 1 of ConnectorDB export. These exports can be generated\n",
      "        by running user.export(dir), possibly on multiple users.\n",
      "Generated :<s>Import all users from the given directory.</s>\n",
      "---------------------------\n",
      "Code: def getmask(inds, n): inds = asarray(inds, 'int') mask = zeros(n, dtype=bool) mask[inds] = True return mask\n",
      "Target: Obtain a binary mask by setting a subset of entries to true.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        inds : array-like\n",
      "            Which indices to set as true.\n",
      "\n",
      "        n : int\n",
      "            The length of the target mask.\n",
      "Generated :<s>Convert a n-dimensional array to a boolean mask. Parameters ---------- inds : numpy.ndarray</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def dict_merge(lft, rgt): if not isinstance(rgt, dict): return rgt result = deepcopy(lft) for key, val in rgt.iteritems(): if key in result and isinstance(result[key], dict): result[key] = dict_merge(result[key], val) else: result[key] = deepcopy(val) return result\n",
      "Target: Recursive dict merge.\n",
      "\n",
      "    Recursively merges dict's. not just simple lft['key'] = rgt['key'], if\n",
      "    both lft and rgt have a key who's value is a dict then dict_merge is\n",
      "    called on both values and the result stored in the returned dictionary.\n",
      "Generated :<s>Merge two dictionaries into a single dict.</s>\n",
      "---------------------------\n",
      "Code: def features(self): if self._features is None: try: self.read_features() except (NoFeaturesFileError, FeaturesNotFound, WrongFeaturesFormatError, FeatureParamsError) as e: try: self._compute_all_features() self.write_features() except IOError: if isinstance(e, FeaturesNotFound) or isinstance(e, FeatureParamsError): msg = \"Computation of the features is needed for \" \"current parameters but no audio file was found.\" \"Please, change your parameters or add the audio\" \" file in %s\" else: msg = \"Couldn't find audio file in %s\" raise NoAudioFileError(msg % self.file_struct.audio_file) if self.feat_type is FeatureTypes.framesync: self._features = self._framesync_features elif self.feat_type is FeatureTypes.est_beatsync: self._features = self._est_beatsync_features elif self.feat_type is FeatureTypes.ann_beatsync: if self._ann_beatsync_features is None: raise FeatureTypeNotFound( \"Feature type %s is not valid because no annotated beats \" \"were found\" % self.feat_type) self._features = self._ann_beatsync_features else: raise FeatureTypeNotFound(\"Feature type %s is not valid.\" % self.feat_type) return self._features\n",
      "Target: This getter will compute the actual features if they haven't\n",
      "        been computed yet.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        features: np.array\n",
      "            The actual features. Each row corresponds to a feature vector.\n",
      "Generated :<s>Retrieve the features from the audio file and store them in self._features. If self._features is None,</s>\n",
      "---------------------------\n",
      "Code: def calc_fwhm(distribution, is_neg_log=True): if isinstance(distribution, interp1d): if is_neg_log: ymin = distribution.y.min() log_prob = distribution.y-ymin else: log_prob = -np.log(distribution.y) log_prob -= log_prob.min() xvals = distribution.x elif isinstance(distribution, Distribution): xvals = distribution._func.x log_prob = distribution._func.y else: raise TypeError(\"Error in computing the FWHM for the distribution. \" \" The input should be either Distribution or interpolation object\"); L = xvals.shape[0] tmp = np.where(log_prob < 0.693147)[0] x_l, x_u = tmp[0], tmp[-1] if L < 2: print (\"Not enough points to compute FWHM: returning zero\") return min(TINY_NUMBER, distribution.xmax - distribution.xmin) else: return max(TINY_NUMBER, xvals[min(x_u+1,L-1)] - xvals[max(0,x_l-1)])\n",
      "Target: Assess the width of the probability distribution. This returns\n",
      "        full-width-half-max\n",
      "Generated :<s>Calculate the FWHM of a distribution. Args: distribution: The distribution to calculate FWHM</s>\n",
      "---------------------------\n",
      "Code: def _finalize(self, store_meta_data=True): self._is_run = False self.f_set_crun(None) if store_meta_data: self.f_store(only_init=True)\n",
      "Target: Final rollback initiated by the environment\n",
      "\n",
      "        Restores the trajectory as root of the tree, and stores meta data to disk.\n",
      "        This updates the trajectory's information about single runs, i.e. if they've been\n",
      "        completed, when they were started, etc.\n",
      "Generated :<s>Finalize the data store. Args: store_meta_data (bool): Whether to store the meta data</s>\n",
      "---------------------------\n",
      "Code: def use_privatekey(self, pkey): if not isinstance(pkey, PKey): raise TypeError(\"pkey must be a PKey instance\") use_result = _lib.SSL_CTX_use_PrivateKey(self._context, pkey._pkey) if not use_result: self._raise_passphrase_exception()\n",
      "Target: Load a private key from a PKey object\n",
      "\n",
      "        :param pkey: The PKey object\n",
      "        :return: None\n",
      "Generated :<s>Use a private key. :param pkey: The private key to use. :type pkey: :class</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def delete_qubits(self, indices): if not isinstance(indices, list): indices = [indices] self._z = np.delete(self._z, indices) self._x = np.delete(self._x, indices) return self\n",
      "Target: Delete pauli at the indices.\n",
      "\n",
      "        Args:\n",
      "            indices(list[int]): the indices of to-be-deleted paulis\n",
      "\n",
      "        Returns:\n",
      "            Pauli: self\n",
      "Generated :<s>Delete the qubits at the given indices. :param indices: The indices to delete. :type indices: list</s>\n",
      "---------------------------\n",
      "Code: def build_constrained_seasonal_transition_noise( drift_scale, num_seasons, is_last_day_of_season): drift_scale_tril_nonzeros = tf.concat([ tf.ones([num_seasons - 1, 1], dtype=drift_scale.dtype), tf.zeros([num_seasons - 1, num_seasons - 2], dtype=drift_scale.dtype)], axis=-1) drift_scale_tril = (drift_scale_tril_nonzeros * drift_scale[..., tf.newaxis, tf.newaxis] / num_seasons) def seasonal_transition_noise(t): noise_scale_tril = dist_util.pick_scalar_condition( is_last_day_of_season(t), drift_scale_tril, tf.zeros_like(drift_scale_tril)) return tfd.MultivariateNormalTriL( loc=tf.zeros(num_seasons-1, dtype=drift_scale.dtype), scale_tril=noise_scale_tril) return seasonal_transition_noise\n",
      "Target: Build transition noise distribution for a ConstrainedSeasonalSSM.\n",
      "Generated :<s>Parameters ---------- drift_scale : float or np.ndarray, shape=(n_samples, n_</s>\n",
      "---------------------------\n",
      "Code: def np_dtype_to_point_format(dtype, unpacked=False): all_dtypes = ( ALL_POINT_FORMATS_DTYPE if not unpacked else UNPACKED_POINT_FORMATS_DTYPES ) for format_id, fmt_dtype in all_dtypes.items(): if fmt_dtype == dtype: return format_id else: raise errors.IncompatibleDataFormat( \"Data type of array is not compatible with any point format (array dtype: {})\".format( dtype ) )\n",
      "Target: Tries to find a matching point format id for the input numpy dtype\n",
      "    To match, the input dtype has to be 100% equal to a point format dtype\n",
      "    so all names & dimensions types must match\n",
      "\n",
      "    Parameters:\n",
      "    ----------\n",
      "    dtype : numpy.dtype\n",
      "        The input dtype\n",
      "    unpacked : bool, optional\n",
      "        [description] (the default is False, which [default_description])\n",
      "\n",
      "    Raises\n",
      "    ------\n",
      "    errors.IncompatibleDataFormat\n",
      "        If No compatible point format was found\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    int\n",
      "        The compatible point format found\n",
      "Generated :<s>Convert a numpy dtype to a point format. Args: dtype (numpy.ndarray</s>\n",
      "---------------------------\n",
      "Code: def _normalize_name(name): try: return HTTPHeaders._normalized_headers[name] except KeyError: if HTTPHeaders._NORMALIZED_HEADER_RE.match(name): normalized = name else: normalized = \"-\".join([w.capitalize() for w in name.split(\"-\")]) HTTPHeaders._normalized_headers[name] = normalized return normalized\n",
      "Target: Converts a name to Http-Header-Case.\n",
      "\n",
      "        >>> HTTPHeaders._normalize_name(\"coNtent-TYPE\")\n",
      "        'Content-Type'\n",
      "Generated :<s>Return a normalized version of the given name.</s>\n",
      "---------------------------\n",
      "Code: def from_regression(cls, clock_model): dc = cls() dc.clock_rate = clock_model['slope'] dc.intercept = clock_model['intercept'] dc.chisq = clock_model['chisq'] if 'chisq' in clock_model else None dc.valid_confidence = clock_model['valid_confidence'] if 'valid_confidence' in clock_model else False if 'cov' in clock_model and dc.valid_confidence: dc.cov = clock_model['cov'] dc.r_val = clock_model['r_val'] return dc\n",
      "Target: Create the conversion object automatically from the tree\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "\n",
      "         clock_model : dict\n",
      "            dictionary as returned from TreeRegression with fields intercept and slope\n",
      "Generated :<s>Create a new DC from the given clock model. Parameters ---------- clock_model : :class:`dict`</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def _recurse(self, inputs, output): if inputs: my_input = inputs[0] name = my_input.name if my_input.state: my_options = my_input.options(self.state) else: my_options = my_input.options for option in my_options: my_output = list(output) my_output.append({name: option}) self._recurse(inputs[1:], my_output) else: try: valid, result = self._function(output) except ValueError: raise RuntimeError(\"function must return 2 values\") print output, valid, result\n",
      "Target: internal recursion routine called by the run method that generates\n",
      "        all input combinations\n",
      "Generated :<s>Recurse through the output of the function. :param inputs: :param output: :return:</s>\n",
      "---------------------------\n",
      "Code: def _no_context_variadic(node, variadic_name, variadic_type, variadics): statement = node.statement() for name in statement.nodes_of_class(astroid.Name): if name.name != variadic_name: continue inferred = safe_infer(name) if isinstance(inferred, (astroid.List, astroid.Tuple)): length = len(inferred.elts) elif isinstance(inferred, astroid.Dict): length = len(inferred.items) else: continue inferred_statement = inferred.statement() if not length and isinstance(inferred_statement, astroid.FunctionDef): is_in_starred_context = _has_parent_of_type(node, variadic_type, statement) used_as_starred_argument = _is_name_used_as_variadic(name, variadics) if is_in_starred_context or used_as_starred_argument: return True return False\n",
      "Target: Verify if the given call node has variadic nodes without context\n",
      "\n",
      "    This is a workaround for handling cases of nested call functions\n",
      "    which don't have the specific call context at hand.\n",
      "    Variadic arguments (variable positional arguments and variable\n",
      "    keyword arguments) are inferred, inherently wrong, by astroid\n",
      "    as a Tuple, respectively a Dict with empty elements.\n",
      "    This can lead pylint to believe that a function call receives\n",
      "    too few arguments.\n",
      "Generated :<s>Return True if variadic_name, variadic_type, variadics are in the statement.</s>\n",
      "---------------------------\n",
      "Code: def create_foundation(length, width, depth=0.0, height=0.0): a_foundation = FoundationRaft() a_foundation.length = length a_foundation.width = width a_foundation.depth = depth a_foundation.height = height return a_foundation\n",
      "Target: Can define a Foundation Object from dimensions.\n",
      "    :param length: Foundation length\n",
      "    :param width: Foundation width\n",
      "    :param depth: Foundation depth\n",
      "    :param height: Foundation height\n",
      "    :return: A Foundation object\n",
      "Generated :<s>Creates a new Foundation. :param length: :param width: :param depth: :param height: :</s>\n",
      "---------------------------\n",
      "Code: def threaded_quit(self, arg): threading_list = threading.enumerate() mythread = threading.currentThread() for t in threading_list: if t != mythread: ctype_async_raise(t, Mexcept.DebuggerQuit) pass pass raise Mexcept.DebuggerQuit\n",
      "Target: quit command when several threads are involved.\n",
      "Generated :<s>Queries the thread to exit.</s>\n",
      "---------------------------\n",
      "Code: def get_private_keys( self, index=0, count=1, security_level=AddressGenerator.DEFAULT_SECURITY_LEVEL, ): return commands.GetPrivateKeysCommand(self.adapter)( seed=self.seed, index=index, count=count, securityLevel=security_level, )\n",
      "Target: Generates one or more private keys from the seed.\n",
      "\n",
      "        As the name implies, private keys should not be shared.\n",
      "        However, in a few cases it may be necessary (e.g., for M-of-N\n",
      "        transactions).\n",
      "\n",
      "        :param index:\n",
      "            The starting key index.\n",
      "\n",
      "        :param count:\n",
      "            Number of keys to generate.\n",
      "\n",
      "        :param security_level:\n",
      "            Number of iterations to use when generating new keys.\n",
      "\n",
      "            Larger values take longer, but the resulting signatures are\n",
      "            more secure.\n",
      "\n",
      "            This value must be between 1 and 3, inclusive.\n",
      "\n",
      "        :return:\n",
      "            Dict with the following items::\n",
      "\n",
      "                {\n",
      "                    'keys': List[PrivateKey],\n",
      "                        Always contains a list, even if only one key was\n",
      "                        generated.\n",
      "                }\n",
      "\n",
      "        References:\n",
      "\n",
      "        - :py:class:`iota.crypto.signing.KeyGenerator`\n",
      "        - https://github.com/iotaledger/wiki/blob/master/multisigs.md#how-m-of-n-works\n",
      "Generated :<s>Get a list of private keys. :param int index: :param int count: :param security_level:</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def purge_deleted( self, vault_name, location, custom_headers=None, raw=False, polling=True, **operation_config): raw_result = self._purge_deleted_initial( vault_name=vault_name, location=location, custom_headers=custom_headers, raw=True, **operation_config ) def get_long_running_output(response): if raw: client_raw_response = ClientRawResponse(None, response) return client_raw_response lro_delay = operation_config.get( 'long_running_operation_timeout', self.config.long_running_operation_timeout) if polling is True: polling_method = ARMPolling(lro_delay, **operation_config) elif polling is False: polling_method = NoPolling() else: polling_method = polling return LROPoller(self._client, raw_result, get_long_running_output, polling_method)\n",
      "Target: Permanently deletes the specified vault. aka Purges the deleted Azure\n",
      "        key vault.\n",
      "\n",
      "        :param vault_name: The name of the soft-deleted vault.\n",
      "        :type vault_name: str\n",
      "        :param location: The location of the soft-deleted vault.\n",
      "        :type location: str\n",
      "        :param dict custom_headers: headers that will be added to the request\n",
      "        :param bool raw: The poller return type is ClientRawResponse, the\n",
      "         direct response alongside the deserialized response\n",
      "        :param polling: True for ARMPolling, False for no polling, or a\n",
      "         polling object for personal polling strategy\n",
      "        :return: An instance of LROPoller that returns None or\n",
      "         ClientRawResponse<None> if raw==True\n",
      "        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n",
      "         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n",
      "        :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n",
      "Generated :<s>Purge a deleted vault. :param vault_name: The name of the vault. :type vault_name</s>\n",
      "---------------------------\n",
      "Code: def reduce_in_chunks(fn, iterable, initializer, chunk_size=0): if len(iterable) == 0: return initializer if chunk_size == 0: chunk_size = len(iterable) return reduce(fn, chunks(iterable, chunk_size), initializer)\n",
      "Target: Reduce the given list of items by splitting it into chunks\n",
      "    of the given size and passing each chunk through the reducer\n",
      "Generated :<s>Reduce an iterable into a single chunk. Args: fn: The function to reduce. iterable:</s>\n",
      "---------------------------\n",
      "Code: def find_code_units(self, morfs): morfs = morfs or self.coverage.data.measured_files() file_locator = self.coverage.file_locator self.code_units = code_unit_factory(morfs, file_locator) if self.config.include: patterns = prep_patterns(self.config.include) filtered = [] for cu in self.code_units: for pattern in patterns: if fnmatch.fnmatch(cu.filename, pattern): filtered.append(cu) break self.code_units = filtered if self.config.omit: patterns = prep_patterns(self.config.omit) filtered = [] for cu in self.code_units: for pattern in patterns: if fnmatch.fnmatch(cu.filename, pattern): break else: filtered.append(cu) self.code_units = filtered self.code_units.sort()\n",
      "Target: Find the code units we'll report on.\n",
      "\n",
      "        `morfs` is a list of modules or filenames.\n",
      "Generated :<s>Find all code units for a given morfs.</s>\n",
      "---------------------------\n",
      "Code: def phistogram(view, a, bins=10, rng=None, normed=False): nengines = len(view.targets) with view.sync_imports(): import numpy rets = view.apply_sync(lambda a, b, rng: numpy.histogram(a,b,rng), Reference(a), bins, rng) hists = [ r[0] for r in rets ] lower_edges = [ r[1] for r in rets ] lower_edges = view.pull('lower_edges', targets=0) hist_array = numpy.array(hists).reshape(nengines, -1) total_hist = numpy.sum(hist_array, 0) if normed: total_hist = total_hist/numpy.sum(total_hist,dtype=float) return total_hist, lower_edges\n",
      "Target: Compute the histogram of a remote array a.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "        view\n",
      "            IPython DirectView instance\n",
      "        a : str\n",
      "            String name of the remote array\n",
      "        bins : int\n",
      "            Number of histogram bins\n",
      "        rng : (float, float)\n",
      "            Tuple of min, max of the range to histogram\n",
      "        normed : boolean\n",
      "            Should the histogram counts be normalized to 1\n",
      "Generated :<s>Calculate the histogram for a given window. Parameters ---------- view : view A view object. a :</s>\n",
      "---------------------------\n",
      "Code: def _save_method_args(self, *args, **kwargs): self._method_args = args self._method_kwargs = kwargs\n",
      "Target: Save the args and kwargs to get/post/put/delete for future use.\n",
      "\n",
      "        These arguments are not saved in the request or handler objects, but\n",
      "        are often needed by methods such as get_stream().\n",
      "Generated :<s>Save the method args and kwargs.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def __get_securities(self, currency: str, agent: str, symbol: str, namespace: str) -> List[dal.Security]: repo = self.get_security_repository() query = repo.query if currency is not None: query = query.filter(dal.Security.currency == currency) if agent is not None: query = query.filter(dal.Security.updater == agent) if symbol is not None: query = query.filter(dal.Security.symbol == symbol) if namespace is not None: query = query.filter(dal.Security.namespace == namespace) query = query.order_by(dal.Security.namespace, dal.Security.symbol) securities = query.all() return securities\n",
      "Target: Fetches the securities that match the given filters\n",
      "Generated :<s>Returns a list of all securities for the given currency, agent and symbol.</s>\n",
      "---------------------------\n",
      "Code: def isPow2(num) -> bool: if not isinstance(num, int): num = int(num) return num != 0 and ((num & (num - 1)) == 0)\n",
      "Target: Check if number or constant is power of two\n",
      "Generated :<s>Checks if the given number is POW2.</s>\n",
      "---------------------------\n",
      "Code: def reorder_resolved_levels(storage, debug): should_reset = True chars = storage['chars'] for _ch in chars[::-1]: if _ch['orig'] in ('B', 'S'): _ch['level'] = storage['base_level'] should_reset = True elif should_reset and _ch['orig'] in ('BN', 'WS'): _ch['level'] = storage['base_level'] else: should_reset = False max_len = len(chars) line_start = line_end = 0 highest_level = 0 lowest_odd_level = EXPLICIT_LEVEL_LIMIT for idx in range(max_len): _ch = chars[idx] char_level = _ch['level'] if char_level > highest_level: highest_level = char_level if char_level % 2 and char_level < lowest_odd_level: lowest_odd_level = char_level if _ch['orig'] == 'B' or idx == max_len - 1: line_end = idx if _ch['orig'] == 'B': line_end -= 1 reverse_contiguous_sequence(chars, line_start, line_end, highest_level, lowest_odd_level) line_start = idx+1 highest_level = 0 lowest_odd_level = EXPLICIT_LEVEL_LIMIT if debug: debug_storage(storage)\n",
      "Target: L1 and L2 rules\n",
      "Generated :<s>Reset all levels in the storage to the lowest level.</s>\n",
      "---------------------------\n",
      "Code: def get_input_shape(self): input = callBigDlFunc(self.bigdl_type, \"getInputShape\", self.value) return self.__process_shape(input)\n",
      "Target: Return a list of shape tuples if there are multiple inputs.\n",
      "        Return one shape tuple otherwise.\n",
      "Generated :<s>Get the shape of the input. Returns: (int, int): The input shape.</s>\n",
      "---------------------------\n",
      "Code: def map_dict_list(ds, key_func=None, value_func=None, if_func=None): return [map_dict(d, key_func, value_func, if_func) for d in ds]\n",
      "Target: :param List[Dict] ds: list of dict\n",
      "    :param func key_func: func which will run on key.\n",
      "    :param func value_func: func which will run on values.\n",
      "Generated :<s>Convert a list of dictionaries into a list of dicts.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def _connToNode(self, conn): for node in self._connections: if self._connections[node] is conn: return node return None\n",
      "Target: Find the node to which a connection belongs.\n",
      "\n",
      "        :param conn: connection object\n",
      "        :type conn: TcpConnection\n",
      "        :returns corresponding node or None if the node cannot be found\n",
      "        :rtype Node or None\n",
      "Generated :<s>Converts a connection to a node. Args: conn: The connection to the node. Returns: The node</s>\n",
      "---------------------------\n",
      "Code: def create_card(self, card_json): return trolly.card.Card( trello_client=self, card_id=card_json['id'], name=card_json['name'], data=card_json, )\n",
      "Target: Create a Card object from JSON object\n",
      "\n",
      "        Returns:\n",
      "            Card: The card from the given `card_json`.\n",
      "Generated :<s>Creates a new Card from a json dictionary. :param dict card_json: The json dictionary of the card</s>\n",
      "---------------------------\n",
      "Code: def get_bound_method(self, instruction): try: return self._bound_instructions[type(instruction)] except KeyError: raise PulseError('Qobj conversion method for %s is not found.' % instruction)\n",
      "Target: Get conversion method for instruction.\n",
      "Generated :<s>Returns the bound method for the given instruction. :param str instruction: The name of the instruction. :return:</s>\n",
      "---------------------------\n",
      "Code: def FreedmanDiaconisBinSize(feature_values): q75, q25 = numpy.percentile(feature_values, [75, 25]) IQR = q75 - q25 return 2.0 * IQR * len(feature_values) ** (-1.0/3.0)\n",
      "Target: The bin size in FD-binning is given by size = 2 * IQR(x) * n^(-1/3)\n",
      "  More Info: https://en.wikipedia.org/wiki/Freedman%E2%80%93Diaconis_rule\n",
      "\n",
      "  If the BinSize ends up being 0 (in the case that all values are the same),\n",
      "  return a BinSize of 1.\n",
      "Generated :<s>Calculate the Freedman-Diaconis bin size for a list of feature values. Parameters ---------- feature</s>\n",
      "---------------------------\n",
      "Code: def get_nextflow_filepath(log_file): with open(log_file) as fh: while 1: line = fh.readline() if not line: raise eh.LogError(\"Nextflow command path could not be found - Is \" \".nextflow.log empty?\") try: pipeline_path = re.match(\".*\\s(.*.nf).*\", line) .group(1) return pipeline_path except AttributeError: continue\n",
      "Target: Gets the nextflow file path from the nextflow log file. It searches for\n",
      "    the nextflow run command throughout the file.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    log_file : str\n",
      "        Path for the .nextflow.log file\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    str\n",
      "        Path for the nextflow file\n",
      "Generated :<s>Returns the absolute path to the next flow file.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def get_database_hook(self): if self.database_type == 'postgres': self.db_hook = PostgresHook(postgres_conn_id=self.db_conn_id, schema=self.database) else: self.db_hook = MySqlHook(mysql_conn_id=self.db_conn_id, schema=self.database) return self.db_hook\n",
      "Target: Retrieve database hook. This is the actual Postgres or MySQL database hook\n",
      "        that uses proxy or connects directly to the Google Cloud SQL database.\n",
      "Generated :<s>Returns a database hook for this connection.</s>\n",
      "---------------------------\n",
      "Code: def wint(wave): pexdoc.exh.addex( TypeError, \"Cannot convert complex to integer\", wave._dep_vector.dtype.name.startswith(\"complex\"), ) ret = copy.copy(wave) ret._dep_vector = ret._dep_vector.astype(np.int) return ret\n",
      "Target: r\"\"\"\n",
      "    Convert a waveform's dependent variable vector to integer.\n",
      "\n",
      "    :param wave: Waveform\n",
      "    :type  wave: :py:class:`peng.eng.Waveform`\n",
      "\n",
      "    :rtype: :py:class:`peng.eng.Waveform`\n",
      "\n",
      "    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n",
      "    .. Auto-generated exceptions documentation for\n",
      "    .. peng.wave_functions.wint\n",
      "\n",
      "    :raises:\n",
      "     * RuntimeError (Argument \\`wave\\` is not valid)\n",
      "\n",
      "     * TypeError (Cannot convert complex to integer)\n",
      "\n",
      "    .. [[[end]]]\n",
      "Generated :<s>Convert a complex to an integer.</s>\n",
      "---------------------------\n",
      "Code: def preview(self, state): if not self._is_kind(Fold): raise TypeError('Must be an instance of Fold to .preview()') pure = lambda a: Const(Nothing()) func = lambda a: Const(Just(a)) return self.apply(func, pure, state).unwrap()\n",
      "Target: Previews a potentially non-existant focus within\n",
      "        `state`. Returns `Just(focus)` if it exists, Nothing otherwise.\n",
      "\n",
      "        Requires kind Fold.\n",
      "Generated :<s>Returns the preview of the given state. :param state: The state of the preview. :return: The preview</s>\n",
      "---------------------------\n",
      "Code: def fromPy(cls, val, typeObj, vldMask=None): assert isinstance(val, str) or val is None vld = 0 if val is None else 1 if not vld: assert vldMask is None or vldMask == 0 val = \"\" else: if vldMask == 0: val = \"\" vld = 0 return cls(val, typeObj, vld)\n",
      "Target: :param val: python string or None\n",
      "        :param typeObj: instance of String HdlType\n",
      "        :param vldMask: if is None validity is resolved from val\n",
      "            if is 0 value is invalidated\n",
      "            if is 1 value has to be valid\n",
      "Generated :<s>Convert a Python value to a Python value.</s>\n",
      "---------------------------\n",
      "Code: def parse(self, data): data = '\\n'.join(self.strip(data.split('\\n'))) tag_re = re.compile( r'^:\\n?(?P<full_tag>(?P<tag>[0-9]{2}|NS)(?P<sub_tag>[A-Z])?):', re.MULTILINE) matches = list(tag_re.finditer(data)) valid_matches = list(self.sanatize_tag_id_matches(matches)) for i, match in enumerate(valid_matches): tag_id = self.normalize_tag_id(match.group('tag')) tag = self.tags.get(match.group('full_tag')) or self.tags[tag_id] if valid_matches[i + 1:]: tag_data = data[match.end():valid_matches[i + 1].start()].strip() else: tag_data = data[match.end():].strip() tag_dict = tag.parse(self, tag_data) for processor in self.processors.get('pre_%s' % tag.slug, []): tag_dict = processor(self, tag, tag_dict) result = tag(self, tag_dict) for processor in self.processors.get('post_%s' % tag.slug, []): result = processor(self, tag, tag_dict, result) if isinstance(tag, mt940.tags.Statement): if not self.transactions: transaction = Transaction(self) self.transactions.append(transaction) if transaction.data.get('id'): transaction = Transaction(self, result) self.transactions.append(transaction) else: transaction.data.update(result) elif issubclass(tag.scope, Transaction) and self.transactions: for k, v in _compat.iteritems(result): if k in transaction.data and hasattr(v, 'strip'): transaction.data[k] += '\\n%s' % v.strip() else: transaction.data[k] = v elif issubclass(tag.scope, Transactions): self.data.update(result) return self.transactions\n",
      "Target: Parses mt940 data, expects a string with data\n",
      "\n",
      "        Args:\n",
      "            data (str): The MT940 data\n",
      "\n",
      "        Returns: :py:class:`list` of :py:class:`Transaction`\n",
      "Generated :<s>Parse tag data.</s>\n",
      "---------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333d03f103ca4283a35c09bfd875a1ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1653.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved due to improvement in 'val_loss' from 2.752179807413728 to 2.7199637416774984\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def create_build(self, tarball_url, env=None, app_name=None): data = { 'source_blob': { 'url': tarball_url } } if env: data['overrides'] = {'env': env} if app_name: data['app'] = {'name': app_name} return self.api_request('POST', '/app-setups', data=data)\n",
      "Target: Creates an app-setups build. Returns response data as a dict.\n",
      "\n",
      "        :param tarball_url: URL of a tarball containing an ``app.json``.\n",
      "        :param env: Dict containing environment variable overrides.\n",
      "        :param app_name: Name of the Heroku app to create.\n",
      "        :returns: Response data as a ``dict``.\n",
      "Generated :<s>Create a new app. :param tarball_url: :param env: :param app_name: :</s>\n",
      "---------------------------\n",
      "Code: def group_transactions(self): groups = [] if self: last_txn = self.tail_transaction current_group = [last_txn] for current_txn in self.transactions[1:]: if current_txn.address == last_txn.address: current_group.append(current_txn) else: groups.append(current_group) current_group = [current_txn] last_txn = current_txn if current_group: groups.append(current_group) return groups\n",
      "Target: Groups transactions in the bundle by address.\n",
      "Generated :<s>Returns a list of transactions in the current transaction group.</s>\n",
      "---------------------------\n",
      "Code: def setZeroResettableKWH(self, password=\"00000000\"): result = False self.setContext(\"setZeroResettableKWH\") try: if not self.requestA(): self.writeCmdMsg(\"Bad read CRC on setting\") else: if not self.serialCmdPwdAuth(password): self.writeCmdMsg(\"Password failure\") else: req_str = \"0157310230304433282903\" req_str += self.calc_crc16(req_str[2:].decode(\"hex\")) self.m_serial_port.write(req_str.decode(\"hex\")) if self.m_serial_port.getResponse(self.getContext()).encode(\"hex\") == \"06\": self.writeCmdMsg(\"Success: 06 returned.\") result = True self.serialPostEnd() except: ekm_log(traceback.format_exc(sys.exc_info())) self.setContext(\"\") return result\n",
      "Target: Serial call to zero resettable kWh registers.\n",
      "\n",
      "        Args:\n",
      "            password (str): Optional password.\n",
      "\n",
      "        Returns:\n",
      "            bool: True on completion and ACK.\n",
      "Generated :<s>Sets the zero resettable KWH. :param password: The password of the user. :type password</s>\n",
      "---------------------------\n",
      "Code: def fill(self, component=None): c = [component] if component is not None else [] gaps = self.find_gaps() if not gaps: return self for iv in gaps: iv.components = c return deepcopy(self) + gaps\n",
      "Target: Fill gaps with the component provided.\n",
      "\n",
      "        Example\n",
      "            t = s.fill(Component({'lithology': 'cheese'}))\n",
      "Generated :<s>Return a deep copy of self with gaps filled.</s>\n",
      "---------------------------\n",
      "Code: def onPublish(self, topic, payload, qos, dup, retain, msgId): log.debug(\"msg={payload}\", payload=payload)\n",
      "Target: Callback Receiving messages from publisher\n",
      "Generated :<s>Publish a message :param topic: :param payload: :param qos: :param dup: :param</s>\n",
      "---------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5804a82c8104c3c8a4c19b1de481dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=29253.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def _num_tasks_per_fetch_process(self): return max(1, int(math.ceil(1.0 * len(self.tasks) / self._sync_parallelism)))\n",
      "Target: How many Celery tasks should be sent to each worker process.\n",
      "\n",
      "        :return: Number of tasks that should be used per process\n",
      "        :rtype: int\n",
      "Generated :<s>Returns the number of tasks per fetch process.</s>\n",
      "---------------------------\n",
      "Code: def anneal(self): strip = self.copy() gaps = strip.find_gaps(index=True) if not gaps: return for gap in gaps: before = strip[gap] after = strip[gap + 1] if strip.order == 'depth': t = (after.top.z-before.base.z)/2 before.base = before.base.z + t after.top = after.top.z - t else: t = (after.base-before.top)/2 before.top = before.top.z + t after.base = after.base.z - t return strip\n",
      "Target: Fill in empty intervals by growing from top and base.\n",
      "\n",
      "        Note that this operation happens in-place and destroys any information\n",
      "        about the ``Position`` (e.g. metadata associated with the top or base).\n",
      "        See GitHub issue #54.\n",
      "Generated :<s>Returns an anneal copy of the current object.</s>\n",
      "---------------------------\n",
      "Code: def description(self): description = self.label if self.locator: description += \" {}\".format(desc(self.locator)) if self.options[\"text\"] is not None: description += \" with text {}\".format(desc(self.options[\"text\"])) description += self.selector.description(self.filter_options) return description\n",
      "Target: str: A long description of this query.\n",
      "Generated :<s>Return the description of the widget.</s>\n",
      "---------------------------\n",
      "Code: def create_tar(files, output_folder=None): if output_folder is None: output_folder = tempfile.mkdtemp() finished_tar = None additions = [] contents = [] for entity in files: info = tarfile.TarInfo(name=entity['name']) info.mode = entity['mode'] info.mtime = int(datetime.datetime.now().strftime('%s')) info.uid = entity[\"uid\"] info.gid = entity[\"gid\"] info.uname = entity[\"uname\"] info.gname = entity[\"gname\"] filey = io.StringIO() content = None try: info.size = filey.write(entity['content']) content = io.BytesIO(entity['content'].encode('utf8')) except Exception: info.size = int(filey.write(entity['content'].decode('utf-8'))) content = io.BytesIO(entity['content'].encode('utf8')) pass if content is not None: addition = {'content': content, 'info': info} additions.append(addition) contents.append(content) if len(additions) > 0: hashy = get_content_hash(contents) finished_tar = \"%s/sha256:%s.tar.gz\" % (output_folder, hashy) if os.path.exists(finished_tar): msg = \"metadata file %s already exists \" % finished_tar msg += \"will over-write.\" bot.debug(msg) tar = tarfile.open(finished_tar, \"w:gz\") for a in additions: tar.addfile(a[\"info\"], a[\"content\"]) tar.close() else: msg = \"No contents, environment or labels\" msg += \" for tarfile, will not generate.\" bot.debug(msg) return finished_tar\n",
      "Target: create_memory_tar will take a list of files (each a dictionary\n",
      "        with name, permission, and content) and write the tarfile\n",
      "        (a sha256 sum name is used) to the output_folder.\n",
      "        If there is no output folde specified, the\n",
      "        tar is written to a temporary folder.\n",
      "Generated :<s>Create a tar archive from a list of files :param files: :param output_folder:</s>\n",
      "---------------------------\n",
      "Code: def split_type(self, typename): name = self._canonicalize_type(typename) if '(' not in name: return name, False, [] base, sub = name.split('(') if len(sub) == 0 or sub[-1] != ')': raise ArgumentError(\"syntax error in complex type, no matching ) found\", passed_type=typename, basetype=base, subtype_string=sub) sub = sub[:-1] subs = sub.split(',') return base, True, subs\n",
      "Target: Given a potentially complex type, split it into its base type and specializers\n",
      "Generated :<s>Split a type into two parts. The first part is the name of the type, and the second part is the</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def comments(self): record_numbers = range(2, self.fward) if not record_numbers: return '' data = b''.join(self.read_record(n)[0:1000] for n in record_numbers) try: return data[:data.find(b'\\4')].decode('ascii').replace('\\0', '\\n') except IndexError: raise ValueError('DAF file comment area is missing its EOT byte') except UnicodeDecodeError: raise ValueError('DAF file comment area is not ASCII text')\n",
      "Target: Return the text inside the comment area of the file.\n",
      "Generated :<s>Return the comments for this DFA file.</s>\n",
      "---------------------------\n",
      "Code: def word_score(word, input_letters, questions=0): score = 0 bingo = 0 filled_by_blanks = [] rack = list(input_letters) for letter in word: if letter in rack: bingo += 1 score += letter_score(letter) rack.remove(letter) else: filled_by_blanks.append(letter_score(letter)) for blank_score in sorted(filled_by_blanks, reverse=True): if questions > 0: score += blank_score questions -= 1 if bingo > 6: score += 50 return score\n",
      "Target: Checks the Scrabble score of a single word.\n",
      "\n",
      "    Args:\n",
      "        word: a string to check the Scrabble score of\n",
      "        input_letters: the letters in our rack\n",
      "        questions: integer of the tiles already on the board to build on\n",
      "\n",
      "    Returns:\n",
      "        an integer Scrabble score amount for the word\n",
      "Generated :<s>Calculate the score for a word. :param word: :param input_letters: :param questions:</s>\n",
      "---------------------------\n",
      "Code: def create_signed_bundle(self, sign_alg='RS256', iss_list=None): data = self.dict(iss_list) _jwt = JWT(self.sign_keys, iss=self.iss, sign_alg=sign_alg) return _jwt.pack({'bundle':data})\n",
      "Target: Create a signed JWT containing a dictionary with Issuer IDs as keys\n",
      "        and JWKSs as values. If iss_list is empty then all available issuers are\n",
      "        included.\n",
      "        \n",
      "        :param sign_alg: Which algorithm to use when signing the JWT\n",
      "        :param iss_list: A list of issuer IDs who's keys should be included in \n",
      "            the signed bundle.\n",
      "        :return: A signed JWT\n",
      "Generated :<s>Create a signed JWT bundle. :param sign_alg: :param iss_list: :return:</s>\n",
      "---------------------------\n",
      "Code: def start(self): self.logger.debug('PostgresGreenlet start') self._stop_event.clear() self.print('=> Discovering DDP endpoints...') if self.verbosity > 1: for api_path in sorted(self.api.api_path_map()): print(' %s' % api_path) self.pgworker.start() self.print('=> Started PostgresGreenlet.') for server in self.servers: thread = gevent.spawn(server.serve_forever) gevent.sleep() self.threads.append(thread) if thread.dead: self.stop() thread.get() if isinstance(server, geventwebsocket.WebSocketServer): self.print( '=> App running at: %s://%s:%d/' % ( 'https' if server.ssl_enabled else 'http', server.server_host, server.server_port, ), ) elif isinstance(server, gevent.backdoor.BackdoorServer): self.print( '=> Debug service running at: telnet://%s:%d/' % ( server.server_host, server.server_port, ), ) self.print('=> Started your app (%s).' % self.wsgi_name)\n",
      "Target: Run PostgresGreenlet and web/debug servers.\n",
      "Generated :<s>Start the PostgresGreenlet.</s>\n",
      "---------------------------\n",
      "Code: def copy_config_file(self, config_file, path=None, overwrite=False): dst = os.path.join(self.location, config_file) if os.path.isfile(dst) and not overwrite: return False if path is None: path = os.path.join(get_ipython_package_dir(), u'config', u'profile', u'default') src = os.path.join(path, config_file) shutil.copy(src, dst) return True\n",
      "Target: Copy a default config file into the active profile directory.\n",
      "\n",
      "        Default configuration files are kept in :mod:`IPython.config.default`.\n",
      "        This function moves these from that location to the working profile\n",
      "        directory.\n",
      "Generated :<s>Copies the config file to the given path. :param config_file: The config file to copy. :</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def _bash_comp_command(self, cmd, add_help=True): out = ['-h', '--help'] if add_help else [] cmd_dict = self._opt_cmds[cmd] if cmd else self._opt_bare for opt, sct in cmd_dict: out.extend(_names(self._conf[sct], opt)) return out\n",
      "Target: Build a list of all options for a given command.\n",
      "\n",
      "        Args:\n",
      "            cmd (str): command name, set to None or '' for bare command.\n",
      "            add_help (bool): add an help option.\n",
      "\n",
      "        Returns:\n",
      "            list of str: list of CLI options strings.\n",
      "Generated :<s>Return a bash-comp command list.</s>\n",
      "---------------------------\n",
      "Code: def find_globals(code): cur_byte = 0 byte_code = code.co_code names = set() while cur_byte < len(byte_code): op = ord(byte_code[cur_byte]) if op >= dis.HAVE_ARGUMENT: if op == _LOAD_GLOBAL: oparg = ord(byte_code[cur_byte + 1]) + (ord(byte_code[cur_byte + 2]) << 8) name = code.co_names[oparg] names.add(name) cur_byte += 2 cur_byte += 1 return names\n",
      "Target: walks the byte code to find the variables which are actually globals\n",
      "Generated :<s>Find all the globals in a bytecode. Args: code: The bytecode to search for globals</s>\n",
      "---------------------------\n",
      "Code: def compiler_format_extension(self): for extension, mimetype in self.environment.mimetypes.items(): if mimetype == self.compiler_mimetype: return extension return None\n",
      "Target: Implicit format extension on the asset by its compilers.\n",
      "Generated :<s>Return the extension of the compiler, or None if not found.</s>\n",
      "---------------------------\n",
      "Code: def sign(self, req, receiver='', iss='', lifetime=0, sign_alg='', aud=None): if not sign_alg: for key_type, s_alg in [('RSA', 'RS256'), ('EC', 'ES256')]: if self.keyjar.get_signing_key(key_type=key_type): sign_alg = s_alg break if not sign_alg: raise NoSigningKeys('Could not find any signing keys') return self.pack(req=req, receiver=receiver, iss=iss, lifetime=lifetime, sign=True, encrypt=False, sign_alg=sign_alg)\n",
      "Target: Creates a signed JWT\n",
      "\n",
      "        :param req: Original metadata statement as a\n",
      "            :py:class:`MetadataStatement` instance\n",
      "        :param receiver: The intended audience for the JWS\n",
      "        :param iss: Issuer or the JWT\n",
      "        :param lifetime: Lifetime of the signature\n",
      "        :param sign_alg: Which signature algorithm to use\n",
      "        :param aud: The audience, a list of receivers.\n",
      "        :return: A signed JWT\n",
      "Generated :<s>Sign a request. :param req: :param receiver: :param iss: :param lifetime: :param sign</s>\n",
      "---------------------------\n",
      "Code: def crz(self, theta, ctl, tgt): return self.append(CrzGate(theta), [ctl, tgt], [])\n",
      "Target: Apply crz from ctl to tgt with angle theta.\n",
      "Generated :<s>Shortcut for :meth:`CrzGate`.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def _builtin_help(self, args): if len(args) == 0: return self.list_dir(self.contexts[-1]) if len(args) == 1: func = self.find_function(self.contexts[-1], args[0]) return annotate.get_help(func) help_text = \"Too many arguments: \" + str(args) + \"\\n\" help_text += \"Usage: help [function]\" return help_text\n",
      "Target: Return help information for a context or function.\n",
      "Generated :<s>Help text for a built-in function.</s>\n",
      "---------------------------\n",
      "Code: def random_ports(port, n): for i in range(min(5, n)): yield port + i for i in range(n-5): yield port + random.randint(-2*n, 2*n)\n",
      "Target: Generate a list of n random ports near the given port.\n",
      "\n",
      "    The first 5 ports will be sequential, and the remaining n-5 will be\n",
      "    randomly selected in the range [port-2*n, port+2*n].\n",
      "Generated :<s>Generates a list of random ports. :param port: port number :param n: number of ports :return</s>\n",
      "---------------------------\n",
      "Code: def shutdown(self): result = _lib.SSL_shutdown(self._ssl) if result < 0: self._raise_ssl_error(self._ssl, result) elif result > 0: return True else: return False\n",
      "Target: Send the shutdown message to the Connection.\n",
      "\n",
      "        :return: True if the shutdown completed successfully (i.e. both sides\n",
      "                 have sent closure alerts), False otherwise (in which case you\n",
      "                 call :meth:`recv` or :meth:`send` when the connection becomes\n",
      "                 readable/writeable).\n",
      "Generated :<s>Shutdown the socket. Returns: bool: True if successful, False otherwise.</s>\n",
      "---------------------------\n",
      "Code: def create(self,obj,cache=False): obj._modeldata = {} data = obj._modeldata data[\"_model\"]=self data[\"_modelcache\"] = {} moddata = data[\"_modelcache\"] moddata[\"group\"] = JSONModelGroup(self,data,obj) modgroup = moddata[\"group\"] if not hasattr(obj,\"batch3d\"): obj.batch3d = pyglet.graphics.Batch() data[\"_manual_render\"]=True moddata[\"vlists\"] = {} for name,region in self.modeldata[\"regions\"].items(): v = region.getVertices(data) vlistlen = int(len(v)/region.dims) if region.enable_tex: vlist = obj.batch3d.add(vlistlen,region.getGeometryType(data),JSONRegionGroup(self,data,region,modgroup), \"v3f/static\", \"t3f/static\", ) else: vlist = obj.batch3d.add(vlistlen,region.getGeometryType(data),modgroup, \"v3f/static\", ) moddata[\"vlists\"][name]=vlist self.setAnimation(obj,self.modeldata[\"default_animation\"].name,transition=\"jump\") self.data = data if not cache: self.redraw(obj)\n",
      "Target: Initializes per-actor data on the given object for this model.\n",
      "        \n",
      "        If ``cache`` is set to True, the entity will not be redrawn after initialization.\n",
      "        \n",
      "        Note that this method may set several attributes on the given object, most of them starting with underscores.\n",
      "        \n",
      "        During initialization of vertex regions, several vertex lists will be created.\n",
      "        If the given object has an attribute called ``batch3d`` it will be used, else it will be created.\n",
      "        \n",
      "        If the batch already existed, the :py:meth:`draw()` method will do nothing, else it will draw the batch.\n",
      "        \n",
      "        Memory leaks may occur if this is called more than once on the same object without calling :py:meth:`cleanup()` first.\n",
      "Generated :<s>:param obj: :param cache: :return:</s>\n",
      "---------------------------\n",
      "Code: def special_handling(self, text): self._attempting(text) return concatenation([ \"?\", self.identifier, \"?\", ], ignore_whitespace=True)(text).retyped(TokenType.special_handling)\n",
      "Target: special_handling = \"?\" , identifier , \"?\" ;\n",
      "Generated :<s>special_handling :param text: :return:</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def format_value(self, value, type, format=None, **kwargs): typed_val = self.convert_to_type(value, type, **kwargs) typeobj = self.get_type(type) if format is None: if hasattr(typeobj, 'default_formatter'): format_func = getattr(typeobj, 'default_formatter') return format_func(typed_val, **kwargs) return str(typed_val) formatter = \"format_%s\" % str(format) if not hasattr(typeobj, formatter): raise ArgumentError(\"Unknown format for type\", type=type, format=format, formatter_function=formatter) format_func = getattr(typeobj, formatter) return format_func(typed_val, **kwargs)\n",
      "Target: Convert value to type and format it as a string\n",
      "\n",
      "        type must be a known type in the type system and format,\n",
      "        if given, must specify a valid formatting option for the\n",
      "        specified type.\n",
      "Generated :<s>Converts a typed value to a given format. Args: value: The value to convert. type: The</s>\n",
      "---------------------------\n",
      "Code: def file(self, path): with open(path, 'r') as f: self.body(str(f.read()))\n",
      "Target: Reads the body to match from a disk file.\n",
      "\n",
      "        Arguments:\n",
      "            path (str): relative or absolute path to file to read from.\n",
      "\n",
      "        Returns:\n",
      "            self: current Mock instance.\n",
      "Generated :<s>Reads a file. :param path: :return:</s>\n",
      "---------------------------\n",
      "Code: def prune(self, leaves, inverse=False): self.visit( lambda n: n.ancestor.descendants.remove(n), lambda n: ((not inverse and n in leaves) or (inverse and n.is_leaf and n not in leaves)) and n.ancestor, mode=\"postorder\")\n",
      "Target: Remove all those nodes in the specified list, or if inverse=True,\n",
      "        remove all those nodes not in the specified list.  The specified nodes\n",
      "        must be leaves and distinct from the root node.\n",
      "\n",
      "        :param nodes: A list of Node objects\n",
      "        :param inverse: Specifies whether to remove nodes in the list or not\\\n",
      "                in the list.\n",
      "Generated :<s>Prune a set of leaves. :param leaves: A list of leaves to prune. :type leaves:</s>\n",
      "---------------------------\n",
      "Code: def fetch_metric(self, metric, start, end, tags={}, aggregator=\"sum\", downsample=None, ms_resolution=True): query = \"{aggregator}:{downsample}{metric}{{{tags}}}\".format( aggregator=aggregator, downsample=downsample + \"-avg:\" if downsample else \"\", metric=metric, tags=','.join(\"%s=%s\" % (k, v) for k, v in tags.items()) ) params = { 'ms': ms_resolution, 'start': '{0:.3f}'.format(start.timestamp()), 'end': '{0:.3f}'.format(end.timestamp()), 'm': query } response = self.__request(\"/query\", params) if response.status_code == 200: try: return response.json()[0]['dps'] except IndexError: return {} raise QueryError(response.json())\n",
      "Target: Fetch time series data from OpenTSDB\n",
      "\n",
      "        Parameters:\n",
      "            metric:\n",
      "                A string representing a valid OpenTSDB metric.\n",
      "\n",
      "            tags:\n",
      "                A dict mapping tag names to tag values. Tag names and values are\n",
      "                always strings.\n",
      "\n",
      "                { 'user_id': '44' }\n",
      "\n",
      "            start:\n",
      "                A datetime.datetime-like object representing the start of the\n",
      "                range to query over.\n",
      "\n",
      "            end:\n",
      "                A datetime.datetime-like object representing the end of the\n",
      "                range to query over.\n",
      "\n",
      "            aggregator:\n",
      "                The function for merging multiple time series together. For\n",
      "                example, if the \"user_id\" tag is not specified, this aggregator\n",
      "                function is used to combine all heart rate time series into one\n",
      "                time series. (Yes, this isn't very useful.)\n",
      "\n",
      "                For queries that return only one time series, this parameter is\n",
      "                not relevant.\n",
      "\n",
      "                Valid values: \"sum\", \"min\", \"max\", \"avg\", \"dev\"\n",
      "\n",
      "                See: http://opentsdb.net/docs/build/html/user_guide/query/aggregators.html\n",
      "\n",
      "            downsampling:\n",
      "                A relative time interval to \"downsample\". This isn't true\n",
      "                downsampling; rather, if you specify a downsampling of \"5m\"\n",
      "                (five minutes), OpenTSDB will split data into five minute\n",
      "                intervals, and return one data point in the middle of each\n",
      "                interval whose value is the average of all data points within\n",
      "                that interval.\n",
      "\n",
      "                Valid relative time values are strings of the following format:\n",
      "\n",
      "                    \"<amount><time_unit>\"\n",
      "\n",
      "                Valid time units: \"ms\", \"s\", \"m\", \"h\", \"d\", \"w\", \"n\", \"y\"\n",
      "\n",
      "                Date and time format: http://opentsdb.net/docs/build/html/user_guide/query/dates.html\n",
      "\n",
      "            ms_resolution:\n",
      "                Whether or not to output data point timestamps in milliseconds\n",
      "                or seconds. If this flag is false and there are multiple\n",
      "                data points within a second, those data points will be down\n",
      "                sampled using the query's aggregation function.\n",
      "\n",
      "        Returns:\n",
      "            A dict mapping timestamps to data points\n",
      "Generated :<s>Fetch a metric. :param metric: :param start: :param end: :param tags: :param</s>\n",
      "---------------------------\n",
      "Code: def reset(self): self.layers = {} self.stack = [] self.set_mask() self.n_vox_in_vol = len(np.where(self.current_mask)[0])\n",
      "Target: Reset/remove all layers, keeping only the initial volume.\n",
      "Generated :<s>Reset the voxels.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def post_updates(self, **values): return self.api.post(self.subpath('/updates'), data=values)\n",
      "Target: Method for `Post Device Updates (Multiple Values to Multiple Streams) <https://m2x.att.com/developer/documentation/v2/device#Post-Device-Updates--Multiple-Values-to-Multiple-Streams->`_ endpoint.\n",
      "\n",
      "        :param values: The values being posted, formatted according to the API docs\n",
      "        :type values: dict\n",
      "\n",
      "        :return: The API response, see M2X API docs for details\n",
      "        :rtype: dict\n",
      "\n",
      "        :raises: :class:`~requests.exceptions.HTTPError` if an error occurs when sending the HTTP request\n",
      "Generated :<s>POST /updates :param values: :return:</s>\n",
      "---------------------------\n",
      "Code: def _get_categories(self): category_list = [] for child in self.vcard.getChildren(): if child.name == \"CATEGORIES\": value = child.value category_list.append( value if isinstance(value, list) else [value]) if len(category_list) == 1: return category_list[0] return sorted(category_list)\n",
      "Target: :rtype: list(str) or list(list(str))\n",
      "Generated :<s>Gets the category list from vCard.</s>\n",
      "---------------------------\n",
      "Code: def values_search(self, **params): return self.api.post(self.subpath('/values/search'), data=params)\n",
      "Target: Method for `Search Values from all Data Streams of a Device <https://m2x.att.com/developer/documentation/v2/device#Search-Values-from-all-Data-Streams-of-a-Device>`_ endpoint.\n",
      "\n",
      "        :param params: Query parameters passed as keyword arguments. View M2X API Docs for listing of available parameters.\n",
      "\n",
      "        :return: The API response, see M2X API docs for details\n",
      "        :rtype: dict\n",
      "\n",
      "        :raises: :class:`~requests.exceptions.HTTPError` if an error occurs when sending the HTTP request\n",
      "Generated :<s>Endpoint: /values/search. Produces: 200 application/json :param \\*\\*params:</s>\n",
      "---------------------------\n",
      "Code: def _cut_off_drivers_of(self, sig: RtlSignalBase): if len(self._outputs) == 1 and sig in self._outputs: self.parentStm = None return self child_keep_mask = [] newIfTrue = [] all_cut_off = True all_cut_off &= self._cut_off_drivers_of_list( sig, self.ifTrue, child_keep_mask, newIfTrue) self.ifTrue = list(compress(self.ifTrue, child_keep_mask)) newElifs = [] anyElifHit = False for cond, stms in self.elIfs: newCase = [] child_keep_mask.clear() all_cut_off &= self._cut_off_drivers_of_list( sig, stms, child_keep_mask, newCase) _stms = list(compress(stms, child_keep_mask)) stms.clear() stms.extend(_stms) if newCase: anyElifHit = True newElifs.append((cond, newCase)) newIfFalse = None if self.ifFalse: newIfFalse = [] child_keep_mask.clear() all_cut_off &= self._cut_off_drivers_of_list( sig, self.ifFalse, child_keep_mask, newIfFalse) self.ifFalse = list(compress(self.ifFalse, child_keep_mask)) assert not all_cut_off, \"everything was cut of but this should be already known at start\" if newIfTrue or newIfFalse or anyElifHit or newIfFalse: cond_sig = self.cond n = self.__class__(cond_sig, newIfTrue) for c, stms in newElifs: assert len(c) == 1 c_sig = c[0] n.Elif(c_sig, stms) if newIfFalse is not None: n.Else(newIfFalse) if self.parentStm is None: ctx = n._get_rtl_context() ctx.statements.add(n) self._inputs.clear() self._inputs.append(cond_sig) for c, _ in self.elIfs: self._inputs.extend(c) self._inputs.append(cond_sig) self._outputs.clear() out_add = self._outputs.append in_add = self._inputs.append for stm in self._iter_stms(): for inp in stm._inputs: in_add(inp) for outp in stm._outputs: out_add(outp) if self._sensitivity is not None or self._enclosed_for is not None: raise NotImplementedError( \"Sensitivity and enclosure has to be cleaned first\") return n\n",
      "Target: Doc on parent class :meth:`HdlStatement._cut_off_drivers_of`\n",
      "Generated :<s>Cuts off the driver of a signal :param sig: signal to be cut off :return: None</s>\n",
      "---------------------------\n",
      "Code: def authorize(self, callback=None, state=None, **kwargs): params = dict(self.request_token_params) or {} params.update(**kwargs) if self.request_token_url: token = self.generate_request_token(callback)[0] url = '%s?oauth_token=%s' % ( self.expand_url(self.authorize_url), url_quote(token) ) if params: url += '&' + url_encode(params) else: assert callback is not None, 'Callback is required for OAuth2' client = self.make_client() if 'scope' in params: scope = params.pop('scope') else: scope = None if isinstance(scope, str): scope = _encode(scope, self.encoding) if 'state' in params: if not state: state = params.pop('state') else: params.pop('state') if callable(state): state = state() session['%s_oauthredir' % self.name] = callback url = client.prepare_request_uri( self.expand_url(self.authorize_url), redirect_uri=callback, scope=scope, state=state, **params ) return redirect(url)\n",
      "Target: Returns a redirect response to the remote authorization URL with\n",
      "        the signed callback given.\n",
      "\n",
      "        :param callback: a redirect url for the callback\n",
      "        :param state: an optional value to embed in the OAuth request.\n",
      "                      Use this if you want to pass around application\n",
      "                      state (e.g. CSRF tokens).\n",
      "        :param kwargs: add optional key/value pairs to the query string\n",
      "Generated :<s>Perform an OAuth2 authorization. Args: callback (:py:class:`oauth2.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def emit(self, record): try: import smtplib try: from email.utils import formatdate except ImportError: formatdate = self.date_time port = self.mailport if not port: port = smtplib.SMTP_PORT smtp = smtplib.SMTP(self.mailhost, port) msg = self.format(record) msg = \"From: %s\\r\\nTo: %s\\r\\nSubject: %s\\r\\nDate: %s\\r\\n\\r\\n%s\" % ( self.fromaddr, ','.join(self.toaddrs), self.getSubject(record), formatdate(), msg ) if self.username: smtp.ehlo() smtp.starttls() smtp.ehlo() smtp.login(self.username, self.password) smtp.sendmail(self.fromaddr, self.toaddrs, msg) smtp.quit() except (KeyboardInterrupt, SystemExit): raise except: self.handleError(record)\n",
      "Target: Emit a record.\n",
      "        Format the record and send it to the specified addressees.\n",
      "Generated :<s>Send an email to the server.</s>\n",
      "---------------------------\n",
      "Code: def eval(self, orig): _le = {} _err = [] for k, v in self.sup_items(): if k in DoNotCompare: continue if k in orig: if is_lesser(orig[k], v): _le[k] = orig[k] else: _err.append({'claim': k, 'policy': orig[k], 'err': v, 'signer': self.iss}) else: _le[k] = v for k, v in orig.items(): if k in DoNotCompare: continue if k not in _le: _le[k] = v self.le = _le self.err = _err\n",
      "Target: Apply the less or equal algorithm on the ordered list of metadata\n",
      "        statements\n",
      "        \n",
      "        :param orig: Start values\n",
      "        :return:\n",
      "Generated :<s>Evaluate the given dict. :param dict orig: The dict to evaluate.</s>\n",
      "---------------------------\n",
      "Code: def fastsmooth(a, win=11): if win % 2 == 0: win += 1 kernel = np.ones(win) / win npad = int((win - 1) / 2) spad = np.full(npad + 1, np.mean(a[:(npad + 1)])) epad = np.full(npad - 1, np.mean(a[-(npad - 1):])) return np.concatenate([spad, np.convolve(a, kernel, 'valid'), epad])\n",
      "Target: Returns rolling - window smooth of a.\n",
      "\n",
      "    Function to efficiently calculate the rolling mean of a numpy\n",
      "    array using 'stride_tricks' to split up a 1D array into an ndarray of\n",
      "    sub - sections of the original array, of dimensions [len(a) - win, win].\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        The 1D array to calculate the rolling gradient of.\n",
      "    win : int\n",
      "        The width of the rolling window.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    array_like\n",
      "        Gradient of a, assuming as constant integer x - scale.\n",
      "Generated :<s>Calculate a fast-smooth version of a numpy array. Parameters ---------- a : numpy array</s>\n",
      "---------------------------\n",
      "Code: def extract_hist_ranges(ranges_str): for range_str in ranges_str.split(): rmatch = range_re.match(range_str) if not rmatch: continue start = int(rmatch.group(\"start\")) end = rmatch.group(\"end\") end = int(end) if end else start+1 if rmatch.group(\"sep\") == \"-\": end += 1 startsess = rmatch.group(\"startsess\") or \"0\" endsess = rmatch.group(\"endsess\") or startsess startsess = int(startsess.replace(\"~\",\"-\")) endsess = int(endsess.replace(\"~\",\"-\")) assert endsess >= startsess if endsess == startsess: yield (startsess, start, end) continue yield (startsess, start, None) for sess in range(startsess+1, endsess): yield (sess, 1, None) yield (endsess, 1, end)\n",
      "Target: Turn a string of history ranges into 3-tuples of (session, start, stop).\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    list(extract_input_ranges(\"~8/5-~7/4 2\"))\n",
      "    [(-8, 5, None), (-7, 1, 4), (0, 2, 3)]\n",
      "Generated :<s>Extracts histranges from a list of ranges. Args: ranges_str (str): A list</s>\n",
      "---------------------------\n",
      "Code: def unset_state(self): glDisable(self.region.material.target) self.region.bone.unsetRotate(self.data)\n",
      "Target: Resets the state required for this actor to the default state.\n",
      "        \n",
      "        Currently only disables the target of the texture of the material, it may still be bound.\n",
      "Generated :<s>Unset the state of the controller.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def setLang(self,lang): self.lang = lang self.peng.cfg[\"i18n.lang\"] = lang if lang not in self.cache: self.cache[lang]={} self.doAction(\"setlang\") self.peng.sendEvent(\"peng3d:i18n.set_lang\",{\"lang\":self.lang,\"i18n\":self})\n",
      "Target: Sets the default language for all domains.\n",
      "        \n",
      "        For recommendations regarding the format of the language code, see\n",
      "        :py:class:`TranslationManager`\\ .\n",
      "        \n",
      "        Note that the ``lang`` parameter of both :py:meth:`translate()` and\n",
      "        :py:meth:`translate_lazy()` will override this setting.\n",
      "        \n",
      "        Also note that the code won't be checked for existence or plausibility.\n",
      "        This may cause the fallback strings to be displayed instead if the language\n",
      "        does not exist.\n",
      "        \n",
      "        Calling this method will cause the ``setlang`` action and the\n",
      "        :peng3d:event`peng3d:i18n.set_lang` event to be triggered. Note that both\n",
      "        action and event will be triggered even if the language did not actually change.\n",
      "        \n",
      "        This method also automatically updates the :confval:`i18n.lang` config value.\n",
      "Generated :<s>Set the language for the current user.</s>\n",
      "---------------------------\n",
      "Code: def add_arguments(self): ApiCli.add_arguments(self) self.parser.add_argument('-f', '--format', dest='format', action='store', required=False, choices=['csv', 'json', 'raw', 'xml'], help='Output format. Default is raw') self.parser.add_argument('-n', '--name', dest='metric_name', action='store', required=True, metavar=\"metric_name\", help='Metric identifier') self.parser.add_argument('-g', '--aggregate', dest='aggregate', action='store', required=False, choices=['sum', 'avg', 'max', 'min'], help='Metric default aggregate') self.parser.add_argument('-r', '--sample', dest='sample', action='store', type=int, metavar=\"sample\", help='Down sample rate sample in seconds') self.parser.add_argument('-s', '--source', dest='source', action='store', metavar=\"source\", required=True, help='Source of measurement') self.parser.add_argument('-b', '--start', dest='start', action='store', required=True, metavar=\"start\", help='Start of time range as ISO 8601 string or epoch seconds') self.parser.add_argument('-d', '--end', dest='end', action='store', metavar=\"end\", required=False, help='End of time range as ISO 8601 string or epoch seconds') self.parser.add_argument('-o', '--date-format', dest='date_format', action='store', metavar=\"format\", required=False, help='For CSV, JSON, and XML output formats dates (see Python date.strftime). ' + 'Default format is %%s')\n",
      "Target: Add specific command line arguments for this command\n",
      "Generated :<s>Add command line arguments to the parser.</s>\n",
      "---------------------------\n",
      "Code: def base64_decode(string): string = want_bytes(string, encoding='ascii', errors='ignore') return base64.urlsafe_b64decode(string + b'=' * (-len(string) % 4))\n",
      "Target: base64 decodes a single bytestring (and is tolerant to getting\n",
      "    called with a unicode string).\n",
      "    The result is also a bytestring.\n",
      "Generated :<s>Decodes a base64-encoded string. :param string: The string to base64 decode. :type</s>\n",
      "---------------------------\n",
      "Code: def groupByWordIndex(self, transaction: 'TransTmpl', offset: int): actualW = None partsInWord = [] wordWidth = self.wordWidth for item in self.splitOnWords(transaction, offset): _actualW = item.startOfPart // wordWidth if actualW is None: actualW = _actualW partsInWord.append(item) elif _actualW > actualW: yield (actualW, partsInWord) actualW = _actualW partsInWord = [item, ] else: partsInWord.append(item) if partsInWord: yield (actualW, partsInWord)\n",
      "Target: Group transaction parts splited on words to words\n",
      "\n",
      "        :param transaction: TransTmpl instance which parts\n",
      "            should be grupped into words\n",
      "        :return: generator of tuples (wordIndex, list of transaction parts\n",
      "            in this word)\n",
      "Generated :<s>Groups by word index. :param transaction: The transaction. :type transaction: Transaction :param offset: The</s>\n",
      "---------------------------\n",
      "Code: def default_view_method(pid, record, template=None, **kwargs): record_viewed.send( current_app._get_current_object(), pid=pid, record=record, ) return render_template( template, pid=pid, record=record, )\n",
      "Target: r\"\"\"Display default view.\n",
      "\n",
      "    Sends record_viewed signal and renders template.\n",
      "\n",
      "    :param pid: PID object.\n",
      "    :param record: Record object.\n",
      "    :param template: Template to render.\n",
      "    :param \\*\\*kwargs: Additional view arguments based on URL rule.\n",
      "    :returns: The rendered template.\n",
      "Generated :<s>Render the view method for the given record.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def replace_config(config, name): global static_stages if static_stages is None: static_stages = PipelineStages() stages = static_stages if 'external_stages_path' in config: path = config['external_stages_path'] if not os.path.isabs(path) and config.get('root_path'): path = os.path.join(config['root_path'], path) try: stages.load_external_stages(config['external_stages_path']) except IOError: return streamcorpus_pipeline if 'external_stages_modules' in config: for mod in config['external_stages_modules']: try: stages.load_module_stages(mod) except ImportError: return streamcorpus_pipeline else: stages = static_stages new_sub_modules = set(stage for stage in stages.itervalues() if hasattr(stage, 'config_name')) return NewSubModules(streamcorpus_pipeline, new_sub_modules)\n",
      "Target: Replace the top-level pipeline configurable object.\n",
      "\n",
      "    This investigates a number of sources, including\n",
      "    `external_stages_path` and `external_stages_modules` configuration\n",
      "    and `streamcorpus_pipeline.stages` entry points, and uses these to\n",
      "    find the actual :data:`sub_modules` for\n",
      "    :mod:`streamcorpus_pipeline`.\n",
      "Generated :<s>Replace the configuration variable with the given name. :param config: configuration dictionary :type config: dict :param</s>\n",
      "---------------------------\n",
      "Code: def register(view=None, *, admin_site=None, admin_class=ModelAdminView): if not admin_site: admin_site = site def wrapped(inner_view): module = inner_view.__module__ app_label = re.search(r\"\\.?(\\w+)\\.admin\", module).group(1) app_config = apps.get_app_config(app_label) label = getattr(inner_view, \"label\", None) if not label: label = re.sub(\"(Admin)|(View)\", \"\", inner_view.__name__).lower() inner_view.label = label model_name = label.capitalize() verbose_name = getattr(inner_view, \"verbose_name\", model_name) inner_view.verbose_name = verbose_name access_perm_codename = \"can_access_\" + model_name.lower() access_perm_name = _(\"Can access {verbose_name}\").format( verbose_name=verbose_name ) permissions = tuple( [(access_perm_codename, access_perm_name)] + list(getattr(inner_view, \"permissions\", [])) ) model = type( model_name, (Model,), { \"__module__\": module + \".__models__\", \"View\": inner_view, \"app_config\": app_config, \"Meta\": type( \"Meta\", (object,), dict( managed=False, abstract=True, app_label=app_config.label, verbose_name=verbose_name, verbose_name_plural=verbose_name, permissions=permissions, ), ), }, ) admin_site._registry[model] = admin_class(model, admin_site) return inner_view if view is None: return wrapped return wrapped(view)\n",
      "Target: Register a generic class based view wrapped with ModelAdmin and fake model\n",
      "\n",
      "    :param view: The AdminView to register.\n",
      "    :param admin_site: The AdminSite to register the view on.\n",
      "        Defaults to bananas.admin.ExtendedAdminSite.\n",
      "    :param admin_class: The ModelAdmin class to use for eg. permissions.\n",
      "        Defaults to bananas.admin.ModelAdminView.\n",
      "\n",
      "    Example:\n",
      "\n",
      "    @register  # Or with args @register(admin_class=MyModelAdminSubclass)\n",
      "    class MyAdminView(bananas.admin.AdminView):\n",
      "        def get(self, request):\n",
      "            return self.render('template.html', {})\n",
      "\n",
      "    # Also possible:\n",
      "    register(MyAdminView, admin_class=MyModelAdminSublass)\n",
      "Generated :<s>Register a view to the admin. :param view: The view to register. :param admin_site: The</s>\n",
      "---------------------------\n",
      "Code: def _create_url_identifier(user, password): if user is not None: user = parse.quote(user.encode('utf-8'), safe=USERINFO_SAFE_CHARS) if password: password = parse.quote(password.encode('utf-8'), safe=USERINFO_SAFE_CHARS) return '{0}:{1}'.format(user, password) return user return None\n",
      "Target: Generate the user+password portion of a URL.\n",
      "\n",
      "    :param str user: the user name or :data:`None`\n",
      "    :param str password: the password or :data:`None`\n",
      "Generated :<s>Create a URL identifier for the given user and password.</s>\n",
      "---------------------------\n",
      "Code: def get_default_redirect_uri(self, client_id, request, *args, **kwargs): request.client = request.client or self._clientgetter(client_id) redirect_uri = request.client.default_redirect_uri log.debug('Found default redirect uri %r', redirect_uri) return redirect_uri\n",
      "Target: Default redirect_uri for the given client.\n",
      "Generated :<s>Get the default redirect URI. :param client_id: :param request: :return:</s>\n",
      "---------------------------\n",
      "Code: def delete_cloud_service(self, cloud_service_id): _validate_not_none('cloud_service_id', cloud_service_id) path = self._get_cloud_services_path(cloud_service_id) return self._perform_delete(path, as_async=True)\n",
      "Target: The Get Cloud Service operation gets all the resources (job collections)\n",
      "        in the cloud service.\n",
      "\n",
      "        cloud_service_id:\n",
      "            The cloud service id\n",
      "Generated :<s>Deletes a Cloud Service. :param str cloud_service_id: The ID of the Cloud Service to delete</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def get_or_create_folder(self, folder): q = \"mimeType='application/vnd.google-apps.folder' and name='%s'\" %folder response = self._service.files().list(q=q, spaces='drive').execute().get('files',[]) if len(response) == 0: folder = self._create_folder(folder) else: folder = response[0] return folder\n",
      "Target: create a folder at the drive root. If the folder already exists,\n",
      "       it is simply returned.\n",
      "\n",
      "           folder = self._get_or_create_folder(self._base)\n",
      "           $ folder\n",
      "             {'id': '1pXR5S8wufELh9Q-jDkhCoYu-BL1NqN9y'}\n",
      "Generated :<s>Get or create a folder. Args: folder (str): Folder name. Returns: str: Folder.</s>\n",
      "---------------------------\n",
      "Code: def formatday(self, day, weekday): self.wkday_not_today = '<td class=\"%s\"><div class=\"td-inner\">' % ( self.cssclasses[weekday]) self.wkday_today = ( '<td class=\"%s calendar-today\"><div class=\"td-inner\">' % ( self.cssclasses[weekday]) ) if URLS_NAMESPACE: url_name = '%s:day_list' % (URLS_NAMESPACE) else: url_name = 'day_list' self.day_url = reverse(url_name, args=(self.yr, self.mo, day)) self.day = day self.anch = '<a href=\"%s\">%d</a>' % ( self.day_url, day ) self.end = '</div></td>'\n",
      "Target: Set some commonly used variables.\n",
      "Generated :<s>Formats the day of the day.</s>\n",
      "---------------------------\n",
      "Code: def cartesian_product(parameter_dict, combined_parameters=()): if not combined_parameters: combined_parameters = list(parameter_dict) else: combined_parameters = list(combined_parameters) for idx, item in enumerate(combined_parameters): if isinstance(item, str): combined_parameters[idx] = (item,) iterator_list = [] for item_tuple in combined_parameters: inner_iterator_list = [parameter_dict[key] for key in item_tuple] zipped_iterator = zip(*inner_iterator_list) iterator_list.append(zipped_iterator) result_dict = {} for key in parameter_dict: result_dict[key] = [] cartesian_iterator = itools.product(*iterator_list) for cartesian_tuple in cartesian_iterator: for idx, item_tuple in enumerate(combined_parameters): for inneridx, key in enumerate(item_tuple): result_dict[key].append(cartesian_tuple[idx][inneridx]) return result_dict\n",
      "Target: Generates a Cartesian product of the input parameter dictionary.\n",
      "\n",
      "    For example:\n",
      "\n",
      "    >>> print cartesian_product({'param1':[1,2,3], 'param2':[42.0, 52.5]})\n",
      "    {'param1':[1,1,2,2,3,3],'param2': [42.0,52.5,42.0,52.5,42.0,52.5]}\n",
      "\n",
      "    :param parameter_dict:\n",
      "\n",
      "        Dictionary containing parameter names as keys and iterables of data to explore.\n",
      "\n",
      "    :param combined_parameters:\n",
      "\n",
      "        Tuple of tuples. Defines the order of the parameters and parameters that are\n",
      "        linked together.\n",
      "        If an inner tuple contains only a single item, you can spare the\n",
      "        inner tuple brackets.\n",
      "\n",
      "\n",
      "        For example:\n",
      "\n",
      "        >>> print cartesian_product( {'param1': [42.0, 52.5], 'param2':['a', 'b'], 'param3' : [1,2,3]}, ('param3',('param1', 'param2')))\n",
      "        {param3':[1,1,2,2,3,3],'param1' : [42.0,52.5,42.0,52.5,42.0,52.5], 'param2':['a','b','a','b','a','b']}\n",
      "\n",
      "    :returns: Dictionary with cartesian product lists.\n",
      "Generated :<s>Creates a cartesian product of a dictionary of parameters. Args: parameter_dict (dict): dictionary of</s>\n",
      "---------------------------\n",
      "Code: def _truncate_float(matchobj, format_str='0.2g'): if matchobj.group(0): return format(float(matchobj.group(0)), format_str) return ''\n",
      "Target: Truncate long floats\n",
      "\n",
      "    Args:\n",
      "        matchobj (re.Match): contains original float\n",
      "        format_str (str): format specifier\n",
      "    Returns:\n",
      "       str: returns truncated float\n",
      "Generated :<s>Truncate a float.</s>\n",
      "---------------------------\n",
      "Code: def excepthook(self, etype, evalue, tb): if self.verbose_crash: return self.crash_handler(etype, evalue, tb) else: return crashhandler.crash_handler_lite(etype, evalue, tb)\n",
      "Target: this is sys.excepthook after init_crashhandler\n",
      "        \n",
      "        set self.verbose_crash=True to use our full crashhandler, instead of\n",
      "        a regular traceback with a short message (crash_handler_lite)\n",
      "Generated :<s>Perform a crash handler for the given etype and evalue.</s>\n",
      "---------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a1552a871c4c059377212688e1877b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1653.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved due to improvement in 'val_loss' from 2.7199637416774984 to 2.7086113689887616\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def authentication(login, password): session = requests.Session() response = session.get('https://m.vk.com') url = re.search(r'action=\"([^\\\"]+)\"', response.text).group(1) data = {'email': login, 'pass': password} response = session.post(url, data=data) return session\n",
      "Target: Authentication on vk.com.\n",
      "\n",
      "    :param login: login on vk.com.\n",
      "    :param password: password on vk.com.\n",
      "    :returns: `requests.Session` session with cookies.\n",
      "Generated :<s>Authenticate against m.vk.com.</s>\n",
      "---------------------------\n",
      "Code: def emitError(self, level): if level in [ABORT, ERROR, WARNING, VERBOSE, VERBOSE1, VERBOSE2, VERBOSE3, DEBUG]: return True return False\n",
      "Target: determine if a level should print to\n",
      "        stderr, includes all levels but INFO and QUIET\n",
      "Generated :<s>Emit an error. Args: level (int): The level of error. Returns: bool: True if</s>\n",
      "---------------------------\n",
      "Code: def possible_exc_types(node): excs = [] if isinstance(node.exc, astroid.Name): inferred = utils.safe_infer(node.exc) if inferred: excs = [inferred.name] elif node.exc is None: handler = node.parent while handler and not isinstance(handler, astroid.ExceptHandler): handler = handler.parent if handler and handler.type: inferred_excs = astroid.unpack_infer(handler.type) excs = (exc.name for exc in inferred_excs if exc is not astroid.Uninferable) else: target = _get_raise_target(node) if isinstance(target, astroid.ClassDef): excs = [target.name] elif isinstance(target, astroid.FunctionDef): for ret in target.nodes_of_class(astroid.Return): if ret.frame() != target: continue val = utils.safe_infer(ret.value) if ( val and isinstance(val, (astroid.Instance, astroid.ClassDef)) and utils.inherit_from_std_ex(val) ): excs.append(val.name) try: return {exc for exc in excs if not utils.node_ignores_exception(node, exc)} except astroid.InferenceError: return set()\n",
      "Target: Gets all of the possible raised exception types for the given raise node.\n",
      "\n",
      "    .. note::\n",
      "\n",
      "        Caught exception types are ignored.\n",
      "\n",
      "\n",
      "    :param node: The raise node to find exception types for.\n",
      "    :type node: astroid.node_classes.NodeNG\n",
      "\n",
      "    :returns: A list of exception types possibly raised by :param:`node`.\n",
      "    :rtype: set(str)\n",
      "Generated :<s>Returns a set of possible exception types for the given node.</s>\n",
      "---------------------------\n",
      "Code: def clean_email(self): email = self.cleaned_data['email'] try: User._default_manager.get(email__iexact=email) except User.DoesNotExist: return email.lower() raise forms.ValidationError(self.error_messages['duplicate_email'])\n",
      "Target: Since User.email is unique, this check is redundant,\n",
      "        but it sets a nicer error message than the ORM. See #13147.\n",
      "Generated :<s>Validates that the email is valid.</s>\n",
      "---------------------------\n",
      "Code: def step(self, step_name): @contextmanager def step_context(step_name): if self.event_receiver.current_case is not None: raise Exception('cannot open a step within a step') self.event_receiver.begin_case(step_name, self.now_seconds(), self.name) try: yield self.event_receiver except: etype, evalue, tb = sys.exc_info() self.event_receiver.error('%r' % [etype, evalue, tb]) raise finally: self.event_receiver.end_case(step_name, self.now_seconds()) return step_context(step_name)\n",
      "Target: Start a new step. returns a context manager which allows you to\n",
      "        report an error\n",
      "Generated :<s>Context manager for opening a step within a step. :param step_name: step name :type step_name</s>\n",
      "---------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2ee2dcb94d4b65b3c617c0cd3b2eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=29253.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def _call(self, utterances_batch: list, utterances_ids: Optional[list]=None) -> list: batch_size = len(utterances_batch) ids = utterances_ids or list(range(batch_size)) batch_history = [self.history[utt_id] for utt_id in ids] responses = [] filtered = self.skills_filter(utterances_batch, batch_history) for skill_i, (filtered_utterances, skill) in enumerate(zip(filtered, self.wrapped_skills)): skill_i_utt_indexes = [utt_index for utt_index, utt_filter in enumerate(filtered_utterances) if utt_filter] if skill_i_utt_indexes: skill_i_utt_batch = [utterances_batch[i] for i in skill_i_utt_indexes] skill_i_utt_ids = [ids[i] for i in skill_i_utt_indexes] res = [(None, 0.)] * batch_size predicted, confidence = skill(skill_i_utt_batch, skill_i_utt_ids) for i, predicted, confidence in zip(skill_i_utt_indexes, predicted, confidence): res[i] = (predicted, confidence) responses.append(res) responses = self.skills_processor(utterances_batch, batch_history, *responses) return responses\n",
      "Target: Processes batch of utterances and returns corresponding responses batch.\n",
      "\n",
      "        Each call of Agent passes incoming utterances batch through skills filter,\n",
      "        agent skills, skills processor. Batch of dialog IDs can be provided, in\n",
      "        other case utterances indexes in incoming batch are used as dialog IDs.\n",
      "\n",
      "        Args:\n",
      "            utterances_batch: Batch of incoming utterances.\n",
      "            utterances_ids: Batch of dialog IDs corresponding to incoming utterances.\n",
      "\n",
      "        Returns:\n",
      "            responses: A batch of responses corresponding to the\n",
      "                utterance batch received by agent.\n",
      "Generated :<s>Returns a list of (predicted, confidence) tuples.</s>\n",
      "---------------------------\n",
      "Code: def recursive_unicode(obj): if isinstance(obj, dict): return dict((recursive_unicode(k), recursive_unicode(v)) for (k,v) in obj.iteritems()) elif isinstance(obj, list): return list(recursive_unicode(i) for i in obj) elif isinstance(obj, tuple): return tuple(recursive_unicode(i) for i in obj) elif isinstance(obj, bytes): return to_unicode(obj) else: return obj\n",
      "Target: Walks a simple data structure, converting byte strings to unicode.\n",
      "\n",
      "    Supports lists, tuples, and dictionaries.\n",
      "Generated :<s>Recursively recursively all recursively nested Python objects.</s>\n",
      "---------------------------\n",
      "Code: def run_with_advanced_retry(self, _retry_args, *args, **kwargs): self._retry_obj = tenacity.Retrying( **_retry_args ) self._retry_obj(self.run, *args, **kwargs)\n",
      "Target: Runs Hook.run() with a Tenacity decorator attached to it. This is useful for\n",
      "        connectors which might be disturbed by intermittent issues and should not\n",
      "        instantly fail.\n",
      "\n",
      "        :param _retry_args: Arguments which define the retry behaviour.\n",
      "            See Tenacity documentation at https://github.com/jd/tenacity\n",
      "        :type _retry_args: dict\n",
      "\n",
      "\n",
      "        :Example::\n",
      "\n",
      "            hook = HttpHook(http_conn_id='my_conn',method='GET')\n",
      "            retry_args = dict(\n",
      "                 wait=tenacity.wait_exponential(),\n",
      "                 stop=tenacity.stop_after_attempt(10),\n",
      "                 retry=requests.exceptions.ConnectionError\n",
      "             )\n",
      "             hook.run_with_advanced_retry(\n",
      "                     endpoint='v1/test',\n",
      "                     _retry_args=retry_args\n",
      "                 )\n",
      "Generated :<s>Wrapper for :meth:`_retry_obj`.</s>\n",
      "---------------------------\n",
      "Code: def gpio_interrupts_enable(self): try: bring_gpio_interrupt_into_userspace() set_gpio_interrupt_edge() except Timeout as e: raise InterruptEnableException( \"There was an error bringing gpio%d into userspace. %s\" % (GPIO_INTERRUPT_PIN, e.message) )\n",
      "Target: Enables GPIO interrupts.\n",
      "Generated :<s>Enable gpio interrupt into userspace.</s>\n",
      "---------------------------\n",
      "Code: def abort(self): assert not self.ready(), \"Can't abort, I am already done!\" return self._client.abort(self.msg_ids, targets=self._targets, block=True)\n",
      "Target: abort my tasks.\n",
      "Generated :<s>Abort the current task.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def _check_qargs(self, qargs): if not all(isinstance(i, tuple) and isinstance(i[0], QuantumRegister) and isinstance(i[1], int) for i in qargs): raise QiskitError(\"qarg not (QuantumRegister, int) tuple\") if not all(self.has_register(i[0]) for i in qargs): raise QiskitError(\"register not in this circuit\") for qubit in qargs: qubit[0].check_range(qubit[1])\n",
      "Target: Raise exception if a qarg is not in this circuit or bad format.\n",
      "Generated :<s>Check that all qubits are in this circuit. :param qargs: list of qubits :type qargs</s>\n",
      "---------------------------\n",
      "Code: def discover(path, filter_specs=filter_specs): for dirpath, _, filenames in os.walk(path): for spec in filter_specs(filenames): yield os.path.join(dirpath, spec)\n",
      "Target: Discover all of the specs recursively inside ``path``.\n",
      "\n",
      "    Successively yields the (full) relative paths to each spec.\n",
      "Generated :<s>Discovers all files in a directory. Args: path (str): Path to the directory. filter_</s>\n",
      "---------------------------\n",
      "Code: def enumerate_bool(bool_array, nstart=0): ind = bool_2_indices(bool_array) ns = np.full(bool_array.size, nstart, dtype=int) for n, lims in enumerate(ind): ns[lims[0]:lims[-1] + 1] = nstart + n + 1 return ns\n",
      "Target: Consecutively numbers contiguous booleans in array.\n",
      "\n",
      "    i.e. a boolean sequence, and resulting numbering\n",
      "    T F T T T F T F F F T T F\n",
      "    0-1 1 1 - 2 ---3 3 -\n",
      "\n",
      "    where ' - '\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    bool_array : array_like\n",
      "        Array of booleans.\n",
      "    nstart : int\n",
      "        The number of the first boolean group.\n",
      "Generated :<s>Enumerate a boolean array. Parameters ---------- bool_array : ndarray ndarray of boolean values</s>\n",
      "---------------------------\n",
      "Code: def get_org(self, organization_name=''): self.organization_name = organization_name if(organization_name == ''): self.organization_name = raw_input('Organization: ') print 'Getting organization.' self.org_retrieved = self.logged_in_gh.organization(organization_name)\n",
      "Target: Retrieves an organization via given org name. If given\n",
      "        empty string, prompts user for an org name.\n",
      "Generated :<s>Get organization.</s>\n",
      "---------------------------\n",
      "Code: def expensive(fn): attr = \"_cache_\" + fn.__name__ def _wrapped(self): if not hasattr(self, attr): setattr(self, attr, fn(self)) return getattr(self, attr) return _wrapped\n",
      "Target: A decorator to cache the result of an expensive operation.\n",
      "\n",
      "    Only applies to methods with no arguments.\n",
      "Generated :<s>Decorator to make sure that the decorated function has the same attributes as this object.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def fetch_items(self, category, **kwargs): from_date = kwargs['from_date'] to_date = kwargs['to_date'] if category == CATEGORY_ISSUE: items = self.__fetch_issues(from_date, to_date) elif category == CATEGORY_PULL_REQUEST: items = self.__fetch_pull_requests(from_date, to_date) else: items = self.__fetch_repo_info() return items\n",
      "Target: Fetch the items (issues or pull_requests)\n",
      "\n",
      "        :param category: the category of items to fetch\n",
      "        :param kwargs: backend arguments\n",
      "\n",
      "        :returns: a generator of items\n",
      "Generated :<s>Fetch a list of issues. :param category: category to fetch issues from :type category: CATEG</s>\n",
      "---------------------------\n",
      "Code: def add_pizza_to_basket(self, item, variant=VARIANT.MEDIUM, quantity=1): item_variant = item[variant] ingredients = item_variant['ingredients'].update([36, 42]) params = { 'stepId': 0, 'quantity': quantity, 'sizeId': variant, 'productId': item.item_id, 'ingredients': ingredients, 'productIdHalfTwo': 0, 'ingredientsHalfTwo': [], 'recipeReferrer': 0 } return self.__post('/Basket/AddPizza', json=params)\n",
      "Target: Add a pizza to the current basket.\n",
      "\n",
      "        :param Item item: Item from menu.\n",
      "        :param int variant: Item SKU id. Some defaults are defined in the VARIANT enum.\n",
      "        :param int quantity: The quantity of pizza to be added.\n",
      "        :return: A response having added a pizza to the current basket.\n",
      "        :rtype: requests.Response\n",
      "Generated :<s>Adds ingredients to an item's basket. :param item: The item to add ingredients to. :param variant:</s>\n",
      "---------------------------\n",
      "Code: def msg(self, msg): if hasattr(self.output, 'writeline'): self.output.writeline(msg) elif hasattr(self.output, 'writelines'): self.output.writelines(msg + \"\\n\") pass return\n",
      "Target: used to write to a debugger that is connected to this\n",
      "        server; `str' written will have a newline added to it\n",
      "Generated :<s>Write a string to the output.</s>\n",
      "---------------------------\n",
      "Code: def dump(ofp, *pb_objs, **kwargs): mode = 'wb' if isinstance(ofp, str): ostream = open(ofp, mode=mode, **kwargs) else: ostream = open(fileobj=ofp, mode=mode, **kwargs) with ostream: ostream.write(*pb_objs)\n",
      "Target: Write to a stream.\n",
      "\n",
      "    Args:\n",
      "        ofp (string or file-like object): output stream.\n",
      "        pb_objs (*protobuf.message.Message): list of protobuf message objects\n",
      "            to be written.\n",
      "Generated :<s>Dump a list of protobjs to a file.</s>\n",
      "---------------------------\n",
      "Code: def to_dict_with_content(fields, row, decrypt_func): assert(len(fields) == len(row)) field_names = list(map(_get_name, fields)) assert 'content' in field_names, \"Missing content field.\" result = dict(zip(field_names, row)) result['content'] = decrypt_func(result['content']) return result\n",
      "Target: Convert a SQLAlchemy row that contains a 'content' field to a dict.\n",
      "\n",
      "    ``decrypt_func`` will be applied to the ``content`` field of the row.\n",
      "\n",
      "    If row is None, return None.\n",
      "\n",
      "    Raises AssertionError if there is no field named 'content' in ``fields``.\n",
      "Generated :<s>Convert a list of fields into a dictionary. :param fields: List of field names. :param row:</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def find_path(name, config, wsonly=False): workspace = Workspace(config) config = config[\"workspaces\"] path_list = {} if name.find('/') != -1: wsonly = False try: ws, repo = name.split('/') except ValueError: raise ValueError(\"There is too many / in `name` argument. \" \"Argument syntax: `workspace/repository`.\") if (workspace.exists(ws)): if (repo in config[ws][\"repositories\"]): path_name = \"%s/%s\" % (ws, repo) path_list[path_name] = config[ws][\"repositories\"][repo] for ws_name, ws in sorted(config.items()): if (name == ws_name): if wsonly is True: return {ws_name: ws[\"path\"]} repositories = sorted(config[ws_name][\"repositories\"].items()) for name, path in repositories: path_list[\"%s/%s\" % (ws_name, name)] = path break for repo_name, repo_path in sorted(ws[\"repositories\"].items()): if (repo_name == name): path_list[\"%s/%s\" % (ws_name, repo_name)] = repo_path return path_list\n",
      "Target: Find path for given workspace and|or repository.\n",
      "Generated :<s>Return a list of all paths in a workspace/repository.</s>\n",
      "---------------------------\n",
      "Code: def failure_data(self): for result in itertools.chain( self.input_expectations, self.output_expectations, self.transforms ): if result.event_type == DagsterEventType.STEP_FAILURE: return result.step_failure_data\n",
      "Target: Returns the failing step's data that happened during this solid's execution, if any\n",
      "Generated :<s>Return the data for step failure.</s>\n",
      "---------------------------\n",
      "Code: def get_datasets_list(self, project_id=None): dataset_project_id = project_id if project_id else self.project_id try: datasets_list = self.service.datasets().list( projectId=dataset_project_id).execute(num_retries=self.num_retries)['datasets'] self.log.info(\"Datasets List: %s\", datasets_list) except HttpError as err: raise AirflowException( 'BigQuery job failed. Error was: {}'.format(err.content)) return datasets_list\n",
      "Target: Method returns full list of BigQuery datasets in the current project\n",
      "\n",
      "        .. seealso::\n",
      "            For more information, see:\n",
      "            https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/list\n",
      "\n",
      "        :param project_id: Google Cloud Project for which you\n",
      "            try to get all datasets\n",
      "        :type project_id: str\n",
      "        :return: datasets_list\n",
      "\n",
      "            Example of returned datasets_list: ::\n",
      "\n",
      "                   {\n",
      "                      \"kind\":\"bigquery#dataset\",\n",
      "                      \"location\":\"US\",\n",
      "                      \"id\":\"your-project:dataset_2_test\",\n",
      "                      \"datasetReference\":{\n",
      "                         \"projectId\":\"your-project\",\n",
      "                         \"datasetId\":\"dataset_2_test\"\n",
      "                      }\n",
      "                   },\n",
      "                   {\n",
      "                      \"kind\":\"bigquery#dataset\",\n",
      "                      \"location\":\"US\",\n",
      "                      \"id\":\"your-project:dataset_1_test\",\n",
      "                      \"datasetReference\":{\n",
      "                         \"projectId\":\"your-project\",\n",
      "                         \"datasetId\":\"dataset_1_test\"\n",
      "                      }\n",
      "                   }\n",
      "                ]\n",
      "Generated :<s>Get dataset list :param project_id: project_id of the dataset :return: dataset list</s>\n",
      "---------------------------\n",
      "Code: def execute(self, context): self._hook = SparkSqlHook(sql=self._sql, conf=self._conf, conn_id=self._conn_id, total_executor_cores=self._total_executor_cores, executor_cores=self._executor_cores, executor_memory=self._executor_memory, keytab=self._keytab, principal=self._principal, name=self._name, num_executors=self._num_executors, master=self._master, yarn_queue=self._yarn_queue ) self._hook.run_query()\n",
      "Target: Call the SparkSqlHook to run the provided sql query\n",
      "Generated :<s>Executes the query with the given context.</s>\n",
      "---------------------------\n",
      "Code: def _build_latex_array(self, aliases=None): columns = 1 if aliases: qregdata = {} for q in aliases.values(): if q[0] not in qregdata: qregdata[q[0]] = q[1] + 1 elif qregdata[q[0]] < q[1] + 1: qregdata[q[0]] = q[1] + 1 else: qregdata = self.qregs for column, layer in enumerate(self.ops, 1): for op in layer: if op.condition: mask = self._get_mask(op.condition[0]) cl_reg = self.clbit_list[self._ffs(mask)] if_reg = cl_reg[0] pos_2 = self.img_regs[cl_reg] if_value = format(op.condition[1], 'b').zfill(self.cregs[if_reg])[::-1] if op.name not in ['measure', 'barrier', 'snapshot', 'load', 'save', 'noise']: nm = op.name qarglist = op.qargs if aliases is not None: qarglist = map(lambda x: aliases[x], qarglist) if len(qarglist) == 1: pos_1 = self.img_regs[(qarglist[0][0], qarglist[0][1])] if op.condition: mask = self._get_mask(op.condition[0]) cl_reg = self.clbit_list[self._ffs(mask)] if_reg = cl_reg[0] pos_2 = self.img_regs[cl_reg] if nm == \"x\": self._latex[pos_1][column] = \"\\\\gate{X}\" elif nm == \"y\": self._latex[pos_1][column] = \"\\\\gate{Y}\" elif nm == \"z\": self._latex[pos_1][column] = \"\\\\gate{Z}\" elif nm == \"h\": self._latex[pos_1][column] = \"\\\\gate{H}\" elif nm == \"s\": self._latex[pos_1][column] = \"\\\\gate{S}\" elif nm == \"sdg\": self._latex[pos_1][column] = \"\\\\gate{S^\\\\dag}\" elif nm == \"t\": self._latex[pos_1][column] = \"\\\\gate{T}\" elif nm == \"tdg\": self._latex[pos_1][column] = \"\\\\gate{T^\\\\dag}\" elif nm == \"u0\": self._latex[pos_1][column] = \"\\\\gate{U_0(%s)}\" % ( op.op.params[0]) elif nm == \"u1\": self._latex[pos_1][column] = \"\\\\gate{U_1(%s)}\" % ( op.op.params[0]) elif nm == \"u2\": self._latex[pos_1][column] = \"\\\\gate{U_2\\\\left(%s,%s\\\\right)}\" % ( op.op.params[0], op.op.params[1]) elif nm == \"u3\": self._latex[pos_1][column] = (\"\\\\gate{U_3(%s,%s,%s)}\" % ( op.op.params[0], op.op.params[1], op.op.params[2])) elif nm == \"rx\": self._latex[pos_1][column] = \"\\\\gate{R_x(%s)}\" % ( op.op.params[0]) elif nm == \"ry\": self._latex[pos_1][column] = \"\\\\gate{R_y(%s)}\" % ( op.op.params[0]) elif nm == \"rz\": self._latex[pos_1][column] = \"\\\\gate{R_z(%s)}\" % ( op.op.params[0]) else: self._latex[pos_1][columns] = \"\\\\gate{%s}\" % nm gap = pos_2 - pos_1 for i in range(self.cregs[if_reg]): if if_value[i] == '1': self._latex[pos_2 + i][column] = \"\\\\control \\\\cw \\\\cwx[-\" + str(gap) + \"]\" gap = 1 else: self._latex[pos_2 + i][column] = \"\\\\controlo \\\\cw \\\\cwx[-\" + str(gap) + \"]\" gap = 1 else: if nm == \"x\": self._latex[pos_1][column] = \"\\\\gate{X}\" elif nm == \"y\": self._latex[pos_1][column] = \"\\\\gate{Y}\" elif nm == \"z\": self._latex[pos_1][column] = \"\\\\gate{Z}\" elif nm == \"h\": self._latex[pos_1][column] = \"\\\\gate{H}\" elif nm == \"s\": self._latex[pos_1][column] = \"\\\\gate{S}\" elif nm == \"sdg\": self._latex[pos_1][column] = \"\\\\gate{S^\\\\dag}\" elif nm == \"t\": self._latex[pos_1][column] = \"\\\\gate{T}\" elif nm == \"tdg\": self._latex[pos_1][column] = \"\\\\gate{T^\\\\dag}\" elif nm == \"u0\": self._latex[pos_1][column] = \"\\\\gate{U_0(%s)}\" % ( op.op.params[0]) elif nm == \"u1\": self._latex[pos_1][column] = \"\\\\gate{U_1(%s)}\" % ( op.op.params[0]) elif nm == \"u2\": self._latex[pos_1][column] = \"\\\\gate{U_2\\\\left(%s,%s\\\\right)}\" % ( op.op.params[0], op.op.params[1]) elif nm == \"u3\": self._latex[pos_1][column] = (\"\\\\gate{U_3(%s,%s,%s)}\" % ( op.op.params[0], op.op.params[1], op.op.params[2])) elif nm == \"rx\": self._latex[pos_1][column] = \"\\\\gate{R_x(%s)}\" % ( op.op.params[0]) elif nm == \"ry\": self._latex[pos_1][column] = \"\\\\gate{R_y(%s)}\" % ( op.op.params[0]) elif nm == \"rz\": self._latex[pos_1][column] = \"\\\\gate{R_z(%s)}\" % ( op.op.params[0]) elif nm == \"reset\": self._latex[pos_1][column] = ( \"\\\\push{\\\\rule{.6em}{0em}\\\\ket{0}\\\\\" \"rule{.2em}{0em}} \\\\qw\") else: self._latex[pos_1][columns] = \"\\\\gate{%s}\" % nm elif len(qarglist) == 2: pos_1 = self.img_regs[(qarglist[0][0], qarglist[0][1])] pos_2 = self.img_regs[(qarglist[1][0], qarglist[1][1])] if op.condition: pos_3 = self.img_regs[(if_reg, 0)] temp = [pos_1, pos_2, pos_3] temp.sort(key=int) bottom = temp[1] gap = pos_3 - bottom for i in range(self.cregs[if_reg]): if if_value[i] == '1': self._latex[pos_3 + i][column] = \"\\\\control \\\\cw \\\\cwx[-\" + str(gap) + \"]\" gap = 1 else: self._latex[pos_3 + i][column] = \"\\\\controlo \\\\cw \\\\cwx[-\" + str(gap) + \"]\" gap = 1 if nm == \"cx\": self._latex[pos_1][column] = \"\\\\ctrl{\" + str(pos_2 - pos_1) + \"}\" self._latex[pos_2][column] = \"\\\\targ\" elif nm == \"cz\": self._latex[pos_1][column] = \"\\\\ctrl{\" + str(pos_2 - pos_1) + \"}\" self._latex[pos_2][column] = \"\\\\control\\\\qw\" elif nm == \"cy\": self._latex[pos_1][column] = \"\\\\ctrl{\" + str(pos_2 - pos_1) + \"}\" self._latex[pos_2][column] = \"\\\\gate{Y}\" elif nm == \"ch\": self._latex[pos_1][column] = \"\\\\ctrl{\" + str(pos_2 - pos_1) + \"}\" self._latex[pos_2][column] = \"\\\\gate{H}\" elif nm == \"swap\": self._latex[pos_1][column] = \"\\\\qswap\" self._latex[pos_2][column] = \"\\\\qswap \\\\qwx[\" + str(pos_1 - pos_2) + \"]\" elif nm == \"crz\": self._latex[pos_1][column] = \"\\\\ctrl{\" + str(pos_2 - pos_1) + \"}\" self._latex[pos_2][column] = \"\\\\gate{R_z(%s)}\" % (op.op.params[0]) elif nm == \"cu1\": self._latex[pos_1][column - 1] = \"\\\\ctrl{\" + str( pos_2 - pos_1) + \"}\" self._latex[pos_2][column - 1] = \"\\\\control\\\\qw\" self._latex[min(pos_1, pos_2)][column] = \"\\\\dstick{%s}\\\\qw\" % (op.op.params[0]) self._latex[max(pos_1, pos_2)][column] = \"\\\\qw\" elif nm == \"cu3\": self._latex[pos_1][column] = \"\\\\ctrl{\" + str(pos_2 - pos_1) + \"}\" self._latex[pos_2][column] = \"\\\\gate{U_3(%s,%s,%s)}\" % (op.op.params[0], op.op.params[1], op.op.params[2]) else: temp = [pos_1, pos_2] temp.sort(key=int) if nm == \"cx\": self._latex[pos_1][column] = \"\\\\ctrl{\" + str( pos_2 - pos_1) + \"}\" self._latex[pos_2][column] = \"\\\\targ\" elif nm == \"cz\": self._latex[pos_1][column] = \"\\\\ctrl{\" + str( pos_2 - pos_1) + \"}\" self._latex[pos_2][column] = \"\\\\control\\\\qw\" elif nm == \"cy\": self._latex[pos_1][column] = \"\\\\ctrl{\" + str( pos_2 - pos_1) + \"}\" self._latex[pos_2][column] = \"\\\\gate{Y}\" elif nm == \"ch\": self._latex[pos_1][column] = \"\\\\ctrl{\" + str( pos_2 - pos_1) + \"}\" self._latex[pos_2][column] = \"\\\\gate{H}\" elif nm == \"swap\": self._latex[pos_1][column] = \"\\\\qswap\" self._latex[pos_2][column] = \"\\\\qswap \\\\qwx[\" + str(pos_1 - pos_2) + \"]\" elif nm == \"crz\": self._latex[pos_1][column] = \"\\\\ctrl{\" + str( pos_2 - pos_1) + \"}\" self._latex[pos_2][column] = \"\\\\gate{R_z(%s)}\" % (op.op.params[0]) elif nm == \"cu1\": self._latex[pos_1][column - 1] = \"\\\\ctrl{\" + str( pos_2 - pos_1) + \"}\" self._latex[pos_2][column - 1] = \"\\\\control\\\\qw\" self._latex[min(pos_1, pos_2)][column] = \"\\\\dstick{%s}\\\\qw\" % (op.op.params[0]) self._latex[max(pos_1, pos_2)][column] = \"\\\\qw\" elif nm == \"cu3\": self._latex[pos_1][column] = \"\\\\ctrl{\" + str( pos_2 - pos_1) + \"}\" self._latex[pos_2][column] = (\"\\\\gate{U_3(%s,%s,%s)}\" % ( op.op.params[0], op.op.params[1], op.op.params[2])) else: start_pos = min([pos_1, pos_2]) stop_pos = max([pos_1, pos_2]) if stop_pos - start_pos >= 2: delta = stop_pos - start_pos self._latex[start_pos][columns] = ( \"\\\\multigate{%s}{%s}\" % (delta, nm)) for i_pos in range(start_pos + 1, stop_pos + 1): self._latex[i_pos][columns] = \"\\\\ghost{%s}\" % nm else: self._latex[start_pos][columns] = ( \"\\\\multigate{1}{%s}\" % nm) self._latex[stop_pos][columns] = \"\\\\ghost{%s}\" % nm elif len(qarglist) == 3: pos_1 = self.img_regs[(qarglist[0][0], qarglist[0][1])] pos_2 = self.img_regs[(qarglist[1][0], qarglist[1][1])] pos_3 = self.img_regs[(qarglist[2][0], qarglist[2][1])] if op.condition: pos_4 = self.img_regs[(if_reg, 0)] temp = [pos_1, pos_2, pos_3, pos_4] temp.sort(key=int) bottom = temp[2] prev_column = [x[column - 1] for x in self._latex] for item, prev_entry in enumerate(prev_column): if 'barrier' in prev_entry: span = re.search('barrier{(.*)}', prev_entry) if span and any(i in temp for i in range( item, int(span.group(1)))): self._latex[item][column - 1] = prev_entry.replace( '\\\\barrier{', '\\\\barrier[-0.65em]{') gap = pos_4 - bottom for i in range(self.cregs[if_reg]): if if_value[i] == '1': self._latex[pos_4 + i][column] = \"\\\\control \\\\cw \\\\cwx[-\" + str(gap) + \"]\" gap = 1 else: self._latex[pos_4 + i][column] = \"\\\\controlo \\\\cw \\\\cwx[-\" + str(gap) + \"]\" gap = 1 if nm == \"ccx\": self._latex[pos_1][column] = \"\\\\ctrl{\" + str( pos_2 - pos_1) + \"}\" self._latex[pos_2][column] = \"\\\\ctrl{\" + str( pos_3 - pos_2) + \"}\" self._latex[pos_3][column] = \"\\\\targ\" if nm == \"cswap\": self._latex[pos_1][column] = \"\\\\ctrl{\" + str( pos_2 - pos_1) + \"}\" self._latex[pos_2][column] = \"\\\\qswap\" self._latex[pos_3][column] = \"\\\\qswap \\\\qwx[\" + str(pos_2 - pos_3) + \"]\" else: temp = [pos_1, pos_2, pos_3] temp.sort(key=int) prev_column = [x[column - 1] for x in self._latex] for item, prev_entry in enumerate(prev_column): if 'barrier' in prev_entry: span = re.search('barrier{(.*)}', prev_entry) if span and any(i in temp for i in range( item, int(span.group(1)))): self._latex[item][column - 1] = prev_entry.replace( '\\\\barrier{', '\\\\barrier[-0.65em]{') if nm == \"ccx\": self._latex[pos_1][column] = \"\\\\ctrl{\" + str( pos_2 - pos_1) + \"}\" self._latex[pos_2][column] = \"\\\\ctrl{\" + str( pos_3 - pos_2) + \"}\" self._latex[pos_3][column] = \"\\\\targ\" elif nm == \"cswap\": self._latex[pos_1][column] = \"\\\\ctrl{\" + str( pos_2 - pos_1) + \"}\" self._latex[pos_2][column] = \"\\\\qswap\" self._latex[pos_3][column] = \"\\\\qswap \\\\qwx[\" + str(pos_2 - pos_3) + \"]\" else: start_pos = min([pos_1, pos_2, pos_3]) stop_pos = max([pos_1, pos_2, pos_3]) if stop_pos - start_pos >= 3: delta = stop_pos - start_pos self._latex[start_pos][columns] = ( \"\\\\multigate{%s}{%s}\" % (delta, nm)) for i_pos in range(start_pos + 1, stop_pos + 1): self._latex[i_pos][columns] = \"\\\\ghost{%s}\" % nm else: self._latex[pos_1][columns] = ( \"\\\\multigate{2}{%s}\" % nm) self._latex[pos_2][columns] = \"\\\\ghost{%s}\" % nm self._latex[pos_3][columns] = \"\\\\ghost{%s}\" % nm elif len(qarglist) > 3: nbits = len(qarglist) pos_array = [self.img_regs[(qarglist[0][0], qarglist[0][1])]] for i in range(1, nbits): pos_array.append(self.img_regs[(qarglist[i][0], qarglist[i][1])]) pos_start = min(pos_array) pos_stop = max(pos_array) delta = pos_stop - pos_start self._latex[pos_start][columns] = ( \"\\\\multigate{%s}{%s}\" % (nbits - 1, nm)) for pos in range(pos_start + 1, pos_stop + 1): self._latex[pos][columns] = \"\\\\ghost{%s}\" % nm elif op.name == \"measure\": if (len(op.cargs) != 1 or len(op.qargs) != 1 or op.op.params): raise exceptions.VisualizationError(\"bad operation record\") if op.condition: raise exceptions.VisualizationError( \"If controlled measures currently not supported.\") qname, qindex = op.qargs[0] cname, cindex = op.cargs[0] if aliases: newq = aliases[(qname, qindex)] qname = newq[0] qindex = newq[1] pos_1 = self.img_regs[(qname, qindex)] pos_2 = self.img_regs[(cname, cindex)] try: self._latex[pos_1][column] = \"\\\\meter\" prev_column = [x[column - 1] for x in self._latex] for item, prev_entry in enumerate(prev_column): if 'barrier' in prev_entry: span = re.search('barrier{(.*)}', prev_entry) if span and ( item + int(span.group(1))) - pos_1 >= 0: self._latex[item][column - 1] = prev_entry.replace( '\\\\barrier{', '\\\\barrier[-1.15em]{') self._latex[pos_2][column] = \"\\\\cw \\\\cwx[-\" + str(pos_2 - pos_1) + \"]\" except Exception as e: raise exceptions.VisualizationError( 'Error during Latex building: %s' % str(e)) elif op.name in ['barrier', 'snapshot', 'load', 'save', 'noise']: if self.plot_barriers: qarglist = op.qargs indexes = [self._get_qubit_index(x) for x in qarglist] start_bit = self.qubit_list[min(indexes)] if aliases is not None: qarglist = map(lambda x: aliases[x], qarglist) start = self.img_regs[start_bit] span = len(op.qargs) - 1 self._latex[start][column] = \"\\\\qw \\\\barrier{\" + str( span) + \"}\" else: raise exceptions.VisualizationError(\"bad node data\")\n",
      "Target: Returns an array of strings containing \\\\LaTeX for this circuit.\n",
      "\n",
      "        If aliases is not None, aliases contains a dict mapping\n",
      "        the current qubits in the circuit to new qubit names.\n",
      "        We will deduce the register names and sizes from aliases.\n",
      "Generated :<s>Build a numpy array from a list of aliases. :param aliases: :return:</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def parse_object_id(collection, id): collection = _validate_string_argument(collection, 'collection') return KeyVaultIdentifier(uri=id, collection=collection)\n",
      "Target: :param collection: The resource collection type.\n",
      "        :type collection: str\n",
      "        :param id: The resource uri.\n",
      "        :type id: str\n",
      "        :rtype: KeyVaultId\n",
      "Generated :<s>Parses a key vault identifier. :param collection: The name of the collection. :param id: The</s>\n",
      "---------------------------\n",
      "Code: def send_plain(self): values = {\"registration_id\": self._registration_id} for key, val in self._data.items(): values[\"data.%s\" % (key)] = val.encode(self.encoding) for key, val in self._kwargs.items(): if val and isinstance(val, bool): val = 1 values[key] = val data = urlencode(sorted(values.items())).encode(self.encoding) result = self._send(data, \"application/x-www-form-urlencoded;charset=UTF-8\") if result.startswith(\"Error=\"): if result in (\"Error=NotRegistered\", \"Error=InvalidRegistration\"): return result raise GCMPushError(result) return result\n",
      "Target: Sends a text/plain GCM message\n",
      "Generated :<s>Send the request to GCMP and return the response.</s>\n",
      "---------------------------\n",
      "Code: def run(self): self._logger.info(\"Parsing data files for ssGSEA...........................\") data = self.load_data() normdat = self.norm_samples(data) gmt = self.load_gmt(gene_list=normdat.index.values, gmt=self.gene_sets) self._logger.info(\"%04d gene_sets used for further statistical testing.....\"% len(gmt)) self._set_cores() self._logger.info(\"Start to run ssGSEA...Might take a while................\") if self.permutation_num == 0 : self.runSamples(df=normdat, gmt=gmt) else: self._logger.warning(\"run ssGSEA with permutation procedure, don't use these part of results for publication.\") self.runSamplesPermu(df=normdat, gmt=gmt) if self._outdir is None: self._tmpdir.cleanup()\n",
      "Target: run entry\n",
      "Generated :<s>Run ssGSEA analyses.</s>\n",
      "---------------------------\n",
      "Code: def ParseID3v1(data): try: data = data[data.index(b'TAG'):] except ValueError: return None if 128 < len(data) or len(data) < 124: return None unpack_fmt = \"3s30s30s30s%ds29sBB\" % (len(data) - 124) try: tag, title, artist, album, year, comment, track, genre = unpack( unpack_fmt, data) except StructError: return None if tag != b\"TAG\": return None def fix(data): return data.split(b'\\x00')[0].strip().decode('latin1') title, artist, album, year, comment = map( fix, [title, artist, album, year, comment]) frames = {} if title: frames['TIT2'] = TIT2(encoding=0, text=title) if artist: frames['TPE1'] = TPE1(encoding=0, text=[artist]) if album: frames['TALB'] = TALB(encoding=0, text=album) if year: frames['TDRC'] = TDRC(encoding=0, text=year) if comment: frames['COMM'] = COMM(encoding=0, lang='eng', desc=\"ID3v1 Comment\", text=comment) if track and ((track != 32) or (data[-3] == b'\\x00'[0])): frames['TRCK'] = TRCK(encoding=0, text=str(track)) if genre != 255: frames['TCON'] = TCON(encoding=0, text=str(genre)) return frames\n",
      "Target: Parse an ID3v1 tag, returning a list of ID3v2.4 frames.\n",
      "Generated :<s>Parse a ID3v1 tag.</s>\n",
      "---------------------------\n",
      "Code: def get_pages(self, namespace, apcontinue=''): params = { \"action\": \"query\", \"list\": \"allpages\", \"aplimit\": self.limit, \"apnamespace\": namespace, \"format\": \"json\" } if apcontinue: params['apcontinue'] = apcontinue return self.call(params)\n",
      "Target: Retrieve all pages from a namespace starting from apcontinue.\n",
      "Generated :<s>Gets all pages in the specified namespace. :param namespace: Namespace to get pages from. :param ap</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def _parse_list_nested_recursive( cls, data, path, iterators, list_vars, cur_vars=None ): cur_vars = dict(cur_vars) if cur_vars else {} if path: p = path[0] path = path[1:] else: for _ in data: list_vars.append(cur_vars) iterators.append(data) return data if p.startswith(\"?\"): for x in data: key, var_path = p.split(\".\") cur_vars.update({key.lstrip(\"?\"): x.xpath(var_path)[0].text}) cls._parse_list_nested_recursive( x, path, iterators, list_vars, cur_vars ) else: x = data.xpath(p) cls._parse_list_nested_recursive(x, path, iterators, list_vars, cur_vars)\n",
      "Target: This helps parsing shit like:\n",
      "\n",
      "            <protocols>\n",
      "                <bgp>\n",
      "                    <group>\n",
      "                        <name>my_peers</name>\n",
      "                        <neighbor>\n",
      "                            <name>192.168.100.2</name>\n",
      "                            <description>adsasd</description>\n",
      "                            <peer-as>65100</peer-as>\n",
      "                        </neighbor>\n",
      "                        <neighbor>\n",
      "                            <name>192.168.100.3</name>\n",
      "                            <peer-as>65100</peer-as>\n",
      "                        </neighbor>\n",
      "                    </group>\n",
      "                    <group>\n",
      "                        <name>my_other_peers</name>\n",
      "                        <neighbor>\n",
      "                            <name>172.20.0.1</name>\n",
      "                            <peer-as>65200</peer-as>\n",
      "                        </neighbor>\n",
      "                    </group>\n",
      "                </bgp>\n",
      "            </protocols>\n",
      "Generated :<s>Recursively parse a list of nested lists.</s>\n",
      "---------------------------\n",
      "Code: def search(self, buffer, freshlen, searchwindowsize=None): absurd_match = len(buffer) first_match = absurd_match for index, s in self._strings: if searchwindowsize is None: offset = -(freshlen+len(s)) else: offset = -searchwindowsize n = buffer.find(s, offset) if n >= 0 and n < first_match: first_match = n best_index, best_match = index, s if first_match == absurd_match: return -1 self.match = best_match self.start = first_match self.end = self.start + len(self.match) return best_index\n",
      "Target: This searches 'buffer' for the first occurence of one of the search\n",
      "        strings.  'freshlen' must indicate the number of bytes at the end of\n",
      "        'buffer' which have not been searched before. It helps to avoid\n",
      "        searching the same, possibly big, buffer over and over again.\n",
      "\n",
      "        See class spawn for the 'searchwindowsize' argument.\n",
      "\n",
      "        If there is a match this returns the index of that string, and sets\n",
      "        'start', 'end' and 'match'. Otherwise, this returns -1.\n",
      "Generated :<s>Find the first match in the buffer. :param buffer: The buffer to search. :param freshlen: The</s>\n",
      "---------------------------\n",
      "Code: def __get(self, path, **kargs): return self.__call_api(self.session.get, path, **kargs)\n",
      "Target: Make a HTTP GET request to the Dominos UK API with the given parameters\n",
      "        for the current session.\n",
      "\n",
      "        :param string path: The API endpoint path.\n",
      "        :params list kargs: A list of arguments.\n",
      "        :return: A response from the Dominos UK API.\n",
      "        :rtype: response.Response\n",
      "Generated :<s>Call the API with the given path and kwargs.</s>\n",
      "---------------------------\n",
      "Code: def overrides_a_method(class_node: astroid.node_classes.NodeNG, name: str) -> bool: for ancestor in class_node.ancestors(): if name in ancestor and isinstance(ancestor[name], astroid.FunctionDef): return True return False\n",
      "Target: return True if <name> is a method overridden from an ancestor\n",
      "Generated :<s>Check if a method is overridden by a class. :param class_node: The class to check. :</s>\n",
      "---------------------------\n",
      "Code: def clinvar_objs(self, submission_id, key_id): submission = self.clinvar_submission_collection.find_one({'_id': ObjectId(submission_id)}) if submission.get(key_id): clinvar_obj_ids = list(submission.get(key_id)) clinvar_objects = self.clinvar_collection.find({'_id' : { \"$in\": clinvar_obj_ids }}) return list(clinvar_objects) else: return None\n",
      "Target: Collects a list of objects from the clinvar collection (variants of case data) as specified by the key_id in the clinvar submission\n",
      "\n",
      "            Args:\n",
      "                submission_id(str): the _id key of a clinvar submission\n",
      "                key_id(str) : either 'variant_data' or 'case_data'. It's a key in a clinvar_submission object.\n",
      "                              Its value is a list of ids of clinvar objects (either variants of casedata objects)\n",
      "\n",
      "            Returns:\n",
      "                clinvar_objects(list) : a list of clinvar objects (either variants of casedata)\n",
      "Generated :<s>:param submission_id: :param key_id:</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def create_subscription(self, topic_project, topic, subscription=None, subscription_project=None, ack_deadline_secs=10, fail_if_exists=False): service = self.get_conn() full_topic = _format_topic(topic_project, topic) if not subscription: subscription = 'sub-{}'.format(uuid4()) if not subscription_project: subscription_project = topic_project full_subscription = _format_subscription(subscription_project, subscription) body = { 'topic': full_topic, 'ackDeadlineSeconds': ack_deadline_secs } try: service.projects().subscriptions().create( name=full_subscription, body=body).execute(num_retries=self.num_retries) except HttpError as e: if str(e.resp['status']) == '409': message = 'Subscription already exists: {}'.format( full_subscription) self.log.warning(message) if fail_if_exists: raise PubSubException(message) else: raise PubSubException( 'Error creating subscription {}'.format(full_subscription), e) return subscription\n",
      "Target: Creates a Pub/Sub subscription, if it does not already exist.\n",
      "\n",
      "        :param topic_project: the GCP project ID of the topic that the\n",
      "            subscription will be bound to.\n",
      "        :type topic_project: str\n",
      "        :param topic: the Pub/Sub topic name that the subscription will be bound\n",
      "            to create; do not include the ``projects/{project}/subscriptions/``\n",
      "            prefix.\n",
      "        :type topic: str\n",
      "        :param subscription: the Pub/Sub subscription name. If empty, a random\n",
      "            name will be generated using the uuid module\n",
      "        :type subscription: str\n",
      "        :param subscription_project: the GCP project ID where the subscription\n",
      "            will be created. If unspecified, ``topic_project`` will be used.\n",
      "        :type subscription_project: str\n",
      "        :param ack_deadline_secs: Number of seconds that a subscriber has to\n",
      "            acknowledge each message pulled from the subscription\n",
      "        :type ack_deadline_secs: int\n",
      "        :param fail_if_exists: if set, raise an exception if the topic\n",
      "            already exists\n",
      "        :type fail_if_exists: bool\n",
      "        :return: subscription name which will be the system-generated value if\n",
      "            the ``subscription`` parameter is not supplied\n",
      "        :rtype: str\n",
      "Generated :<s>Create a new subscription. :param str topic_project: Name of the topic to subscribe to :type topic_</s>\n",
      "---------------------------\n",
      "Code: def border(self): if callable(self._border): return util.WatchingList(self._border(*(self.widget.pos+self.widget.size)),self._wlredraw_border) else: return util.WatchingList(self._border,self._wlredraw_border)\n",
      "Target: Property to be used for setting and getting the border of the layer.\n",
      "        \n",
      "        Note that setting this property causes an immediate redraw.\n",
      "Generated :<s>The :class:`~werkzeug.core.filters.WatchingList` or :class</s>\n",
      "---------------------------\n",
      "Code: def fallback_to_default_project_id(func): @functools.wraps(func) def inner_wrapper(self, *args, **kwargs): if len(args) > 0: raise AirflowException( \"You must use keyword arguments in this methods rather than\" \" positional\") if 'project_id' in kwargs: kwargs['project_id'] = self._get_project_id(kwargs['project_id']) else: kwargs['project_id'] = self._get_project_id(None) if not kwargs['project_id']: raise AirflowException(\"The project id must be passed either as \" \"keyword project_id parameter or as project_id extra \" \"in GCP connection definition. Both are not set!\") return func(self, *args, **kwargs) return inner_wrapper\n",
      "Target: Decorator that provides fallback for Google Cloud Platform project id. If\n",
      "        the project is None it will be replaced with the project_id from the\n",
      "        service account the Hook is authenticated with. Project id can be specified\n",
      "        either via project_id kwarg or via first parameter in positional args.\n",
      "\n",
      "        :param func: function to wrap\n",
      "        :return: result of the function call\n",
      "Generated :<s>Decorator to use the default project_id for the function. :param func: :return:</s>\n",
      "---------------------------\n",
      "Code: def done(self): if self._process is None: raise AirflowException(\"Tried to see if it's done before starting!\") if self._done: return True if self._result_queue and not self._result_queue.empty(): self._result = self._result_queue.get_nowait() self._done = True self.log.debug(\"Waiting for %s\", self._process) self._process.join() return True if self._result_queue and not self._process.is_alive(): self._done = True if not self._result_queue.empty(): self._result = self._result_queue.get_nowait() self.log.debug(\"Waiting for %s\", self._process) self._process.join() return True return False\n",
      "Target: Check if the process launched to process this file is done.\n",
      "\n",
      "        :return: whether the process is finished running\n",
      "        :rtype: bool\n",
      "Generated :<s>Waits until the result queue is empty. :return: True if the result queue has been empty, else False</s>\n",
      "---------------------------\n",
      "Code: def flexible_str(obj): if obj is None: return None elif(not isinstance(obj, str) and isinstance(obj, collections.abc.Iterable)): return ', '.join(str(item) for item in sorted(obj)) else: return str(obj)\n",
      "Target: A more flexible str function which intelligently handles stringifying\n",
      "    strings, lists and other iterables. The results are lexographically sorted\n",
      "    to ensure generated responses are consistent when iterables such as Set\n",
      "    are used.\n",
      "Generated :<s>Convert an object to a string representation. Args: obj (object): Object to convert. Returns: str</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def cosmic_link(variant_obj): cosmic_ids = variant_obj.get('cosmic_ids') if not cosmic_ids: return None else: cosmic_id = cosmic_ids[0] url_template = (\"https://cancer.sanger.ac.uk/cosmic/mutation/overview?id={}\") return url_template.format(cosmic_id)\n",
      "Target: Compose link to COSMIC Database.\n",
      "\n",
      "    Args:\n",
      "        variant_obj(scout.models.Variant)\n",
      "\n",
      "    Returns:\n",
      "        url_template(str): Link to COSMIIC database if cosmic id is present\n",
      "Generated :<s>Returns the URL of the cosmic link for the given variant object.</s>\n",
      "---------------------------\n",
      "Code: def hager_zhang(value_and_gradients_function, initial_step_size=None, value_at_initial_step=None, value_at_zero=None, converged=None, threshold_use_approximate_wolfe_condition=1e-6, shrinkage_param=0.66, expansion_param=5.0, sufficient_decrease_param=0.1, curvature_param=0.9, step_size_shrink_param=0.1, max_iterations=50, name=None): with tf.compat.v1.name_scope(name, 'hager_zhang', [ initial_step_size, value_at_initial_step, value_at_zero, converged, threshold_use_approximate_wolfe_condition, shrinkage_param, expansion_param, sufficient_decrease_param, curvature_param]): val_0, val_initial, f_lim, prepare_evals = _prepare_args( value_and_gradients_function, initial_step_size, value_at_initial_step, value_at_zero, threshold_use_approximate_wolfe_condition) valid_inputs = (hzl.is_finite(val_0) & (val_0.df < 0) & tf.math.is_finite(val_initial.x) & (val_initial.x > 0)) if converged is None: init_converged = tf.zeros_like(valid_inputs) else: init_converged = tf.convert_to_tensor(value=converged) failed = ~init_converged & ~valid_inputs active = ~init_converged & valid_inputs fix_step_evals, val_c, fix_failed = _fix_step_size( value_and_gradients_function, val_initial, active, step_size_shrink_param) init_interval = HagerZhangLineSearchResult( converged=init_converged, failed=failed | fix_failed, func_evals=prepare_evals + fix_step_evals, iterations=tf.convert_to_tensor(value=0), left=val_0, right=hzl.val_where(init_converged, val_0, val_c)) def _apply_bracket_and_search(): return _bracket_and_search( value_and_gradients_function, init_interval, f_lim, max_iterations, shrinkage_param, expansion_param, sufficient_decrease_param, curvature_param) init_active = ~init_interval.failed & ~init_interval.converged return prefer_static.cond( tf.reduce_any(input_tensor=init_active), _apply_bracket_and_search, lambda: init_interval)\n",
      "Target: The Hager Zhang line search algorithm.\n",
      "\n",
      "  Performs an inexact line search based on the algorithm of\n",
      "  [Hager and Zhang (2006)][2].\n",
      "  The univariate objective function `value_and_gradients_function` is typically\n",
      "  generated by projecting a multivariate objective function along a search\n",
      "  direction. Suppose the multivariate function to be minimized is\n",
      "  `g(x1,x2, .. xn)`. Let (d1, d2, ..., dn) be the direction along which we wish\n",
      "  to perform a line search. Then the projected univariate function to be used\n",
      "  for line search is\n",
      "\n",
      "  ```None\n",
      "    f(a) = g(x1 + d1 * a, x2 + d2 * a, ..., xn + dn * a)\n",
      "  ```\n",
      "\n",
      "  The directional derivative along (d1, d2, ..., dn) is needed for this\n",
      "  procedure. This also corresponds to the derivative of the projected function\n",
      "  `f(a)` with respect to `a`. Note that this derivative must be negative for\n",
      "  `a = 0` if the direction is a descent direction.\n",
      "\n",
      "  The usual stopping criteria for the line search is the satisfaction of the\n",
      "  (weak) Wolfe conditions. For details of the Wolfe conditions, see\n",
      "  ref. [3]. On a finite precision machine, the exact Wolfe conditions can\n",
      "  be difficult to satisfy when one is very close to the minimum and as argued\n",
      "  by [Hager and Zhang (2005)][1], one can only expect the minimum to be\n",
      "  determined within square root of machine precision. To improve the situation,\n",
      "  they propose to replace the Wolfe conditions with an approximate version\n",
      "  depending on the derivative of the function which is applied only when one\n",
      "  is very close to the minimum. The following algorithm implements this\n",
      "  enhanced scheme.\n",
      "\n",
      "  ### Usage:\n",
      "\n",
      "  Primary use of line search methods is as an internal component of a class of\n",
      "  optimization algorithms (called line search based methods as opposed to\n",
      "  trust region methods). Hence, the end user will typically not want to access\n",
      "  line search directly. In particular, inexact line search should not be\n",
      "  confused with a univariate minimization method. The stopping criteria of line\n",
      "  search is the satisfaction of Wolfe conditions and not the discovery of the\n",
      "  minimum of the function.\n",
      "\n",
      "  With this caveat in mind, the following example illustrates the standalone\n",
      "  usage of the line search.\n",
      "\n",
      "  ```python\n",
      "    # Define value and gradient namedtuple\n",
      "    ValueAndGradient = namedtuple('ValueAndGradient', ['x', 'f', 'df'])\n",
      "    # Define a quadratic target with minimum at 1.3.\n",
      "    def value_and_gradients_function(x):\n",
      "      return ValueAndGradient(x=x, f=(x - 1.3) ** 2, df=2 * (x-1.3))\n",
      "    # Set initial step size.\n",
      "    step_size = tf.constant(0.1)\n",
      "    ls_result = tfp.optimizer.linesearch.hager_zhang(\n",
      "        value_and_gradients_function, initial_step_size=step_size)\n",
      "    # Evaluate the results.\n",
      "    with tf.Session() as session:\n",
      "      results = session.run(ls_result)\n",
      "      # Ensure convergence.\n",
      "      assert results.converged\n",
      "      # If the line search converged, the left and the right ends of the\n",
      "      # bracketing interval are identical.\n",
      "      assert results.left.x == result.right.x\n",
      "      # Print the number of evaluations and the final step size.\n",
      "      print (\"Final Step Size: %f, Evaluations: %d\" % (results.left.x,\n",
      "                                                       results.func_evals))\n",
      "  ```\n",
      "\n",
      "  ### References:\n",
      "  [1]: William Hager, Hongchao Zhang. A new conjugate gradient method with\n",
      "    guaranteed descent and an efficient line search. SIAM J. Optim., Vol 16. 1,\n",
      "    pp. 170-172. 2005.\n",
      "    https://www.math.lsu.edu/~hozhang/papers/cg_descent.pdf\n",
      "\n",
      "  [2]: William Hager, Hongchao Zhang. Algorithm 851: CG_DESCENT, a conjugate\n",
      "    gradient method with guaranteed descent. ACM Transactions on Mathematical\n",
      "    Software, Vol 32., 1, pp. 113-137. 2006.\n",
      "    http://users.clas.ufl.edu/hager/papers/CG/cg_compare.pdf\n",
      "\n",
      "  [3]: Jorge Nocedal, Stephen Wright. Numerical Optimization. Springer Series in\n",
      "    Operations Research. pp 33-36. 2006\n",
      "\n",
      "  Args:\n",
      "    value_and_gradients_function: A Python callable that accepts a real scalar\n",
      "      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n",
      "      correspond to scalar tensors of real dtype containing the point at which\n",
      "      the function was evaluated, the value of the function, and its\n",
      "      derivative at that point. The other namedtuple fields, if present,\n",
      "      should be tensors or sequences (possibly nested) of tensors.\n",
      "      In usual optimization application, this function would be generated by\n",
      "      projecting the multivariate objective function along some specific\n",
      "      direction. The direction is determined by some other procedure but should\n",
      "      be a descent direction (i.e. the derivative of the projected univariate\n",
      "      function must be negative at 0.).\n",
      "      Alternatively, the function may represent the batching of `n` such line\n",
      "      functions (e.g. projecting a single multivariate objective function along\n",
      "      `n` distinct directions at once) accepting n points as input, i.e. a\n",
      "      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n",
      "      namedtuple should each be a tensor of shape [n], with the corresponding\n",
      "      input points, function values, and derivatives at those input points.\n",
      "    initial_step_size: (Optional) Scalar positive `Tensor` of real dtype, or\n",
      "      a tensor of shape [n] in batching mode. The initial value (or values) to\n",
      "      try to bracket the minimum. Default is `1.` as a float32.\n",
      "      Note that this point need not necessarily bracket the minimum for the line\n",
      "      search to work correctly but the supplied value must be greater than 0.\n",
      "      A good initial value will make the search converge faster.\n",
      "    value_at_initial_step: (Optional) The full return value of evaluating\n",
      "      value_and_gradients_function at initial_step_size, i.e. a namedtuple with\n",
      "      'x', 'f', 'df', if already known by the caller. If supplied the value of\n",
      "      `initial_step_size` will be ignored, otherwise the tuple will be computed\n",
      "      by evaluating value_and_gradients_function.\n",
      "    value_at_zero: (Optional) The full return value of\n",
      "      value_and_gradients_function at `0.`, i.e. a namedtuple with\n",
      "      'x', 'f', 'df', if already known by the caller. If not supplied the tuple\n",
      "      will be computed by evaluating value_and_gradients_function.\n",
      "    converged: (Optional) In batching mode a tensor of shape [n], indicating\n",
      "      batch members which have already converged and no further search should\n",
      "      be performed. These batch members are also reported as converged in the\n",
      "      output, and both their `left` and `right` are set to the\n",
      "      `value_at_initial_step`.\n",
      "    threshold_use_approximate_wolfe_condition: Scalar positive `Tensor`\n",
      "      of real dtype. Corresponds to the parameter 'epsilon' in\n",
      "      [Hager and Zhang (2006)][2]. Used to estimate the\n",
      "      threshold at which the line search switches to approximate Wolfe\n",
      "      conditions.\n",
      "    shrinkage_param: Scalar positive Tensor of real dtype. Must be less than\n",
      "      `1.`. Corresponds to the parameter `gamma` in\n",
      "      [Hager and Zhang (2006)][2].\n",
      "      If the secant**2 step does not shrink the bracketing interval by this\n",
      "      proportion, a bisection step is performed to reduce the interval width.\n",
      "    expansion_param: Scalar positive `Tensor` of real dtype. Must be greater\n",
      "      than `1.`. Used to expand the initial interval in case it does not bracket\n",
      "      a minimum. Corresponds to `rho` in [Hager and Zhang (2006)][2].\n",
      "    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n",
      "      Bounded above by the curvature param. Corresponds to `delta` in the\n",
      "      terminology of [Hager and Zhang (2006)][2].\n",
      "    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n",
      "      by `1.`. Corresponds to 'sigma' in the terminology of\n",
      "      [Hager and Zhang (2006)][2].\n",
      "    step_size_shrink_param: Positive scalar `Tensor` of real dtype. Bounded\n",
      "      above by `1`. If the supplied step size is too big (i.e. either the\n",
      "      objective value or the gradient at that point is infinite), this factor\n",
      "      is used to shrink the step size until it is finite.\n",
      "    max_iterations: Positive scalar `Tensor` of integral dtype or None. The\n",
      "      maximum number of iterations to perform in the line search. The number of\n",
      "      iterations used to bracket the minimum are also counted against this\n",
      "      parameter.\n",
      "    name: (Optional) Python str. The name prefixed to the ops created by this\n",
      "      function. If not supplied, the default name 'hager_zhang' is used.\n",
      "\n",
      "  Returns:\n",
      "    results: A namedtuple containing the following attributes.\n",
      "      converged: Boolean `Tensor` of shape [n]. Whether a point satisfying\n",
      "        Wolfe/Approx wolfe was found.\n",
      "      failed: Boolean `Tensor` of shape [n]. Whether line search failed e.g.\n",
      "        if either the objective function or the gradient are not finite at\n",
      "        an evaluation point.\n",
      "      iterations: Scalar int32 `Tensor`. Number of line search iterations made.\n",
      "      func_evals: Scalar int32 `Tensor`. Number of function evaluations made.\n",
      "      left: A namedtuple, as returned by value_and_gradients_function,\n",
      "        of the left end point of the final bracketing interval. Values are\n",
      "        equal to those of `right` on batch members where converged is True.\n",
      "        Otherwise, it corresponds to the last interval computed.\n",
      "      right: A namedtuple, as returned by value_and_gradients_function,\n",
      "        of the right end point of the final bracketing interval. Values are\n",
      "        equal to those of `left` on batch members where converged is True.\n",
      "        Otherwise, it corresponds to the last interval computed.\n",
      "Generated :<s>Eager version of hager_zhang. Args: value_and_gradients_function: a</s>\n",
      "---------------------------\n",
      "Code: def from_python(python_obj, destination_frame=None, header=0, separator=\",\", column_names=None, column_types=None, na_strings=None): return H2OFrame(python_obj, destination_frame, header, separator, column_names, column_types, na_strings)\n",
      "Target: [DEPRECATED] Use constructor ``H2OFrame()`` instead.\n",
      "Generated :<s>Create an H2OFrame from a Python object. :param python_obj: Python object :param destination_</s>\n",
      "---------------------------\n",
      "Code: def get_client_secret(self, client_key, request): log.debug('Get client secret of %r', client_key) if not request.client: request.client = self._clientgetter(client_key=client_key) if request.client: return request.client.client_secret return None\n",
      "Target: Get client secret.\n",
      "\n",
      "        The client object must has ``client_secret`` attribute.\n",
      "Generated :<s>Retrieves the client secret for a given client. :type client_key: str :param client_key</s>\n",
      "---------------------------\n",
      "Code: def _get_exchanged_states(self, old_states, exchange_proposed, exchange_proposed_n, sampled_replica_states, sampled_replica_results): with tf.compat.v1.name_scope('get_exchanged_states'): target_log_probs = [] for replica in range(self.num_replica): replica_log_prob = _get_field(sampled_replica_results[replica], 'target_log_prob') inverse_temp = self.inverse_temperatures[replica] target_log_probs.append(replica_log_prob / inverse_temp) target_log_probs = tf.stack(target_log_probs, axis=0) dtype = target_log_probs.dtype num_state_parts = len(sampled_replica_states[0]) exchanged_states = [ tf.TensorArray( dtype, size=self.num_replica, dynamic_size=False, tensor_array_name='exchanged_states', element_shape=sampled_replica_states[0][k].shape) for k in range(num_state_parts) ] sample_shape = tf.concat( ([self.num_replica // 2], tf.shape(input=target_log_probs)[1:]), axis=0) log_uniforms = tf.math.log( tf.random.uniform( shape=sample_shape, dtype=dtype, seed=self._seed_stream())) def _swap(is_exchange_accepted, x, y): with tf.compat.v1.name_scope('swap_where_exchange_accepted'): new_x = mcmc_util.choose(is_exchange_accepted, y, x) new_y = mcmc_util.choose(is_exchange_accepted, x, y) return new_x, new_y def cond(i, unused_exchanged_states): return i < exchange_proposed_n def body(i, exchanged_states): m, n = tf.unstack(exchange_proposed[i]) temp_diff = self.inverse_temperatures[m] - self.inverse_temperatures[n] log_accept_ratio = mcmc_util.safe_sum( [-temp_diff * target_log_probs[m], temp_diff * target_log_probs[n]]) is_exchange_accepted = log_uniforms[i] < log_accept_ratio for k in range(num_state_parts): new_m, new_n = _swap(is_exchange_accepted, old_states[k].read(m), old_states[k].read(n)) exchanged_states[k] = exchanged_states[k].write(m, new_m) exchanged_states[k] = exchanged_states[k].write(n, new_n) return i + 1, exchanged_states return tf.while_loop( cond=cond, body=body, loop_vars=[tf.constant(0), exchanged_states])[1]\n",
      "Target: Get list of TensorArrays holding exchanged states, and zeros.\n",
      "Generated :<s>Parameters ---------- old_states : tf.Tensor with shape [batch_size, num_replica]</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def write(self, ostream, kmip_version=enums.KMIPVersion.KMIP_1_0): tstream = BytearrayStream() self.certificate_type.write(tstream, kmip_version=kmip_version) self.certificate_value.write(tstream, kmip_version=kmip_version) self.length = tstream.length() super(Certificate, self).write(ostream, kmip_version=kmip_version) ostream.write(tstream.buffer)\n",
      "Target: Write the data encoding the Certificate object to a stream.\n",
      "\n",
      "        Args:\n",
      "            ostream (Stream): A data stream in which to encode object data,\n",
      "                supporting a write method; usually a BytearrayStream object.\n",
      "            kmip_version (KMIPVersion): An enumeration defining the KMIP\n",
      "                version with which the object will be encoded. Optional,\n",
      "                defaults to KMIP 1.0.\n",
      "Generated :<s>:param ostream: :param kmip_version: :return:</s>\n",
      "---------------------------\n",
      "Code: def peng(number, frac_length, rjust=True): if number == 0: number = \"0.{zrs}\".format(zrs=\"0\" * frac_length) if frac_length else \"0\" return \"{0} \".format(number.rjust(5 + frac_length)) if rjust else number sign = +1 if number >= 0 else -1 ssign = \"-\" if sign == -1 else \"\" anumber = abs(number) if anumber < 1e-24: anumber = 1e-24 number = sign * 1e-24 exp = 3.0 * math.floor(math.floor(math.log10(anumber)) / 3.0) mant = number / 10 ** exp smant = str(mant) ppos = smant.find(\".\") if len(smant) - ppos - 1 > frac_length: mant += sign * 5 * 10 ** (-frac_length - 1) if abs(mant) >= 1000: exp += 3 mant = mant / 1e3 smant = str(mant) ppos = smant.find(\".\") bfrac_length = bool(frac_length) flength = ppos - (not bfrac_length) + frac_length + 1 new_mant = smant[:flength].ljust(flength, \"0\") if exp > 24: new_mant, exp = ( \"{sign}999.{frac}\".format(sign=ssign, frac=\"9\" * frac_length), 24, ) new_mant = new_mant.rjust(rjust * (4 + bfrac_length + frac_length)) num = \"{mant}{suffix}\".format( mant=new_mant, suffix=_POWER_TO_SUFFIX_DICT[exp] if exp else \" \" * bool(rjust) ) return num\n",
      "Target: r\"\"\"\n",
      "    Convert a number to engineering notation.\n",
      "\n",
      "    The absolute value of the number (if it is not exactly zero) is bounded to\n",
      "    the interval [1E-24, 1E+24)\n",
      "\n",
      "    :param number: Number to convert\n",
      "    :type  number: integer or float\n",
      "\n",
      "    :param frac_length: Number of digits of fractional part\n",
      "    :type  frac_length: `NonNegativeInteger\n",
      "                        <https://pexdoc.readthedocs.io/en/stable/\n",
      "                        ptypes.html#nonnegativeinteger>`_\n",
      "\n",
      "    :param rjust: Flag that indicates whether the number is\n",
      "                  right-justified (True) or not (False)\n",
      "    :type  rjust: boolean\n",
      "\n",
      "    :rtype: string\n",
      "\n",
      "    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n",
      "    .. Auto-generated exceptions documentation for peng.functions.peng\n",
      "\n",
      "    :raises:\n",
      "     * RuntimeError (Argument \\`frac_length\\` is not valid)\n",
      "\n",
      "     * RuntimeError (Argument \\`number\\` is not valid)\n",
      "\n",
      "     * RuntimeError (Argument \\`rjust\\` is not valid)\n",
      "\n",
      "    .. [[[end]]]\n",
      "\n",
      "    The supported engineering suffixes are:\n",
      "\n",
      "    +----------+-------+--------+\n",
      "    | Exponent | Name  | Suffix |\n",
      "    +==========+=======+========+\n",
      "    | 1E-24    | yocto | y      |\n",
      "    +----------+-------+--------+\n",
      "    | 1E-21    | zepto | z      |\n",
      "    +----------+-------+--------+\n",
      "    | 1E-18    | atto  | a      |\n",
      "    +----------+-------+--------+\n",
      "    | 1E-15    | femto | f      |\n",
      "    +----------+-------+--------+\n",
      "    | 1E-12    | pico  | p      |\n",
      "    +----------+-------+--------+\n",
      "    | 1E-9     | nano  | n      |\n",
      "    +----------+-------+--------+\n",
      "    | 1E-6     | micro | u      |\n",
      "    +----------+-------+--------+\n",
      "    | 1E-3     | milli | m      |\n",
      "    +----------+-------+--------+\n",
      "    | 1E+0     |       |        |\n",
      "    +----------+-------+--------+\n",
      "    | 1E+3     | kilo  | k      |\n",
      "    +----------+-------+--------+\n",
      "    | 1E+6     | mega  | M      |\n",
      "    +----------+-------+--------+\n",
      "    | 1E+9     | giga  | G      |\n",
      "    +----------+-------+--------+\n",
      "    | 1E+12    | tera  | T      |\n",
      "    +----------+-------+--------+\n",
      "    | 1E+15    | peta  | P      |\n",
      "    +----------+-------+--------+\n",
      "    | 1E+18    | exa   | E      |\n",
      "    +----------+-------+--------+\n",
      "    | 1E+21    | zetta | Z      |\n",
      "    +----------+-------+--------+\n",
      "    | 1E+24    | yotta | Y      |\n",
      "    +----------+-------+--------+\n",
      "\n",
      "    For example:\n",
      "\n",
      "        >>> import peng\n",
      "        >>> peng.peng(1235.6789E3, 3, False)\n",
      "        '1.236M'\n",
      "Generated :<s>Convert a number to a string. Args: number: The number to convert. frac_length:</s>\n",
      "---------------------------\n",
      "Code: def add_to_manifest(dynamodb_client, table_name, run_id): dynamodb_client.put_item( TableName=table_name, Item={ DYNAMODB_RUNID_ATTRIBUTE: { 'S': run_id } } )\n",
      "Target: Add run_id into DynamoDB manifest table\n",
      "\n",
      "    Arguments:\n",
      "    dynamodb_client - boto3 DynamoDB client (not service)\n",
      "    table_name - string representing existing table name\n",
      "    run_id - string representing run_id to store\n",
      "Generated :<s>:type dynamodb_client: dynamodb.client.DynamodbClient :type table_</s>\n",
      "---------------------------\n",
      "Code: def get_parts_by_class(self, cls): return (part for part in self.parts.values() if isinstance(part, cls))\n",
      "Target: Return all parts of this package that are instances of cls\n",
      "\t\t(where cls is passed directly to isinstance, so can be a class\n",
      "\t\tor sequence of classes).\n",
      "Generated :<s>get_parts_by_class :param cls: :return:</s>\n",
      "---------------------------\n",
      "Code: def exec_module(self, module): fullname = module.__name__ cached = self._cache[fullname] cached[\"module\"] = module spec = cached[\"spec\"] filename = spec.loader_state[\"filename\"] path_stats = self.path_stats(filename) ns_name = demunge(fullname) ns: runtime.Namespace = runtime.set_current_ns(ns_name).value ns.module = module if os.getenv(_NO_CACHE_ENVVAR, None) == \"true\": self._exec_module(fullname, spec.loader_state, path_stats, module) else: try: self._exec_cached_module( fullname, spec.loader_state, path_stats, module ) except (EOFError, ImportError, IOError, OSError) as e: logger.debug(f\"Failed to load cached Basilisp module: {e}\") self._exec_module(fullname, spec.loader_state, path_stats, module) runtime.Namespace.add_default_import(ns_name)\n",
      "Target: Compile the Basilisp module into Python code.\n",
      "\n",
      "        Basilisp is fundamentally a form-at-a-time compilation, meaning that\n",
      "        each form in a module may require code compiled from an earlier form, so\n",
      "        we incrementally compile a Python module by evaluating a single top-level\n",
      "        form at a time and inserting the resulting AST nodes into the Pyton module.\n",
      "Generated :<s>Load the module from the cache. :param module: The module to load.</s>\n",
      "---------------------------\n",
      "[Test model results]\n",
      "---------------------------\n",
      "Code: def get_realms(self, token, request): log.debug('Get realms of %r', token) tok = request.request_token or self._grantgetter(token=token) if not tok: return [] request.request_token = tok if hasattr(tok, 'realms'): return tok.realms or [] return []\n",
      "Target: Realms for this request token.\n",
      "Generated :<s>Get realms from the token. :param token: The token. :type token: str :param request: The</s>\n",
      "---------------------------\n",
      "Code: def contains_add(self, item): try: self._lens_contains_add except AttributeError: message = 'Don\\'t know how to add an item to {}' raise NotImplementedError(message.format(type(self))) else: return self._lens_contains_add(item)\n",
      "Target: Takes a collection and an item and returns a new collection\n",
      "    of the same type that contains the item. The notion of \"contains\"\n",
      "    is defined by the object itself; The following must be ``True``:\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        item in contains_add(obj, item)\n",
      "\n",
      "    This function is used by some lenses (particularly ContainsLens)\n",
      "    to add new items to containers when necessary.\n",
      "\n",
      "    The corresponding method call for this hook is\n",
      "    ``obj._lens_contains_add(item)``.\n",
      "\n",
      "    There is no default implementation.\n",
      "Generated :<s>Check if an item is contained in the list. :param item: The item to check. :type item:</s>\n",
      "---------------------------\n",
      "Code: def check_function( state, name, index=0, missing_msg=None, params_not_matched_msg=None, expand_msg=None, signature=True, ): append_missing = missing_msg is None append_params_not_matched = params_not_matched_msg is None if missing_msg is None: missing_msg = MISSING_MSG if expand_msg is None: expand_msg = PREPEND_MSG if params_not_matched_msg is None: params_not_matched_msg = SIG_ISSUE_MSG stu_out = state.ast_dispatcher(\"function_calls\", state.student_ast) sol_out = state.ast_dispatcher(\"function_calls\", state.solution_ast) student_mappings = state.ast_dispatcher(\"mappings\", state.student_ast) fmt_kwargs = { \"times\": get_times(index + 1), \"ord\": get_ord(index + 1), \"index\": index, \"mapped_name\": get_mapped_name(name, student_mappings), } try: sol_parts = {**sol_out[name][index]} except KeyError: raise InstructorError( \"`check_function()` couldn't find a call of `%s()` in the solution code. Make sure you get the mapping right!\" % name ) except IndexError: raise InstructorError( \"`check_function()` couldn't find %s calls of `%s()` in your solution code.\" % (index + 1, name) ) try: stu_parts = {**stu_out[name][index]} except (KeyError, IndexError): _msg = state.build_message(missing_msg, fmt_kwargs, append=append_missing) state.report(Feedback(_msg, state)) if signature: signature = None if isinstance(signature, bool) else signature get_sig = partial( getSignatureInProcess, name=name, signature=signature, manual_sigs=state.get_manual_sigs(), ) try: sol_sig = get_sig( mapped_name=sol_parts[\"name\"], process=state.solution_process ) sol_parts[\"args\"] = bind_args(sol_sig, sol_parts[\"args\"]) except Exception as e: raise InstructorError( \"`check_function()` couldn't match the %s call of `%s` to its signature:\\n%s \" % (get_ord(index + 1), name, e) ) try: stu_sig = get_sig( mapped_name=stu_parts[\"name\"], process=state.student_process ) stu_parts[\"args\"] = bind_args(stu_sig, stu_parts[\"args\"]) except Exception: _msg = state.build_message( params_not_matched_msg, fmt_kwargs, append=append_params_not_matched ) state.report( Feedback( _msg, StubState(stu_parts[\"node\"], state.highlighting_disabled) ) ) append_message = {\"msg\": expand_msg, \"kwargs\": fmt_kwargs} child = part_to_child( stu_parts, sol_parts, append_message, state, node_name=\"function_calls\" ) return child\n",
      "Target: Check whether a particular function is called.\n",
      "\n",
      "    ``check_function()`` is typically followed by:\n",
      "    \n",
      "    - ``check_args()`` to check whether the arguments were specified.\n",
      "      In turn, ``check_args()`` can be followed by ``has_equal_value()`` or ``has_equal_ast()``\n",
      "      to assert that the arguments were correctly specified.\n",
      "    - ``has_equal_value()`` to check whether rerunning the function call coded by the student\n",
      "      gives the same result as calling the function call as in the solution.\n",
      "\n",
      "    Checking function calls is a tricky topic. Please visit the\n",
      "    `dedicated article <articles/checking_function_calls.html>`_ for more explanation,\n",
      "    edge cases and best practices.\n",
      "\n",
      "    Args:\n",
      "        name (str): the name of the function to be tested. When checking functions in packages, always\n",
      "            use the 'full path' of the function.\n",
      "        index (int): index of the function call to be checked. Defaults to 0.\n",
      "        missing_msg (str): If specified, this overrides an automatically generated feedback message in case\n",
      "            the student did not call the function correctly.\n",
      "        params_not_matched_msg (str): If specified, this overrides an automatically generated feedback message\n",
      "            in case the function parameters were not successfully matched.\n",
      "        expand_msg (str): If specified, this overrides any messages that are prepended by previous SCT chains.\n",
      "        signature (Signature): Normally, check_function() can figure out what the function signature is,\n",
      "            but it might be necessary to use ``sig_from_params()`` to manually build a signature and pass this along.\n",
      "        state (State): State object that is passed from the SCT Chain (don't specify this).\n",
      "\n",
      "    :Examples:\n",
      "\n",
      "        Student code and solution code::\n",
      "\n",
      "            import numpy as np\n",
      "            arr = np.array([1, 2, 3, 4, 5])\n",
      "            np.mean(arr)\n",
      "\n",
      "        SCT::\n",
      "\n",
      "            # Verify whether arr was correctly set in np.mean\n",
      "            Ex().check_function('numpy.mean').check_args('a').has_equal_value()\n",
      "\n",
      "            # Verify whether np.mean(arr) produced the same result\n",
      "            Ex().check_function('numpy.mean').has_equal_value()\n",
      "Generated :<s>Check if a function is a valid function. :param state: The state. :param name: The name of</s>\n",
      "---------------------------\n",
      "Code: def transpose_table(table): if len(table) == 0: return table else: num_columns = len(table[0]) return [[row[i] for row in table] for i in range(num_columns)]\n",
      "Target: Transposes a table, turning rows into columns.\n",
      "    :param table: A 2D string grid.\n",
      "    :type table: [[``str``]]\n",
      "\n",
      "    :return: The same table, with rows and columns flipped.\n",
      "    :rtype: [[``str``]]\n",
      "Generated :<s>Transpose a table to a list of lists. :param table: The table to transpose. :type table</s>\n",
      "---------------------------\n",
      "Code: def clinvar(institute_id, case_name, variant_id): data = controllers.clinvar_export(store, institute_id, case_name, variant_id) if request.method == 'GET': return data else: form_dict = request.form.to_dict() submission_objects = set_submission_objects(form_dict) open_submission = store.get_open_clinvar_submission(current_user.email, institute_id) updated_submission = store.add_to_submission(open_submission['_id'], submission_objects) return redirect(url_for('cases.clinvar_submissions', institute_id=institute_id))\n",
      "Target: Build a clinVar submission form for a variant.\n",
      "Generated :<s>Creates a institute and adds it to the database.</s>\n",
      "---------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0663e998dd6c4452ab039b0300b9bb27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1653.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training stopped due to no improvement in 0 epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, \n",
    "            optimizer=optim, \n",
    "            lr_scheduler=scheduler, \n",
    "            scheduler_step_frequency='step',\n",
    "            loaders=dataloaders, \n",
    "            max_epochs=MAX_EPOCHS, \n",
    "            device=device,\n",
    "            best_model=best_model,\n",
    "            patience=PATIENCE, \n",
    "            padding_idx=bart_tokenizer.pad_token_id\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQeP6WZqjZC4"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    code = \"def foo(x,y): x + y\"\n",
    "    inp = shared_spm.tokenize(code)\n",
    "    inp = torch.tensor(inp).view(1,-1)\n",
    "    generated = model.generate(input_ids=inp.to(model.device), \n",
    "                               decoder_start_token_id=shared_spm.bos_id(), \n",
    "                               num_beams=10,\n",
    "                               max_length=100, no_repeat_ngram_size=4)\n",
    "    generated = generated[0].cpu().tolist()\n",
    "    print(shared_spm.decode(generated))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Fine-tuning code_to_docstring transformer model",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0170af284d8f461185a74a9e846ad69e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0170fe02467340fb8bfcf58f0f59813d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "032c5e2018084305857f9ad4c7a09ec3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04d20bb756174e6eae44bf594c67bf10": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": " 25%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3974ec9f1be45fcae9f3ac5dd43af5e",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bd97154633b2421bb3f478889c5c58f7",
      "value": 1
     }
    },
    "065c90a0fb96402695ecb3d0723bea64": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08310e3e94964cdda729f31fcf46e9fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_26a9432d39384357ae6895db77cd306e",
      "max": 2241,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_49afa35c19414cff826aac49f948ad4f",
      "value": 2241
     }
    },
    "0ff4d3c660114a90a9ecbcc8b74b03fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4c951b96425242a9969c08101c187198",
       "IPY_MODEL_a23d31de48ee4de99a6bfbe75d9d5382"
      ],
      "layout": "IPY_MODEL_dbfe2779d8be4d1882e2e8567b96d1b1"
     }
    },
    "18459f31bb9543aa8ad3c2420e134ab2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56b0c2d6c01a40eaaec787f2427c9ea4",
      "placeholder": "​",
      "style": "IPY_MODEL_7a6074e43d65464f8ca5a088ba075ce5",
      "value": " 39957/39957 [57:28&lt;00:00, 11.59it/s]"
     }
    },
    "1bd81390f4d7430e981c1f13a1c79d7f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c5c8da1be4f40d5b2a9e9ac46c643f6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ce0ebb938304d5cafa1b5f23f8ecf69": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "224af8630019457ba874067c146f1bba": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26a9432d39384357ae6895db77cd306e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27d1422d4bd042938661ac7bab629c0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_963f53c3cf404e7595dc554fc63d4df4",
       "IPY_MODEL_74203a2bcb2a4e3e9fb71ca475f9c259"
      ],
      "layout": "IPY_MODEL_4505a00686194672ae8925431e44d346"
     }
    },
    "27e4dd787061427591847cb34c3b03ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "32ce0ac9d280447587bd4eda65a190d4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34cd929da6c244b4a4d5cb7dbb1a2e0a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "368178aefa7e465aa82222bef29cbed5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a0d782f37e645fe9445b1fdc3acfde9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a6bc9d0d8020406f85570c441ebb03a9",
       "IPY_MODEL_7763b7074338498cbdd0ce8fa9534564"
      ],
      "layout": "IPY_MODEL_065c90a0fb96402695ecb3d0723bea64"
     }
    },
    "3aed8bcf8b564a48a638cadc0b0151e3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d82364d3bef42fcb15d324891e709e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "41836f7bf92c4cc5b7d0ed6753d7566f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4505a00686194672ae8925431e44d346": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "464a8630bb5344be9af17d2e3b234544": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49afa35c19414cff826aac49f948ad4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4c05e986759b450abe976683f157af42": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32ce0ac9d280447587bd4eda65a190d4",
      "placeholder": "​",
      "style": "IPY_MODEL_5040d47de6a54a4eb48097a58a2dab1d",
      "value": " 2258/2258 [00:01&lt;00:00, 1152.21it/s]"
     }
    },
    "4c951b96425242a9969c08101c187198": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": " 29%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_368178aefa7e465aa82222bef29cbed5",
      "max": 4995,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c211b46cfe6f4f90b762aad20f7ee2b9",
      "value": 1451
     }
    },
    "4cac3adb7ea2492882f11a42f8558189": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f31cb2b4adf4756ab2d5e08fd980436": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5040d47de6a54a4eb48097a58a2dab1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "56b0c2d6c01a40eaaec787f2427c9ea4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58411f1b150249a39158f7ed1e254206": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a6eb37f358354c1abd67c876691036c9",
       "IPY_MODEL_4c05e986759b450abe976683f157af42"
      ],
      "layout": "IPY_MODEL_ac1cc59595cd40af90daa26387f853f0"
     }
    },
    "5b3278fc935e4d07b65aa735590758f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c5c8da1be4f40d5b2a9e9ac46c643f6",
      "max": 2241,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_27e4dd787061427591847cb34c3b03ae",
      "value": 2241
     }
    },
    "5b510735452c441f9a855ea43b2b3b6d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "677a45bd99784f26a4941aabb8784a04": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_08310e3e94964cdda729f31fcf46e9fd",
       "IPY_MODEL_6c42ce2210784ffca06210211751bf60"
      ],
      "layout": "IPY_MODEL_93474465b1d0414ab292bd92aa4e39f4"
     }
    },
    "67bbfa6548a34d828cab44ecdb09acba": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c42ce2210784ffca06210211751bf60": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_032c5e2018084305857f9ad4c7a09ec3",
      "placeholder": "​",
      "style": "IPY_MODEL_b05d914f44bb4a269383636295f0ddfe",
      "value": " 2241/2241 [57:21&lt;00:00,  1.54s/it]"
     }
    },
    "6d40bb935de74b1499d2b33c8b7eca95": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "6e82cf5b42184e078e10c342adbf0fca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "6f6a6fd0019b4e64aae6e8bbf8ce144d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5b3278fc935e4d07b65aa735590758f9",
       "IPY_MODEL_f6d62ccd9c624855bb953f060020a79b"
      ],
      "layout": "IPY_MODEL_ad564ace58bc43738801d278055cf70b"
     }
    },
    "73cf070109c046a6ba833c860918b3df": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "74203a2bcb2a4e3e9fb71ca475f9c259": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c11e5211a0854a82978fef8e72d2b3ee",
      "placeholder": "​",
      "style": "IPY_MODEL_ec01cb9d81b947d395a473dba7a0be06",
      "value": " 4995/4995 [1:00:48&lt;00:00,  1.37it/s, avg_train_loss=5.71, train_step_loss=5.07]"
     }
    },
    "7476d0bd592843f1a06fbd3ce50b15cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f31cb2b4adf4756ab2d5e08fd980436",
      "placeholder": "​",
      "style": "IPY_MODEL_cce045e884bd4aac874bc110c14e33b0",
      "value": " 1/4 [1:02:09&lt;3:06:27, 3729.24s/it]"
     }
    },
    "75cbbe18416b404d9c8b9d88415b05c5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7763b7074338498cbdd0ce8fa9534564": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d4c264c0f19a4265af7148ac0814b48d",
      "placeholder": "​",
      "style": "IPY_MODEL_cdc8cb62d0e1416e90c791544be95f66",
      "value": " 40563/40563 [00:31&lt;00:00, 1274.88it/s]"
     }
    },
    "7a6074e43d65464f8ca5a088ba075ce5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d17b5188a924dabbb4d4962d7b826ab": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8449169063654aca906068acff76652d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "851cf5a5b35142299d2761f0742841a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "87676b91d4834f93adb91622e59c63fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8a18144909074193bb17ea5c43c7ae11",
       "IPY_MODEL_fcf0e7e96aec45c8b5433524e769c957"
      ],
      "layout": "IPY_MODEL_67bbfa6548a34d828cab44ecdb09acba"
     }
    },
    "88991afeec49461e9aaff7a0d1da365b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_de2624bf5c7e4e1aaccdf867efced9f9",
       "IPY_MODEL_98f0e71a7a914a7ba68a24b8f79d72fe"
      ],
      "layout": "IPY_MODEL_0170af284d8f461185a74a9e846ad69e"
     }
    },
    "8a18144909074193bb17ea5c43c7ae11": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8d7607d2b8e40f88e2b433e3d649e21",
      "max": 39957,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d83f2d363c544aadb23733b705a508e3",
      "value": 39957
     }
    },
    "8cca36eca10f45a9bd48aa9360c49253": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f0d023d16b840379f397ac8e7c3493c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "923942a6c1ba4257bdddc929c4ee6892": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_04d20bb756174e6eae44bf594c67bf10",
       "IPY_MODEL_7476d0bd592843f1a06fbd3ce50b15cc"
      ],
      "layout": "IPY_MODEL_75cbbe18416b404d9c8b9d88415b05c5"
     }
    },
    "93474465b1d0414ab292bd92aa4e39f4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "963f53c3cf404e7595dc554fc63d4df4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3aed8bcf8b564a48a638cadc0b0151e3",
      "max": 4995,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6d40bb935de74b1499d2b33c8b7eca95",
      "value": 4995
     }
    },
    "98f0e71a7a914a7ba68a24b8f79d72fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_224af8630019457ba874067c146f1bba",
      "placeholder": "​",
      "style": "IPY_MODEL_73cf070109c046a6ba833c860918b3df",
      "value": " 281/281 [01:18&lt;00:00,  3.59it/s, avg_val_loss=5.23, val_step_loss=5.73]"
     }
    },
    "9da5f0d18a72477db595f3fb5b404a4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fb257000c67142a9b2e7cac50f879922",
       "IPY_MODEL_cc1e493375c84e55a5e2e7640671d66b"
      ],
      "layout": "IPY_MODEL_7d17b5188a924dabbb4d4962d7b826ab"
     }
    },
    "a23d31de48ee4de99a6bfbe75d9d5382": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b510735452c441f9a855ea43b2b3b6d",
      "placeholder": "​",
      "style": "IPY_MODEL_8cca36eca10f45a9bd48aa9360c49253",
      "value": " 1451/4995 [17:42&lt;41:15,  1.43it/s, avg_train_loss=4.9, train_step_loss=5.13]"
     }
    },
    "a6bc9d0d8020406f85570c441ebb03a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc4a36e3d20f466db30c766b9cad498e",
      "max": 40563,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3d82364d3bef42fcb15d324891e709e3",
      "value": 40563
     }
    },
    "a6eb37f358354c1abd67c876691036c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db68a029d1cf44ea901142753a0a63a1",
      "max": 2258,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fb595b0e7fad4cf99bb5f226baaa56c6",
      "value": 2258
     }
    },
    "ac1cc59595cd40af90daa26387f853f0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad564ace58bc43738801d278055cf70b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b05d914f44bb4a269383636295f0ddfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b3974ec9f1be45fcae9f3ac5dd43af5e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbce2ecea1d8484b97377ebc9aab5a55": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d958317e72594ba8990df000f53ffa3b",
       "IPY_MODEL_18459f31bb9543aa8ad3c2420e134ab2"
      ],
      "layout": "IPY_MODEL_34cd929da6c244b4a4d5cb7dbb1a2e0a"
     }
    },
    "bd97154633b2421bb3f478889c5c58f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c11e5211a0854a82978fef8e72d2b3ee": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c211b46cfe6f4f90b762aad20f7ee2b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c577f0e201ca40b9b6cfedd733b4ef20": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "cc1e493375c84e55a5e2e7640671d66b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1bd81390f4d7430e981c1f13a1c79d7f",
      "placeholder": "​",
      "style": "IPY_MODEL_41836f7bf92c4cc5b7d0ed6753d7566f",
      "value": " 2203/2203 [00:22&lt;00:00, 99.20it/s]"
     }
    },
    "cce045e884bd4aac874bc110c14e33b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cdc8cb62d0e1416e90c791544be95f66": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d4c264c0f19a4265af7148ac0814b48d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d83f2d363c544aadb23733b705a508e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d958317e72594ba8990df000f53ffa3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ce0ebb938304d5cafa1b5f23f8ecf69",
      "max": 39957,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6e82cf5b42184e078e10c342adbf0fca",
      "value": 39957
     }
    },
    "db68a029d1cf44ea901142753a0a63a1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbfe2779d8be4d1882e2e8567b96d1b1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc4a36e3d20f466db30c766b9cad498e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de2624bf5c7e4e1aaccdf867efced9f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f0d023d16b840379f397ac8e7c3493c",
      "max": 281,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c577f0e201ca40b9b6cfedd733b4ef20",
      "value": 281
     }
    },
    "e4dcef5e456c4b3588f3c23289243291": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8d7607d2b8e40f88e2b433e3d649e21": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec01cb9d81b947d395a473dba7a0be06": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f6d62ccd9c624855bb953f060020a79b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cac3adb7ea2492882f11a42f8558189",
      "placeholder": "​",
      "style": "IPY_MODEL_0170fe02467340fb8bfcf58f0f59813d",
      "value": " 2241/2241 [00:00&lt;00:00, 5295.56it/s]"
     }
    },
    "fb257000c67142a9b2e7cac50f879922": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_464a8630bb5344be9af17d2e3b234544",
      "max": 2203,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_851cf5a5b35142299d2761f0742841a6",
      "value": 2203
     }
    },
    "fb595b0e7fad4cf99bb5f226baaa56c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "fcf0e7e96aec45c8b5433524e769c957": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8449169063654aca906068acff76652d",
      "placeholder": "​",
      "style": "IPY_MODEL_e4dcef5e456c4b3588f3c23289243291",
      "value": " 39957/39957 [57:24&lt;00:00, 11.60it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
